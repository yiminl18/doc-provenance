[
  {
    "06688002": {
      "question": "Which math competition's lemma was used to test ImProver?",
      "answer": "2022 International Math Olympiad",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            23,
            24,
            25
          ],
          "provenance_ids": [
            24
          ],
          "provenance": "In this example, ImProver optimizes a human-written\nlemma (right) from the 2022 International Math Olympiad (Question 2, solution from Comp-\nﬁles (David Renshaw, 2024)) for length.",
          "time": 7.208366632461548,
          "input_token_size": 17546,
          "output_token_size": 45,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_06688002",
        "created_at": 1749869901.5850568,
        "processing_time": 13.313737630844116,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "shorter_provenance",
        "sentence_count": 3,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 3,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.821084"
      }
    }
  },
  {
    "07fe5033": {
      "question": "How does ImProver's Chain-of-States technique improve proof optimization?",
      "answer": "ImProver's Chain-of-States technique improves proof optimization by making intermediate proof states explicit, allowing the model to reason about these states and construct more optimized proofs through additional symbolic data.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69
          ],
          "provenance_ids": [
            47,
            48,
            49,
            50,
            51,
            56,
            57,
            58
          ],
          "provenance": "3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.The intermediate states often contain valuable information (e.g., an expression\nafter it has been simpliﬁed) that is not present in the tactics.To allow the model to reason about\nthese intermediate goals and hypotheses, we use tools from Lean metaprogramming to automatically\nannotate each proof state as a comment prior to each tactic.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.Figure 2 shows an example. This explicit reasoning aims to help the generator model construct more optimized proofs via addi-\ntional symbolic data.3.1.2 OUTPUT FORMATTING. LLM outputs often contain ancillary and syntactically invalid content, especially before and after\nthe actual proof.Additionally, by applying additional structure to the LLM outputs, we may hope to\ngenerate more structured proofs.",
          "time": 21.522778749465942,
          "input_token_size": 18235,
          "output_token_size": 244,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_07fe5033",
        "created_at": 1749869916.0507374,
        "processing_time": 28.37543034553528,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "shorter_provenance",
        "sentence_count": 23,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 23,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.821084"
      }
    }
  },
  {
    "089bb9e2": {
      "question": "Which language model is mentioned as a baseline comparison for ImProver?",
      "answer": "GPT-4o",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187
          ],
          "provenance_ids": [
            62,
            63,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            176,
            177
          ],
          "provenance": "3\n\n\fWithout Chain-of-States\n\nWith Chain-of-States\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u) := by\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u)\n\n:= by\n\nrintro x (hxs, xti | hxs, xui)\n· use xs; left; exact xt\n.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.More speciﬁcally, our scoring function is given by the 2-ary\ncomparison function S, whose arguments are output objects y, y′.max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.This comparison function can be extended to evaluate the best output of any ﬁnite n via induction.This best-of-n technique is implemented as a curried function such that each of the n calls can be\nhandled by any arbitrary sampling method, or just a single standard prompt at user discretion.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.The reﬁnement process relies on user-deﬁned pa-\nrameters n and prev_num to specify the number of iterations and the number of previous iteration\ninfo to forward, respectively.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.Similar to the best-of-n tech-\nnique, it relies on an argument n for the number of reﬁnement steps, and is curried such that each\nreﬁnement step can be handled by any other prompting function.However, unlike best-of-n, there is\nno opportunity for parallelism as each iteration is dependent on information from the previous call.Combination Sampling and Compound Prompt Functions Compound prompt functions utilize\nthe curried nature of the implementations of best-of-n and reﬁnement to nest these techniques within\none another.For example:\n\n4\n\n\fbest_of_n((refinement,m),n) is a compound sampling method that run a best-of-n,\nwhere each call is a m-step reﬁnement.refinement((best_of_n,m),n) is a compound sampling method that runs a n-step reﬁne-\nment, where each call is a best-of-m call to the LLM.Note that with each of these compound prompt functions, there are always a total of mn iterations.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.Overall, we conclude that although the performance of both ImProver and GPT-4o decreases on\nlength and readability optimization on more difﬁcult datasets, ImProver signiﬁcantly outperforms\nGPT-4o on all datasets for length and readability optimization.4.2.1 ABLATION TESTING\n\nWe perform ablation studies using a subset of the MIL dataset as discussed in §4.1.1.",
          "time": 43.830129623413086,
          "input_token_size": 15453,
          "output_token_size": 17,
          "original_provenance_id": 0
        },
        {
          "provenance_id": 1,
          "input_sentence_ids": [
            205,
            206,
            207
          ],
          "provenance_ids": [
            205
          ],
          "provenance": "MIL\n\nSet Theory Group Theory\n\nOverall\n\nGPT-4o\nImProver\n\n18.18%\n45.45%\n\n25%\n33.33%\n\n21.73%\n39.13%\n\n4.3 QUALITATIVE RESULTS\n\nNext, we discuss qualitative examples showing the improvements from ImProver in proof optimiza-\ntion.",
          "time": 54.08962273597717,
          "input_token_size": 39548,
          "output_token_size": 49,
          "original_provenance_id": 1
        },
        {
          "provenance_id": 2,
          "input_sentence_ids": [
            202,
            203,
            204
          ],
          "provenance_ids": [
            202
          ],
          "provenance": "ImProver substantially outperforms GPT-4o across all topics, with an 80% increase in accuracy\ncompared to the base model, showing that proof optimization systems are easily extendable to NTP\nsystems.",
          "time": 56.64997339248657,
          "input_token_size": 39835,
          "output_token_size": 57,
          "original_provenance_id": 2
        },
        {
          "provenance_id": 3,
          "input_sentence_ids": [
            199,
            200,
            201
          ],
          "provenance_ids": [
            201
          ],
          "provenance": "Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-4o.",
          "time": 58.4652955532074,
          "input_token_size": 39959,
          "output_token_size": 61,
          "original_provenance_id": 3
        },
        {
          "provenance_id": 4,
          "input_sentence_ids": [
            188,
            189,
            190,
            191,
            192
          ],
          "provenance_ids": [
            189
          ],
          "provenance": "Now, as GPT-4o-mini is signiﬁcantly less computationally expensive than\nits GPT-4o counterpart, we test both models with the sample method ﬁxed to best-of-n, and vary\nn = 1, 3, 5, 7, 10, 15, and for GPT-4o-mini, also n = 20.",
          "time": 62.28335881233215,
          "input_token_size": 41021,
          "output_token_size": 70,
          "original_provenance_id": 4
        }
      ],
      "metadata": {
        "question_id": "paper_089bb9e2",
        "created_at": 1749665204.3519676,
        "processing_time": 72.61327719688416,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev",
        "reason": "shorter_provenance",
        "sentence_count": 202,
        "provenance_count": 5,
        "alternatives": [
          {
            "source": "test_outputs_prev",
            "sentence_count": 202,
            "provenance_count": 5,
            "has_answer": true
          },
          {
            "source": "test_outputs_prev1",
            "sentence_count": 202,
            "provenance_count": 5,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.824379"
      }
    }
  },
  {
    "0920f3ef": {
      "question": "What improvements does ImProver have over naive LLM applications?",
      "answer": "1. Use of symbolic Lean context in a novel Chain-of-States technique.\n2. Error-correction and retrieval mechanisms.\n3. Ability to optimize for arbitrary user-defined metrics (e.g., length, readability).\n4. Generation of substantially shorter, more modular, and more readable proofs.\n5. Best-of-n sampling method for selecting optimal outputs.\n6. Iterative refinement process to improve generated proofs.\n7. Retrieval-augmented generation for selecting relevant examples and documents.\n8. Enhanced output formatting options (flat and structured outputs).\n9. Improved accuracy in generating correct outputs compared to naive LLM applications.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance": "4\n2\n0\n2\n \nt\nc\nO\n \n7\n \n \n]\nI\n\nA\n. s\nc\n[\n \n \n1\nv\n3\n5\n7\n4\n0\n.0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.To this end, we study a new problem of automated proof\noptimization: rewriting a proof so that it is correct and optimizes for an arbitrary\ncriterion, such as length or readability.As a ﬁrst method for automated proof opti-\nmization, we present ImProver, a large-language-model agent that rewrites proofs\nto optimize arbitrary user-deﬁned metrics in Lean.We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.In\npractice, however, informal, natural-language proofs are prone to imprecision, ambiguity, and error.Using a formal language such as Lean (Moura & Ullrich, 2021) removes ambiguity and precision\nand enables a proof assistant to verify correctness down to the primitives of a formal axiomatic\nsystem.Formal proofs, however, can be hard to read and often suffer from low reusability or excessive detail.For example, formal proofs in Lean’s extensive mathematical library, Mathlib (mathlib Community,\n2020), are generally designed to be concise and very general, often at the expense of readability.Formal proofs in an expository text, in contrast, may include detailed calculations steps, making\nthem readable but verbose.Machine learning systems trained on such proofs learn to mimic these\nvaried conventions (Hu et al., 2024), which may not be the optimal use of the limited supply of\nhuman-written proofs.As a result, we would like to be able to automatically refactor proofs to meet\na secondary objective such as length or readability.To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is\ncorrect and optimizes a criterion such as length or readability.We ﬁnd that naively applying LLMs\nto proof optimization falls short, often resulting in incorrect or poorly optimized proofs.We develop\nvarious improvements that can be applied on top of a black-box language model, including Chain-\nof-States prompting–an analogy to chain-of-thought prompting (Wei et al., 2022) that shows inter-\nmediate proof states–along with error-correction and retrieval.We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.1\n\n\fWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more readable, and more declarative in style.1\n\nOriginal (human-written)\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\nImProver (length-optimized)\n\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nrw [h1u _ (Classical.choose_spec _)]\nobtain hw, h1e', h1u'i := h1 y\nrw [h1u' _ ((h2 _ _).mpr h1e)]\nexact h1u' _ (Classical.choose_spec _)\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nhave h2' : Classical.choose (h1 x).exists =\n\ny :=\nh1u _ (Classical.choose_spec (h1\nx).exists)\n\nrw [h2']\nobtain hw, h1e', h1u'i := h1 y\nhave h4 := Classical.choose_spec (h1\n\nhave hxw : x = w := by\n\ny).exists\n\napply h1u'\nrw [h2]\nexact h1e\n\nrw [hxw]\nexact h1u' _ h4\n\nFigure 1: ImProver automatically rewrites formal proofs to optimize a criterion such as length\nor readability while remaining correct.In this example, ImProver optimizes a human-written\nlemma (right) from the 2022 International Math Olympiad (Question 2, solution from Comp-\nﬁles (David Renshaw, 2024)) for length.ImProver’s optimized proof is correct and more concise.2 RELATED WORK\n\nRecently there has been wide interest in automating theorem proving in interactive proof assistants;\nsee (Lu et al., 2023; Li et al., 2024) for surveys.A typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs\nsuch as Lean’s Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024).A model learns from the distribution of proofs in the cor-\npus, such as Mathlib-style proofs.Recently, the AlphaProof (AlphaProof & Teams, 2024) system\nwas shown to produce proofs with an arcane, non-human structure and syntax.We consider the\nnew problem of rewriting a proof to optimize a metric, such as rewriting a proof into a more read-\nable or more concise one.Proof optimization is more general than theorem proving, since we can\nalso rewrite an empty proof to optimize correctness.Finally, there is a rich literature on the var-\nied styles of (human) formal proofs (e.g., (Autexier & Dietrich, 2010; Wiedijk, 2004)).Our model,\nImProver, builds on neural theorem proving techniques including full proof generation (Jiang et al.,\n2023; First et al., 2023), conditioning on example proofs (Jiang et al., 2023), retrieval (Yang et al.,\n2023; Thakur et al., 2024), and preceding ﬁle context (First et al., 2023; Hu et al., 2024), as well as\nerror correction (Madaan et al., 2023; Chen et al., 2023) and documentation retrieval (Zhou et al.,\n2023) from code generation.ImProver brings these code generation techniques, along with new\nChain-of-States prompting and meta-programmed contextual information, into a uniﬁed proof opti-\nmization agent.3 AUTOMATED PROOF OPTIMIZATION WITH ImProver\n\nAutomated Proof Optimization.Given a theorem statement x, additional context c, and an initial\nproof y0, proof optimization consists of generating a new proof y that is correct and minimizes (or\nmaximizes) a metric µ(x, c, y0, y) → R.\n\n1Code is available at https://github.com/riyazahuja/ImProver.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.Note that shorter proofs often\nrepresent more efﬁcient proofs.Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.Concretely, we evaluate this using the ratio of number of explicitly typed\nhave tactics to total number of tactic invocations.Completion Metric: The completion of a proof simply describes its correctness.This is a trivial\nmetric which measures the number of errors present.The completion metric is used for concretely\nviewing proof optimization as a generalization of neural theorem proving.3.1\n\nIMPROVER\n\nWe develop several improvements that can be applied to a black-box LLM generator yout ∼\nG(·|xin), such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these pa-\nrameters.The explicit prompts and templates that are sent to the LLM can be found in (§A).3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.The intermediate states often contain valuable information (e.g., an expression\nafter it has been simpliﬁed) that is not present in the tactics.To allow the model to reason about\nthese intermediate goals and hypotheses, we use tools from Lean metaprogramming to automatically\nannotate each proof state as a comment prior to each tactic.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.The implementation of this extraction system is mod-\neled from the work (Kim Morrison, 2024).Speciﬁcally, in the compiler’s elaboration and eval-\nuation stages – where the parsed theorem code is ﬁrst converted into concrete syntax trees (in\npractice, Syntax objects) and abstract syntax trees (Expr objects) – we convert the CST and\nAST output objects into the relevant proof data and proof states in the form of proof trees\n(Lean.Elab.InfoTree).These proof trees contain detailed context and information on a tactic-\nby-tactic level relating to the modiﬁcation of the proof state, metavariable context, and proof cor-\nrectness.After state extraction is completed and cached for efﬁcient future access, we annotate the proof text\nitself to contain the intermediate states in the form as comments.Figure 2 shows an example. This explicit reasoning aims to help the generator model construct more optimized proofs via addi-\ntional symbolic data.3.1.2 OUTPUT FORMATTING. LLM outputs often contain ancillary and syntactically invalid content, especially before and after\nthe actual proof.Additionally, by applying additional structure to the LLM outputs, we may hope to\ngenerate more structured proofs.To analyze this hypothesis, we introduce two additional output for-\nmats to the standard str output: flat and structured.The former enforces a tactic sequence\noutput as a list of strings, and the latter enforces a proof tree output as a tree of strings.3.1.3 SAMPLING METHOD\n\nWe also introduce different methods of sampling between many (sequential or parallel) LLM infer-\nence calls, involving best-of-n and iterative reﬁnement implementations, as well as combinations\nthereof.3\n\n\fWithout Chain-of-States\n\nWith Chain-of-States\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u) := by\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u)\n\n:= by\n\nrintro x (hxs, xti | hxs, xui)\n· use xs; left; exact xt\n.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!-/\n. use xs; right; exact xu\n/-\nGoals Solved!-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.More speciﬁcally, our scoring function is given by the 2-ary\ncomparison function S, whose arguments are output objects y, y′.max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.This comparison function can be extended to evaluate the best output of any ﬁnite n via induction.This best-of-n technique is implemented as a curried function such that each of the n calls can be\nhandled by any arbitrary sampling method, or just a single standard prompt at user discretion.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.The reﬁnement process relies on user-deﬁned pa-\nrameters n and prev_num to specify the number of iterations and the number of previous iteration\ninfo to forward, respectively.Each iteration carries information on the last prev_num iterations,\nincluding input, output, metric score, correctness, and error messages.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.Similar to the best-of-n tech-\nnique, it relies on an argument n for the number of reﬁnement steps, and is curried such that each\nreﬁnement step can be handled by any other prompting function.However, unlike best-of-n, there is\nno opportunity for parallelism as each iteration is dependent on information from the previous call.Combination Sampling and Compound Prompt Functions Compound prompt functions utilize\nthe curried nature of the implementations of best-of-n and reﬁnement to nest these techniques within\none another.For example:\n\n4\n\n\fbest_of_n((refinement,m),n) is a compound sampling method that run a best-of-n,\nwhere each call is a m-step reﬁnement.refinement((best_of_n,m),n) is a compound sampling method that runs a n-step reﬁne-\nment, where each call is a best-of-m call to the LLM.Note that with each of these compound prompt functions, there are always a total of mn iterations.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.More speciﬁcally, example retrieval selects the most relevant user-generated examples of proof op-\ntimization on a speciﬁc metric.Namely, each metric is loaded with a cached (vector) database\npopulated with human-made examples of preoptimized and postoptimized pairs of Lean theorems.The number of examples that are retrieved is user-speciﬁed.Document retrieval extracts information using MMR from a pair of ﬁxed (vector) databases.The\ndatabases store semantically chunked data from the Theorem Proving in Lean (TPiL) handbook –\ncontaining syntax guides and tactic explanations – and the Mathlib mathematics libary – containing\nthousands of theorems and lemmas.The chunking is handled by a recursive character splitter, which\nsplits the TPiL markdown ﬁles at on its headers and Mathlib ﬁles at the start of theorems, examples,\nlemmas, and deﬁnitions – with chunk sizes of 1000 characters with a 200 character overlap.The Mathlib retriever ﬁnds the top k documents that score the highest MMR score against the current\ntheorem, the TPiL retriever ﬁnds the top k documents that score the highest MMR score against the\ncurrent theorem in context and all current error messages.This retrieval process helps in generating\nmore contextually accurate prompts that allow the language model to better correct its own errors as\nwell as ﬁnd useful lemmas to reference.4 EXPERIMENTS\n\nWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics and compare its results to those of the base GPT-4o and GPT-4o-mini\nmodels.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.4.1 SETUP\n\nOur experimentation is split into three distinct stages.We ﬁrst perform ablation testing on the\nImProver model parameters (§3.1) to ensure that ImProver’s parameter speciﬁcation is the optimal\none with respect to correctness and metric optimization score.We then evaluate this optimal param-\neter combination on datasets of varying complexity and analyze the performance and results thereof.Lastly, we note the performance of ImProver in NTP applications in comparison to the base GPT-4o\nand GPT-4o-mini models.Datasets. We evaluate ImProver on subsets of the following datasets.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.We use exercise solutions from set theory, elementary number theory, group\ntheory, topology, differential calculus, and integration & measure theory.This dataset contains the-\norems at an undergraduate-level of complexity.For our main results, we evaluated on 72 theorems\nfrom exercise solutions from MIL chapters 4, 5, 8, 9, and 10.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.This is a\ndataset of internationally-renowned competitive math problems, many of which are readable, yet\nquite verbose.This dataset contains theorems of a competitive format, and although they contain\n\n5\n\n\fconcepts only at a high-school level, the logical complexity of internationally-renowned competi-\ntion results is far above that.For our main results, we used all 26 theorems and lemmas from the\nCompﬁles database of complete solutions to the International Mathematics Olympiad (IMO) and\nthe American Mathematics Olympiad (USAMO) from 2016-2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.These proofs are extremely\nefﬁcient, concise, and generalized - which often comes at the cost of readability and understandabil-\nity.These results and theorems often are at the cutting edge of research.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.This is the most difﬁcult dataset. Models.Our base generator uses GPT-4o (OpenAI et al., 2024).Since no prior methods currently\nexist for automated proof optimization, we consider a prompted GPT-4o without the improvements\ndescribed in (§3.1) as our baseline.Additionally, for a given metric, we write a prompt that brieﬂy\ndescribes the metric and the proof optimization task.We also provide instructions, context, and in-\nformation depending on the features selected, and add the theorem and proof to the prompt.Speciﬁc\nprompt information is detailed in (§A)\n\nPerformance metrics.Since proof optimization is a new task, we deﬁne four performance metrics\nfor measuring aspects of correctness and improvement.First, we deﬁne improvement for length as percentage change in length, µlen(y0)−µlen(y)\n× 100.For\nreadability, we use the difference, µread(y)−µread(yo).If no correct output is generated by the model\nfor a speciﬁc theorem, improvement is deﬁned to be zero.We deﬁne nonempty improvement as\nthe improvement restricted to theorems for which some output has nonzero improvement.µlen(y0)\n\nIntuitively, improvement is the expected improvement in metric score from the input to output, ac-\ncounting for errors in the generation.The nonempty improvement score is the expected improvement\nin metric score, given that there are no errors in the generation.Similar improvement scores can be\ndeﬁned for other metrics using a binary function of the metric assigned to the original and optimized\nproofs.Additionally, the accuracy is the percentage of theorems in the dataset which the model was able to\ngenerate a correct output for.The improved accuracy is the percentage of theorems in the dataset\nwhich the model was able to generate a correct output for, as well as improve the metric to be\nnonzero.4.1.1 ABLATIONS\n\nWhen performing our ablation studies, we used a ﬁxed dataset (MIL) and metric (length) and varied\nthe parameters of all the features to ﬁnd the optimal combination.However, as there are over 8640\npossible combinations, it is inefﬁcient to test all combinations at once.As such, we evaluate using a\nfactorial testing method. Testing Groups.We deﬁne the following testing groups with the speciﬁed parameter combina-\ntions:\n\nGPT-4o-mini/GPT-4o: This varies the GPT-4o model, outputting a string with no other features.Output and CoS: We evaluate the effects of different output formatting styles (string, string\nlist, string tree) and CoS (True, False), with the model ﬁxed as GPT-4o, with no other\nfeatures enabled.Example Retrieval: We evaluate the effects of increasing the number of examples provided (multi-\nshot prompting) in the range of 0, 3, 5, 7, and 10, with the model ﬁxed as GPT-4o, CoS and output\nformatting ﬁxed as the best combination from the previous test, and no other features enabled.Sampling Method: Here, we evaluate the effects of best-of-n and reﬁnement for a ﬁxed n = 5.Additionally we test on the reﬁnement cases if forwarding the most recent iteration result, or all\nprevious iteration results is the best, and if we should keep the best out of the iterations, or the most\n\n6\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 1: Average Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nMetric\n\nModel\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n3.7\n20.96\n\n2.21\n9.34\n\n6.25\n30.54\n\n4.18\n13.45\n\n15.15\n55.29\n\n8.02\n30.53\n\n18.58\n56.56\n\n14.48\n30.97\n\n26.36%\n100.0%\n\n18.75%\n100.0%\n\n8.31%\n35.44%\n\n6.13 %\n24.56%\n\n37.5%\n100.0%\n\n28.85%\n100.0%\n\n14.42%\n50.0%\n\n11.54%\n34.21%\n\nTable 2: MIL Proof optimization results.Improvement Nonempty Improvement Accuracy\n\nImproved Acc. recent.The model is ﬁxed as GPT-4o, CoS, output formatting, and examples are ﬁxed as the best\ncombination from the previous test, and no other features enabled.n and Model: Here, we evaluate the effects of larger n values and different models.We test n =\n3, 5, 7, 10, 15 on GPT-4o and GPT-4o-mini, as well as n = 20 for GPT-4o-mini (as it is of a far\nlower token cost).CoS, output formatting, examples, and sampling method are ﬁxed as the best\ncombination from the previous test, and no other features enabled.Combos and RAG: We evaluate combination methods refinement(best_of_m',m) and\nbest_of_m'(refinement(m)), for m 6= m′ with mm′ equal to the optimal value m from\nthe previous test.We also test the effect of enabling document retrieval.Model, CoS, output for-\nmatting, examples, n, and sampling method are ﬁxed as the best combination from the previous\ntest.Ablation data. We evaluate our ablations on a subset of MIL.However, due to the increase in\nmodel calls for larger n values, we switch a representative sample of this subset for some test groups.Namely,\n\nGPT-4o-mini, GPT-4o, Output and Cos, Example Retrieval, and Sampling Method are tested\non the 133 theorems in the solutions of C03_Logic, C04_Sets_and_Functions, and\nC05_Elementary_Number_Theory.n and Model are tested on 55 theorems from a representative sample of the aforementioned, and\nCombos and RAG are tested on a representative sample of 32 theorems from the aforementioned.4.2 RESULTS\n\nImProver is capable of optimizing proofs in all settings.From Table 2, Table 3, and Table 4,\nwe can see that ImProver is capable of optimizing proofs on all datasets for both the length and\nreadability metrics.Furthermore, Table 1 shows that across all metrics, ImProver signiﬁcantly\noutperforms GPT-4o on proof optimization tasks on every experimental measure – aggregated from\nall datasets.Additionally, from Table 2, Table 3, and Table 4, we can see that ImProver outperforms\nGPT-4o on each dataset as well.We proceed to analyze this data and its implications. Length optimization.First focusing on the length metric, we see that ImProver outperforms GPT-\n4o with respect to the improvement score by 566% (aggregated over all datasets).Additionally, we\nare guaranteed that ImProver produces a correct output, although that output may just be the same\nas the input.However, 35.44% of the time, it generates a correct output that is not the same length\nas the input, and in that case, we expect an average of a 55.29% reduction in length.Comparing\nthis with GPT-4o, we conclude that not only can ImProver optimize at a higher level on arbitrary\ntheorems, but its ability to generate nontrivial correct outputs is far greater in comparison to GPT-4o.7\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 3: Compﬁles Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n2.75\n18.86\n\n0.39\n5.74\n\n0.0\n6.19\n\n0.0\n4.63\n\n30.7\n54.48\n\n3.38\n24.89\n\n0.0\n53.65\n\n0.0\n33.19\n\n11.54%\n100.0%\n\n14.1%\n100.0%\n\n5.13%\n34.62%\n\n1.28%\n19.23%\n\n16.67%\n100.0%\n\n4.65%\n100.0%\n\n0.0%\n11.54%\n\n0.0%\n11.63%\n\nMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 4: Mathlib Proof optimization results. Readability optimization.Readability optimization is similar, with ImProver outperforming\nGPT-4o by 423%.Moreover, the accuracy, improved accuracy, and nonempty improvement dis-\nparities for readability parallel those of the length tests.However, it should be noted that for both\nGPT-4o and ImProver, the accuracy and improved accuracy scores were markedly smaller for read-\nability than length optimization.This suggests that for both models, it was generally more “difﬁcult”\nto generate a correct output, and moreover, generate a correct output with a better metric score\nthan the input, for readability optimization than length optimization.In other words, optimizing for\nreadability is more difﬁcult for the underlying generator than optimizing for length.However, we\nspeculate with higher-quality prompts, descriptions of the metric, and examples, this disparity can\nbe minimized.Regardless, we note that different metrics can be less likely to be correctly optimized,\nand that model performance is correlated with the metric it seeks to optimize – both for GPT-4o and\nImProver.Optimization varies based on dataset difﬁculty.Additionally noting Table 2, Table 3, and\nTable 4, we observe that the improvement score for both metrics for both GPT-4o and ImProver\nis highest for the MIL dataset, lower for Compﬁles, and the lowest on the Mathlib theorems.This\nsuggests that the expected improvement in metric score decreases with higher difﬁcultly – with\nundergraduate-level theorems having a signiﬁcantly higher expected improvement than research-\nlevel theorems.However, it should be noted that for both metrics, the nonempty improvement of\nImProver stayed consistent, whereas for GPT-4o, it followed the aforementioned trend of decreas-\ning with difﬁculty.Similarly, the accuracy and improved accuracy scores for both metrics and\nmodels decreased with higher difﬁculty datasets (disregarding ImProver’s accuracy scores, as they\nare ensured to be 100%).This suggests that although the base GPT-4o generator is less likely to\ngenerate a correct output for higher difﬁculty datasets, the improvements that ImProver makes to\nthe base generator allows it to maintain its improvement in the metric score whenever a correct out-\nput is generated.As such, we can speculate that the bottleneck in the improvement score is not the\nmodel’s ability to optimize the proof for a metric, but rather its ability to generate a new correct\nproof at all.As such, we conjecture that with more capable generator models, the accuracy – and\nthus, the improvement score – in optimization tasks will continue to increase, until the improvement\nscores match the nonempty improvement.Overall, we conclude that although the performance of both ImProver and GPT-4o decreases on\nlength and readability optimization on more difﬁcult datasets, ImProver signiﬁcantly outperforms\nGPT-4o on all datasets for length and readability optimization.4.2.1 ABLATION TESTING\n\nWe perform ablation studies using a subset of the MIL dataset as discussed in §4.1.1.The results\nof this factorial study are aggregated in Table 5.We measure the baseline results from the GPT-\n4o and GPT-4o-mini models, noting that GPT-4o is the better-scoring model (with respect to the\nimprovement score).Thus, ﬁxing this model, we vary the output formatting type and if CoS is\nenabled, and determine that outputting flat with CoS enabled maximizes the improvement score.8\n\n\fTable 5: Ablation results.Each cell in the ablation tests shows best / worst, which are the best\nand worst parameter combinations in the test group.The ImProver speciﬁcation outputs the input\ntheorem when no correct proof is generated, which results in an accuracy of 100% on MIL.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o-mini\nGPT-4o\n+ Output and CoS\n+ Example Retrieval\n+ Sampling Method\n+ n and Model\n+ Combos and RAG\nImProver\n\n0\n7.03\n8.04 / 6.31\n9.34 / 5.67\n15.35 / 9.34\n23.51 / 3.65\n34.88 / 28.25\n\n0\n19.67\n12.38 / 14.17\n14.7 / 8.44\n18.44 / 14.7\n26.28 / 4.63\n57.56 / 33.48\n\n3.62%\n35.77%\n\n0%\n15.33%\n\n64.96% / 44.53% 21.17% / 16.06%\n63.5% / 67.15%\n21.9% / 16.79%\n36.5% / 21.9%\n83.21% / 63.5%\n89.47% / 78.95% 45.61% / 8.77%\n60.61% / 84.38% 54.55% / 53.12%\n\n34.88\n\n57.56\n\n100%\n\n54.55%\n\nTable 6: CoS Readability Ablation results.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o\nImProver, CoS Disabled\nImProver\n\n4.97\n9.23\n16.69\n\n15.89\n24.61\n31.42\n\n37.5%\n100.0%\n100.0%\n\n12.5%\n28.12%\n46.88%\n\nFixing these parameters, we now vary the number of examples retrieved, noting that prompting\nwith 10 examples maximizes the improvement score.Fixing this parameter, we vary the sampling\nmethods (excluding compound methods and ﬁxing n = 5) and observe that best-of-n is the best\nparameter combination.Now, as GPT-4o-mini is signiﬁcantly less computationally expensive than\nits GPT-4o counterpart, we test both models with the sample method ﬁxed to best-of-n, and vary\nn = 1, 3, 5, 7, 10, 15, and for GPT-4o-mini, also n = 20.We conclude that GPT-4o with n = 15\nis the most effective.Fixing these parameters, we consider all mixed compound sampling methods\nwith and without document retrieval enabled, concluding that a 5-step reﬁnement with best-of-3 on\neach iteration, with RAG enabled, is the optimal combination.Thus, as we can see from Table 5, the optimal parameter combination comes from gpt-4o outputting\nas a string list with CoS, RAG, 10 examples, 5-step reﬁnement with each iteration being a\nbest-of-3 evaluation.Changing any one of these parameters them leads to a reduction in performance.Additional ablation data can be found at (§B.1).Readability and Chain-of-States (CoS) Ablation.We additionally examine the effects of dis-\nabling CoS on readability optimization tasks, as the previous study focused on length optimization\ntasks, and we speculate that CoS has a high impact on the performance of readability optimization\ntasks, as the proof states that are embedded due to CoS seem to be a critical aspect to generating the\nexplicit declarations that the readability metric measures.We conﬁrm this result by considering Table 6 and observe that simply enabling CoS nearly doubles\nthe improvement score, and signiﬁcantly improves the nonempty improvement score, suggesting that\nCoS has a high degree of impact on optimizing for the readability metric, as conjectured.However,\nwe also note a signiﬁcant increase in improved accuracy, which suggests that embedding the chain\nof states also improves the ability of the model to generate nontrivial correct outputs, implying that\nthe symbolic information contained in the states are critical to effectively modifying the structure\nand content of a proof.4.2.2 NEURAL THEOREM PROVING EVALUATION\n\nWe evaluate ImProver’s neural theorem proving (NTP) performance using the completion metric.We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input\nproof.Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-4o.ImProver substantially outperforms GPT-4o across all topics, with an 80% increase in accuracy\ncompared to the base model, showing that proof optimization systems are easily extendable to NTP\nsystems.9\n\n\fTable 7: Proof generation results.Each cell shows percent accuracy.MIL\n\nSet Theory Group Theory\n\nOverall\n\nGPT-4o\nImProver\n\n18.18%\n45.45%\n\n25%\n33.33%\n\n21.73%\n39.13%\n\n4.3 QUALITATIVE RESULTS\n\nNext, we discuss qualitative examples showing the improvements from ImProver in proof optimiza-\ntion.These examples show the balance between correctness and the desired optimization metric,\nshowing how ImProver can produce more concise or readable proofs depending on the use case.Additional examples can be found in (§B.2)\n\nExample 1: Compﬁles Optimization.Consider Figure 1, a lemma from the 2022 IMO Question\n2 (Compﬁles) that we optimize for length.The original proof consisted of 12 tactic steps and multiple intermediate calculations.After applying\nImProver for length optimization, the proof was halved to only 6 tactics, eliminating redundant steps\nwhile maintaining correctness.In comparison, the shortened proof makes multiple nontrivial optimizations, such as eliminating the\nh2’ and h4 and hxw hypotheses, as well as fully generating proof terms for speciﬁc rewrites and\nother tactics.Example 2: MIL. Consider Figure 3, a result from MIL that we optimize for readability.Original (human-written)\n\nImProver (readability-optimized)\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ncard G = card H * card K)\n\n: K ≃* G / H := by\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\n· rw [← ker_eq_bot_iff, (QuotientGroup.mk'\n\nH).ker_restrict K]\nsimp [h]\n\n· symm\n\nexact aux_card_eq h'\n\ncard G = card H * card K)\n:= by\n\n: K ≃* G / H\n\nhave injectivity : Function.Injective\n\n((QuotientGroup.mk' H).restrict K) := by\nrw [← ker_eq_bot_iff, (QuotientGroup.mk'\nH).ker_restrict K]\nsimp [h]\n\nhave card_eq : card (G / H) = card K := by\n\nexact aux_card_eq h'\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\nexact injectivity\nsymm\nexact card_eq\n\nFigure 3: Optimizing a group-theoretic result from MIL Chapter 8 Section 1 for readability.We\ndeﬁne a proof to be readable if it is written in a declarative style, which consists of intermediate\nconjectures (have · · · statements).ImProver introduces two intermediate conjectures into the proof.This original proof carried a score of 0, as it does not contain any declarative statements.In compar-\nison, after applying ImProver, we transformed the proof to be more declarative, with many more\nintermediate steps with explicit have tactics for improved clarity.Additionally observe how the\nmodel deﬁnes hypotheses for use in the latter half of the proof; these predeﬁned hypotheses could\neasily be converted into standalone lemmas for reuse.Example 3: Full Proof Generation.We analyze the application of ImProver to neural theorem\nproving in the MIL example from Figure 4.This theorem relating to group theory originally has no proof, however, ImProver generates one\nfrom scratch.This generated proof is veriﬁed to be correct by Lean, utilizing all the included hy-\npotheses as well as a retrieved mathlib theorem.10\n\n\fOriginal (human-written)\n\nImProver (completeness-optimized)\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n: S ≤ T) : comap ϕ S ≤ comap ϕ T := by\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n\n: S ≤ T) : comap ϕ S ≤ comap ϕ T\n\n:= by\n\nsorry\n\nintro g\nsimp only [mem_comap]\nintro hS\nexact hST hS\n\nFigure 4: Solving a group theorem exercise from MIL Chapter 8 Section 1 for readability.5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.By incorporating CoS, RAG, and other features, ImProver signiﬁcantly outperforms base\nlanguage models in proof optimization over undergraduate, competition, and research-level prob-\nlems.However, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance\non black-box LLM’s.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.As such, we believe that ImProver sets the stage for further\nwork on proof optimization to advance the study and use of AI in mathematics.ACKNOWLEDGEMENTS\n\nRiyaz Ahuja thanks the L3 Lab and Hoskinson Center for Formal Mathematics for their support.Sean Welleck thanks Convergent Research, the Lean FRO, and the OpenAI Researcher Access Pro-\ngram for their support.11\n\n\fREFERENCES\n\nand\n\nAlphaGeometry\n\nAlphaProof\nstandard\nhttps://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\n2024.\n\nsilver-medal\nproblems.achieves\nolympiad\n\nmathematical\n\ninternational\n\nsolving\n\nTeams.AI\n\nSerge Autexier and Dominik Dietrich.A tactic language for declarative proofs.In Matt Kaufmann\nand Lawrence C. Paulson (eds.), Interactive Theorem Proving, pp.99–114, Berlin, Heidelberg,\n2010. Springer Berlin Heidelberg.Jaime Carbonell and Jade Goldstein.The use of mmr, diversity-based reranking for reordering doc-\numents and producing summaries.In Proceedings of the 21st Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ’98, pp.335–336,\nISBN 1581130155. doi:\nNew York, NY, USA, 1998.Association for Computing Machinery. 10.1145/290941.291025.URL https://doi.org/10.1145/290941.291025.Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.Teaching large language models\n\nto self-debug, 2023.URL https://arxiv.org/abs/2304.05128. David Renshaw. compﬁles.https://github.com/dwrensha/compfiles, 2024.Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun.Baldur: Whole-proof generation and\n\nrepair with large language models, 2023.Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu.Proof artifact co-\nIn International Conference on Learning\n\ntraining for theorem proving with language models.Representations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.Jiewen Hu, Thomas Zhu, and Sean Welleck.minictx: Neural theorem proving with (long-)contexts,\n\n2024.URL https://arxiv.org/abs/2408.03350.Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu.Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs.In The Eleventh International Conference on Learning\nRepresentations, 2023.URL https://openreview.net/forum?id=SMa9EAovKMC.Kim Morrison. lean-training-data.https://github.com/kim-em/lean-training-data,\n\nGuillaume Lample, Timothee Lacroix, Marie anne Lachaux, Aurelien Rodriguez, Amaury\nHypertree proof search\nHayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.for neural\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=J4pX8Q8cxHH. theorem proving. 2024. 2024.Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie\n\nSi.A survey on deep learning for theorem proving, 2024.Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang.A survey of deep\nlearning for mathematical reasoning.In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp.14605–14631, Toronto, Canada, July 2023.Association for Computational Linguistics.10.18653/v1/2023.acl-long.817.URL\nhttps://aclanthology.org/2023.acl-long.817.doi:\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-reﬁne:\nIterative reﬁnement with self-feedback.In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.URL https://openreview.net/forum?id=S37hOerQLB.12\n\nleanprover-community. mathematics_in_lean.https://github.com/leanprover-community/mathematics_in_lea\n\n\fThe mathlib Community.The lean mathematical library.In Proceedings of the 9th ACM SIGPLAN\nInternational Conference on Certiﬁed Programs and Proofs, POPL ’20.ACM, January 2020. doi:\n10.1145/3372885.3373824.URL http://dx.doi.org/10.1145/3372885.3373824.Leonardo de Moura and Sebastian Ullrich.The lean 4 theorem prover and programming lan-\nIn Automated Deduction – CADE 28: 28th International Conference on Automated\nguage.Deduction, Virtual Event, July 12–15, 2021, Proceedings, pp.625–635, Berlin, Heidelberg,\n2021. Springer-Verlag.ISBN 978-3-030-79875-8. doi: 10.1007/978-3-030-79876-5_37.URL\nhttps://doi.org/10.1007/978-3-030-79876-5_37.OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red\nAvila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Moham-\nmad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher\nBerner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brit-\ntany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek\nChen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,\nHyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas\nDegry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,\nAdrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada\nFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel,\nTarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff\nHarris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jo-\nmoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,\nNitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik\nKim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, An-\ndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,\nStephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim\nMalfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew\nMayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil,\nDavid Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin,\nVinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeon-\nwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley\nPantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov,\nAndrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde\nde Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea\nPower, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick\nRyder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David\nSchnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah\nShoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama,\nIan Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie\nSummers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin\nTootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón\nUribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welin-\nder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,\nHannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah\nYoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,\nShengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.Gpt-4 technical\nreport, 2024. URL https://arxiv.org/abs/2303.08774.Stanislas Polu and Ilya Sutskever.Generative language modeling for automated theorem proving,\n\n2020.13\n\n\fStanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\n\nSutskever.Formal mathematics statement curriculum learning, 2022.Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, and Swarat Chaudhuri.An in-\n\ncontext learning agent for formal theorem-proving, 2024.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H.\nthought prompting elicits reasoning\nChi, Quoc V Le, and Denny Zhou.in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=_VjQlMeSB_J.Chain of\n\nFreek Wiedijk. Formal proof sketches.In Stefano Berardi, Mario Coppo, and Ferruccio Damiani\n(eds.), Types for Proofs and Programs, pp.378–393, Berlin, Heidelberg, 2004.Springer Berlin\nHeidelberg. ISBN 978-3-540-24849-1.Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil,\nRyan Prenger, and Anima Anandkumar.LeanDojo: Theorem proving with retrieval-augmented\nlanguage models.In Neural Information Processing Systems (NeurIPS), 2023.Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig.Docprompting: Gener-\nating code by retrieving the docs.In The Eleventh International Conference on Learning Repre-\nsentations, 2023.URL https://openreview.net/forum?id=ZTCxT2t2Ru.14\n\n\fA PROMPTS\n\nA.1 TEMPLATE\n\nIn this appendix, we note the prompts used by ImProver both for general LLM prompting, as well\nas the metric-speciﬁc prompts.For the main prompt sent to the LLM on each sample, we build a prompt string using a chat prompt\ntemplate that is then invoked at runtime to ﬁll in the variables.Namely, these variables include the set of metric prompts, previous results, input theorem, context,\na syntax documents, Mathlib documents, and examples.The prompt template is a conversation of the format:\n\nPlaceholder: All metric prompts with a ‘System’ role\nSystem: You will be given the proof context (i.e.the lean ﬁle contents/imports leading up\nto the theorem declaration) wrapped by <CONTEXT>...</CONTEXT>.You will be given the previous num_prev input/output pairs as well as their metric (met-\nric.name) score and correctness score, as well as any error messages, for your reference to\nimprove upon.Each of these previous results will be wrapped with <PREV I=0></PREV\nI=0>,...,<PREV I=num_prev-1></PREV I=num_prev-1>, with I=num_prev-1 being the\nmost recent result.Remember to use lean 4 syntax, which has signiﬁcant changes from the lean 3 syntax.To\nassist with the syntax relating to the current theorem and current error messages, you will\nbe given num_syntax_docs documents to refer to for ﬁxing these syntax issues.Each of\nthese documents will be wrapped with <SYNTAX_DOC>...</SYNTAX_DOC>.You will also receive num_mathlib_docs documents relevant to the current theorem to\nhelp with formulating your modiﬁed proof.Each of these will be wrapped with <CON-\nTENT_DOC>...</CONTENT_DOC>\nYou will also receive num_examples examples of input-output pairs of proofs that\nwere optimized for the metric metric.Each of these will be wrapped with <EXAM-\nPLE>...</EXAMPLE>\nYou will be given the tactic states as comments for reference.The current theorem will be\nwrapped in <CURRENT>...</CURRENT>\nSystem: Output format instructions\nPlaceholder: All retrieved syntax documentation\nPlaceholder: All retrieved mathlib documentation\nPlaceholder: All retrieved examples\nUser: <CONTEXT> context </CONTEXT>\nPlaceholder: Previous results and inputs/outputs\nPlaceholder: All metric prompts with a ‘User’ role\nUser: <CURRENT> theorem </CURRENT>\n\nThis prompt is then invoked and sent to the language model by ﬁlling in all the variables and place-\nholders.Notably, when we invoke the chain given by chain|llm|parser, we throttle the invoca-\ntion with a randomized exponential rate limit throttling to account for API rate limits, especially in\nhighly-parallelized requests like when benchmarking over a large number of theorems.A.2 METRIC PROMPTS\n\nLength Metric\n\nSystem: You are an AI assistant who shortens Lean 4 proofs while ensuring their correct-\nness.You will aim to reduce the number of lines of the tactic proof while ensuring that it\nproperly compiles in Lean 4.15\n\n\fUser: Shorten the current theorem (wrapped in <CURRENT>...</CURRENT>) to be as\nshort in length—measured in the number of lines of the proof—as possible, while also\nensuring that the output is still syntactically correct.\"Readability Metric\n\nCompletion Metric\n\nSystem: You are an AI assistant who rewrites Lean 4 proofs to be more readable while\nensuring their correctness.We measure readablity by considering the ratio of the number\nofexplicitly typed have tactics against the total number of tactics in the proof, as this is\nproportional to whether a proof is declarative in style, and thus, readable.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is\nmore readable and declarative and modular.System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.You will receive a Lean 4 proof you must\nmodify to eliminate any errors so that it compiles correctly and eliminate any “sorry”s with\nfull proofs.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a\nformal, complete, and correct Lean 4 proof by ﬁlling in its tactic proof.B ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this section, we provide more detailed information on the experimental setup and results used to\nevaluate ImProver.B.1 ABLATION DETAILS\n\nWe now proceed to show detailed results from our ablation testing.Table 8: Output and Chain-of-States Ablations\n\nOutput Format\n\nCoS\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.string\nTrue\nstring\nFalse\nstring list True\nstring list\nFalse\nstring tree\nTrue\nstring tree\nFalse\n\n7.53\n7.03\n8.04\n7.04\n7.62\n6.31\n\n16.12\n19.67\n12.38\n13.58\n15.34\n14.17\n\n46.72%\n35.77%\n64.96%\n51.82%\n49.64%\n44.53%\n\n16.79%\n15.33%\n21.17%\n18.98%\n18.25%\n16.06%\n\nBy Table 8, we see that the optimal combination in this testing group is a string list output\nformat with CoS enabled.Fix these values for all future tests.Examples\n\nTable 9: Example Retrieval Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.0\n3\n5\n7\n10\n\n5.67\n8.49\n8.38\n7.56\n9.34\n\n67.15%\n62.04%\n64.96%\n62.77%\n63.5%\n\n16.79%\n19.71%\n21.17%\n19.71%\n21.9%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the number of examples.By Table 9,\nwe see that the optimal combination in this testing group is 10 examples.Fix this value for all future\ntests.8.44\n13.68\n12.9\n12.04\n14.7\n\n16\n\n\fTable 10: Sampling Method Ablations\n\nMethod\n\nForward Keep Best\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.None\nreﬁnement\nreﬁnement\nreﬁnement\nreﬁnement\nbest-of-n\n\nN/A\n1\n5\n1\n5\nN/A\n\nN/A\nFalse\nFalse\nTrue\nTrue\nN/A\n\n9.34\n14.76\n12.5\n14.95\n13.15\n15.35\n\n14.7\n30.63\n20.88\n14.95\n13.15\n18.44\n\n63.5%\n48.18%\n59.85%\n100.0%\n100.0%\n83.21%\n\n21.9%\n30.66%\n30.66%\n30.66%\n29.93%\n36.5%\n\nNote that forward and keep-best values are parameters for reﬁnement of how many previous itera-\ntions to forward, and whether to keep the most recent or the best iteration in subsequent reﬁnement\nsteps.Now, with the previous optimal parameters ﬁxed, run the ablation on the sample method.By\nTable 10, we see that the optimal combination in this testing group is best-of-n.Fix this value\nfor all future tests.Table 11: Model and n Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.Model\n\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\n\nn\n\n3\n5\n7\n10\n15\n3\n5\n7\n10\n15\n20\n\n19.66\n20.12\n22.44\n21.73\n23.51\n3.65\n5.12\n3.65\n4.99\n4.35\n4.87\n\n24.36\n24.97\n27.21\n25.28\n26.28\n4.63\n6.21\n4.34\n5.69\n5.06\n5.56\n\n80.7%\n80.56%\n82.46%\n85.96%\n89.47%\n78.95%\n82.46%\n84.21%\n87.72%\n85.96%\n87.72%\n\n38.6%\n36.11%\n42.11%\n40.35%\n45.61%\n8.77%\n10.53%\n8.77%\n12.28%\n12.28%\n14.04%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the value of n and model.By\nTable 11, we see that the optimal combination in this testing group is GPT-4o with n = 15.Fix this\nvalue for all future tests.Table 12: RAG and Combination Sampling Method Ablations\n\nRAG Improvement Nonempty Improve. Accuracy\n\nImproved Acc.Combination\n\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nbest-of-n\nbest-of-n\n\nm\n\nm′\n\n5\n3\n5\n3\n3\n5\n3\n5\n5\n3\n5\n3\n3\n5\n5\n3\nN/A 15\nN/A 15\n\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n33.78\n31.23\n31.85\n31.35\n51.32\n50.1\n57.56\n49.75\n32.71\n33.48\n\n100.0%\n100.0%\n100.0%\n100.0%\n63.64%\n65.62%\n60.61%\n59.38%\n90.62%\n84.38%\n\n50.0%\n46.88%\n50.0%\n50.0%\n48.48%\n53.12%\n54.55%\n43.75%\n56.25%\n53.12%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the combination methods and if\nRAG is enabled.By Table 12, we see that the optimal combination in this testing group is a 5-step\nreﬁnement with each iteration being a best-of-3 call, with RAG enabled.B.2 ADDITIONAL QUALITATIVE EXAMPLES\n\nIn this section, we provide qualitative examples demonstrating the improvements ImProver achieves\nin proof optimization.33.78\n31.23\n31.85\n31.35\n32.66\n32.88\n34.88\n29.54\n29.64\n28.25\n\n17\n\n\fCompﬁles: Length Optimization See (§4.3)\n\nCompﬁles: Readability Optimization Consider Figure 5, in which a lemma from the 2019 IMO\nproblem 1 (from the Compﬁles dataset) is optimized for readability.This introduces multiple new\nhypotheses, which generalize a linear_property of the functions, and then reuses and instanti-\nates that (and others, too) hypothesis throughout the proof, creating a signiﬁcantly more declaritive,\nmodular, and therefore readable proof.Original (human-written)\n\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\nrefine hf 1, fun a => ?_i\nchange g a = g 1 * a\nrw [mul_comm, ← smul_eq_mul, ←\n\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nImProver (readability-optimized)\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\n\nhave linear_property : ∀ a, f a = g a := by\n\nintro a\nrfl\n\nhave g_smul : ∀ a, g a = g 1 * a := by\n\nintro a\nrw [mul_comm, ← smul_eq_mul, ←\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nrefine hf 1, fun a => ?_i\nhave f_eq_g : f a = g a := linear_property a\nhave g_a_eq : g a = g 1 * a := g_smul a\nrw [f_eq_g, linear_property 1, g_a_eq]\n\nFigure 5: Optimizing a lemma from IMO 2019 P1 for readability\n\nMIL: Length Optimization Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, eliminating simp calls and introducing proof terms\ninto the structure of the proof to shorten it from 9 tactic invocations to 7.Original (human-written)\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\n\n:= by\n\next x\nsimp only [mem_map]\nconstructor\n· rintro hy, y_in, hyi\n\nexact hϕ y, hy, y_in, rfli, hyi\n\n· rintro hy, hz, z_in, hzi, hyi\n\nuse z, z_in\ncalc ψ.comp ϕ z = ψ (ϕ z) := rfl\n_\n= ψ y := by congr\n\nImProver (length-optimized)\n\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\nby\next x\nsimp only [mem_map]\nconstructor\nrintro hy, y_in, hyi; exact hϕ y, hy, y_in,\n\n:=\n\nrfli, hyi\n\nrintro hy, hz, z_in, hzi, hyi; exact hz,\nz_in, (congr_arg ψ hz).trans hyi\n\nFigure 6: Optimizing a lemma from the solutions of MIL CH08 S01 for length\n\nMIL: Length Optimization 2 Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, converting a full tactic proof into a single proof term\nto shorten it from 28 tactic invocations to 1.Note that the model does not have access to the Lean\ncommands that symbolically generate proof terms, and therefore generates and estimates the proof\nterm entirely by itself.MIL: Readability Optimization See (§4.3)\n\nMathlib: Length Optimization Consider Figure 8, which optimizes a theorem in algebraic topol-\nogy from mathlib for length, eliminating simp calls and combining tactics to shorten it from 3 tactic\ninvocations to 1.Mathlib: Readability Optimization Consider Figure 9, a theorem from Mathlib that we optimize\nfor readability.This original proof carried a score of 0, as it does not contain any declarative statements.It is concise\nand efﬁcient, however, it is difﬁcult to understand and read.18\n\n\fOriginal (human-written)\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\next x; constructor\n· rintro (hxs, xnti | hxt, xnsi)\n\nImProver (length-optimized)\n\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\nexact Set.ext fun x => hfun h => h.elim\n\n(fun hxs, xnti => hOr.inl xs, fun h_, xti =\n> xnt xti) (fun hxt, xnsi => hOr.inr xt,\nfun hxs, _i => xns xsi),\n\nfun hh, nxsti => h.elim (fun xs => Or.inl h\nxs, fun xt => nxst hxs, xtii) (fun xt =>\nOr.inr hxt, fun xs => nxst hxs, xtii)i\n\n· constructor\n\nleft\nexact xs\nrintro h_, xti\ncontradiction\n\n.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.This intermediate tactic greatly helps in the understandability and clarity of the proof.Original (human-written)\n\nImProver (readability-optimized)\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n:=\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]; rfl\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\n:=\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nhave h1 : ∀ x y : Y, Subsingleton\n\n(Path.Homotopic.Quotient x y) ↔ ∀ {p1 p2\n: Path x y}, Path.Homotopic p1 p2 := by\nintro x y\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]\nrfl\n\nsimp only [h1]\n\nFigure 9: Optimizing a theorem from Mathlib/FundamentalGroupoid/SimplyConnected\nfor readability\n\n19",
          "time": 902.888908624649,
          "input_token_size": 16876,
          "output_token_size": 200,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_0920f3ef",
        "created_at": 1749869923.2757013,
        "processing_time": 911.9451098442078,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.824379"
      }
    }
  },
  {
    "0dd0cacb": {
      "question": "What are the primary metrics used to evaluate ImProver's performance?",
      "answer": "1. Length Metric\n2. Readability Metric\n3. Completion Metric",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            19,
            27,
            38,
            40,
            44,
            45,
            47,
            50,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            79,
            80,
            81,
            82,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            92,
            93,
            96,
            97,
            98,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114
          ],
          "provenance": "We develop\nvarious improvements that can be applied on top of a black-box language model, including Chain-\nof-States prompting–an analogy to chain-of-thought prompting (Wei et al., 2022) that shows inter-\nmediate proof states–along with error-correction and retrieval.A typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs\nsuch as Lean’s Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024).We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.The completion metric is used for concretely\nviewing proof optimization as a generalization of neural theorem proving.3.1\n\nIMPROVER\n\nWe develop several improvements that can be applied to a black-box LLM generator yout ∼\nG(·|xin), such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these pa-\nrameters.3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.More speciﬁcally, our scoring function is given by the 2-ary\ncomparison function S, whose arguments are output objects y, y′.max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.This comparison function can be extended to evaluate the best output of any ﬁnite n via induction.This best-of-n technique is implemented as a curried function such that each of the n calls can be\nhandled by any arbitrary sampling method, or just a single standard prompt at user discretion.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.The reﬁnement process relies on user-deﬁned pa-\nrameters n and prev_num to specify the number of iterations and the number of previous iteration\ninfo to forward, respectively.Each iteration carries information on the last prev_num iterations,\nincluding input, output, metric score, correctness, and error messages.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.Similar to the best-of-n tech-\nnique, it relies on an argument n for the number of reﬁnement steps, and is curried such that each\nreﬁnement step can be handled by any other prompting function.However, unlike best-of-n, there is\nno opportunity for parallelism as each iteration is dependent on information from the previous call.For example:\n\n4\n\n\fbest_of_n((refinement,m),n) is a compound sampling method that run a best-of-n,\nwhere each call is a m-step reﬁnement.refinement((best_of_n,m),n) is a compound sampling method that runs a n-step reﬁne-\nment, where each call is a best-of-m call to the LLM.Note that with each of these compound prompt functions, there are always a total of mn iterations.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.Namely, each metric is loaded with a cached (vector) database\npopulated with human-made examples of preoptimized and postoptimized pairs of Lean theorems.The number of examples that are retrieved is user-speciﬁed.Document retrieval extracts information using MMR from a pair of ﬁxed (vector) databases.The\ndatabases store semantically chunked data from the Theorem Proving in Lean (TPiL) handbook –\ncontaining syntax guides and tactic explanations – and the Mathlib mathematics libary – containing\nthousands of theorems and lemmas.The chunking is handled by a recursive character splitter, which\nsplits the TPiL markdown ﬁles at on its headers and Mathlib ﬁles at the start of theorems, examples,\nlemmas, and deﬁnitions – with chunk sizes of 1000 characters with a 200 character overlap.The Mathlib retriever ﬁnds the top k documents that score the highest MMR score against the current\ntheorem, the TPiL retriever ﬁnds the top k documents that score the highest MMR score against the\ncurrent theorem in context and all current error messages.This retrieval process helps in generating\nmore contextually accurate prompts that allow the language model to better correct its own errors as\nwell as ﬁnd useful lemmas to reference.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.4.1 SETUP\n\nOur experimentation is split into three distinct stages.Lastly, we note the performance of ImProver in NTP applications in comparison to the base GPT-4o\nand GPT-4o-mini models.Datasets. We evaluate ImProver on subsets of the following datasets.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.This is a\ndataset of internationally-renowned competitive math problems, many of which are readable, yet\nquite verbose.This dataset contains theorems of a competitive format, and although they contain\n\n5\n\n\fconcepts only at a high-school level, the logical complexity of internationally-renowned competi-\ntion results is far above that.For our main results, we used all 26 theorems and lemmas from the\nCompﬁles database of complete solutions to the International Mathematics Olympiad (IMO) and\nthe American Mathematics Olympiad (USAMO) from 2016-2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.These proofs are extremely\nefﬁcient, concise, and generalized - which often comes at the cost of readability and understandabil-\nity.These results and theorems often are at the cutting edge of research.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.This is the most difﬁcult dataset. Models.Our base generator uses GPT-4o (OpenAI et al., 2024).Since no prior methods currently\nexist for automated proof optimization, we consider a prompted GPT-4o without the improvements\ndescribed in (§3.1) as our baseline.Additionally, for a given metric, we write a prompt that brieﬂy\ndescribes the metric and the proof optimization task.We also provide instructions, context, and in-\nformation depending on the features selected, and add the theorem and proof to the prompt.",
          "time": 176.14561676979065,
          "input_token_size": 16878,
          "output_token_size": 61,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_0dd0cacb",
        "created_at": 1749869908.8610613,
        "processing_time": 179.50461196899414,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.824379"
      }
    }
  },
  {
    "175a0142": {
      "question": "What does ImProver aim to make proofs shorter and more readable?",
      "answer": "ImProver aims to make proofs shorter and more readable by rewriting proofs to optimize arbitrary user-defined metrics such as length and readability.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93
          ],
          "provenance_ids": [
            1,
            7,
            9,
            10,
            11,
            12,
            13,
            15,
            16,
            17,
            18,
            20,
            22,
            24,
            34,
            37,
            38,
            39,
            40
          ],
          "provenance": "0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.In\npractice, however, informal, natural-language proofs are prone to imprecision, ambiguity, and error.Using a formal language such as Lean (Moura & Ullrich, 2021) removes ambiguity and precision\nand enables a proof assistant to verify correctness down to the primitives of a formal axiomatic\nsystem.Formal proofs, however, can be hard to read and often suffer from low reusability or excessive detail.For example, formal proofs in Lean’s extensive mathematical library, Mathlib (mathlib Community,\n2020), are generally designed to be concise and very general, often at the expense of readability.Machine learning systems trained on such proofs learn to mimic these\nvaried conventions (Hu et al., 2024), which may not be the optimal use of the limited supply of\nhuman-written proofs.As a result, we would like to be able to automatically refactor proofs to meet\na secondary objective such as length or readability.To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is\ncorrect and optimizes a criterion such as length or readability.We ﬁnd that naively applying LLMs\nto proof optimization falls short, often resulting in incorrect or poorly optimized proofs.We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nrw [h1u _ (Classical.choose_spec _)]\nobtain hw, h1e', h1u'i := h1 y\nrw [h1u' _ ((h2 _ _).mpr h1e)]\nexact h1u' _ (Classical.choose_spec _)\n\n(h1 : ∀ x, ∃!In this example, ImProver optimizes a human-written\nlemma (right) from the 2022 International Math Olympiad (Question 2, solution from Comp-\nﬁles (David Renshaw, 2024)) for length.ImProver brings these code generation techniques, along with new\nChain-of-States prompting and meta-programmed contextual information, into a uniﬁed proof opti-\nmization agent.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.Note that shorter proofs often\nrepresent more efﬁcient proofs.Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.",
          "time": 44.67820453643799,
          "input_token_size": 15998,
          "output_token_size": 104,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_175a0142",
        "created_at": 1750198706.9430954,
        "processing_time": 51.204349517822266,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 94,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 94,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.824379"
      }
    }
  },
  {
    "1c68e45a": {
      "question": "What does the document mention about the cost and runtime of ImProver?",
      "answer": "Cost: High  \nRuntime: Slow",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234
          ],
          "provenance_ids": [
            188,
            189,
            190,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            206,
            209,
            210,
            212,
            213,
            214,
            219,
            220,
            221,
            222,
            226,
            227,
            231,
            232,
            233
          ],
          "provenance": "Fixing this parameter, we vary the sampling\nmethods (excluding compound methods and ﬁxing n = 5) and observe that best-of-n is the best\nparameter combination.Now, as GPT-4o-mini is signiﬁcantly less computationally expensive than\nits GPT-4o counterpart, we test both models with the sample method ﬁxed to best-of-n, and vary\nn = 1, 3, 5, 7, 10, 15, and for GPT-4o-mini, also n = 20.We conclude that GPT-4o with n = 15\nis the most effective.Readability and Chain-of-States (CoS) Ablation.We additionally examine the effects of dis-\nabling CoS on readability optimization tasks, as the previous study focused on length optimization\ntasks, and we speculate that CoS has a high impact on the performance of readability optimization\ntasks, as the proof states that are embedded due to CoS seem to be a critical aspect to generating the\nexplicit declarations that the readability metric measures.We conﬁrm this result by considering Table 6 and observe that simply enabling CoS nearly doubles\nthe improvement score, and signiﬁcantly improves the nonempty improvement score, suggesting that\nCoS has a high degree of impact on optimizing for the readability metric, as conjectured.However,\nwe also note a signiﬁcant increase in improved accuracy, which suggests that embedding the chain\nof states also improves the ability of the model to generate nontrivial correct outputs, implying that\nthe symbolic information contained in the states are critical to effectively modifying the structure\nand content of a proof.4.2.2 NEURAL THEOREM PROVING EVALUATION\n\nWe evaluate ImProver’s neural theorem proving (NTP) performance using the completion metric.We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input\nproof.Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-4o.ImProver substantially outperforms GPT-4o across all topics, with an 80% increase in accuracy\ncompared to the base model, showing that proof optimization systems are easily extendable to NTP\nsystems.9\n\n\fTable 7: Proof generation results.Each cell shows percent accuracy.These examples show the balance between correctness and the desired optimization metric,\nshowing how ImProver can produce more concise or readable proofs depending on the use case.The original proof consisted of 12 tactic steps and multiple intermediate calculations.After applying\nImProver for length optimization, the proof was halved to only 6 tactics, eliminating redundant steps\nwhile maintaining correctness.Example 2: MIL. Consider Figure 3, a result from MIL that we optimize for readability.Original (human-written)\n\nImProver (readability-optimized)\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ncard G = card H * card K)\n\n: K ≃* G / H := by\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\n· rw [← ker_eq_bot_iff, (QuotientGroup.mk'\n\nH).ker_restrict K]\nsimp [h]\n\n· symm\n\nexact aux_card_eq h'\n\ncard G = card H * card K)\n:= by\n\n: K ≃* G / H\n\nhave injectivity : Function.Injective\n\n((QuotientGroup.mk' H).restrict K) := by\nrw [← ker_eq_bot_iff, (QuotientGroup.mk'\nH).ker_restrict K]\nsimp [h]\n\nhave card_eq : card (G / H) = card K := by\n\nexact aux_card_eq h'\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\nexact injectivity\nsymm\nexact card_eq\n\nFigure 3: Optimizing a group-theoretic result from MIL Chapter 8 Section 1 for readability.We\ndeﬁne a proof to be readable if it is written in a declarative style, which consists of intermediate\nconjectures (have · · · statements).Example 3: Full Proof Generation.We analyze the application of ImProver to neural theorem\nproving in the MIL example from Figure 4.This theorem relating to group theory originally has no proof, however, ImProver generates one\nfrom scratch.This generated proof is veriﬁed to be correct by Lean, utilizing all the included hy-\npotheses as well as a retrieved mathlib theorem.However, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance\non black-box LLM’s.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.Sean Welleck thanks Convergent Research, the Lean FRO, and the OpenAI Researcher Access Pro-\ngram for their support.11\n\n\fREFERENCES\n\nand\n\nAlphaGeometry\n\nAlphaProof\nstandard\nhttps://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\n2024.\n\nsilver-medal\nproblems.achieves\nolympiad\n\nmathematical\n\ninternational\n\nsolving\n\nTeams.",
          "time": 43.49860191345215,
          "input_token_size": 23609,
          "output_token_size": 29,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_1c68e45a",
        "created_at": 1750198648.7613935,
        "processing_time": 47.57138156890869,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 47,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 47,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.824379"
      }
    }
  },
  {
    "221c5bbb": {
      "question": "What are the two additional output formats introduced by ImProver?",
      "answer": "Flat and structured.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            58,
            59,
            60
          ],
          "provenance_ids": [
            59
          ],
          "provenance": "To analyze this hypothesis, we introduce two additional output for-\nmats to the standard str output: flat and structured.",
          "time": 6.155411720275879,
          "input_token_size": 17207,
          "output_token_size": 28,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_221c5bbb",
        "created_at": 1750198619.6896029,
        "processing_time": 11.72065544128418,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 3,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 3,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.824379"
      }
    }
  },
  {
    "283ade03": {
      "question": "In which datasets was ImProver's performance tested?",
      "answer": "1. Mathematics in Lean (MIL)\n2. Compfiles\n3. Mathlib",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116
          ],
          "provenance_ids": [
            98,
            102,
            106,
            108,
            109,
            112,
            114,
            115,
            116
          ],
          "provenance": "Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.These results and theorems often are at the cutting edge of research.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.Since no prior methods currently\nexist for automated proof optimization, we consider a prompted GPT-4o without the improvements\ndescribed in (§3.1) as our baseline.We also provide instructions, context, and in-\nformation depending on the features selected, and add the theorem and proof to the prompt.Speciﬁc\nprompt information is detailed in (§A)\n\nPerformance metrics.Since proof optimization is a new task, we deﬁne four performance metrics\nfor measuring aspects of correctness and improvement.",
          "time": 37.75709247589111,
          "input_token_size": 26547,
          "output_token_size": 149,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_283ade03",
        "created_at": 1749665211.7565665,
        "processing_time": 41.166879177093506,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev",
        "reason": "shorter_provenance",
        "sentence_count": 23,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev",
            "sentence_count": 23,
            "provenance_count": 1,
            "has_answer": true
          },
          {
            "source": "test_outputs_prev1",
            "sentence_count": 23,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.829852"
      }
    }
  },
  {
    "2ec24144": {
      "question": "What future improvements are suggested for ImProver to overcome its current limitations?",
      "answer": "1. Apply fine-tuning and reinforcement learning on a smaller model to match performance at a lower cost.\n2. Improve the efficiency and runtime of ImProver.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            1,
            2,
            3,
            4,
            8,
            9,
            15,
            17,
            25,
            32,
            35,
            37,
            38,
            40,
            41,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            128,
            129,
            130,
            132,
            133,
            135,
            136,
            137,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            172,
            173,
            174,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            199,
            200,
            201,
            202,
            203,
            204,
            208,
            212,
            213,
            214,
            215,
            216,
            223,
            224,
            226,
            227,
            228,
            229,
            230,
            231,
            236,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            248,
            249,
            251,
            252,
            253,
            254,
            255,
            257,
            258,
            263,
            264,
            265,
            266,
            271,
            272,
            273,
            277,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            293,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            307,
            308,
            309,
            310,
            311,
            315,
            316,
            317,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            361,
            362,
            363,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance": "0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.Machine learning systems trained on such proofs learn to mimic these\nvaried conventions (Hu et al., 2024), which may not be the optimal use of the limited supply of\nhuman-written proofs.To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is\ncorrect and optimizes a criterion such as length or readability.ImProver’s optimized proof is correct and more concise.Finally, there is a rich literature on the var-\nied styles of (human) formal proofs (e.g., (Autexier & Dietrich, 2010; Wiedijk, 2004)).3 AUTOMATED PROOF OPTIMIZATION WITH ImProver\n\nAutomated Proof Optimization.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.Concretely, we evaluate this using the ratio of number of explicitly typed\nhave tactics to total number of tactic invocations.This is a trivial\nmetric which measures the number of errors present.The completion metric is used for concretely\nviewing proof optimization as a generalization of neural theorem proving.3.1\n\nIMPROVER\n\nWe develop several improvements that can be applied to a black-box LLM generator yout ∼\nG(·|xin), such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these pa-\nrameters.The explicit prompts and templates that are sent to the LLM can be found in (§A).3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.The intermediate states often contain valuable information (e.g., an expression\nafter it has been simpliﬁed) that is not present in the tactics.To allow the model to reason about\nthese intermediate goals and hypotheses, we use tools from Lean metaprogramming to automatically\nannotate each proof state as a comment prior to each tactic.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.The implementation of this extraction system is mod-\neled from the work (Kim Morrison, 2024).Speciﬁcally, in the compiler’s elaboration and eval-\nuation stages – where the parsed theorem code is ﬁrst converted into concrete syntax trees (in\npractice, Syntax objects) and abstract syntax trees (Expr objects) – we convert the CST and\nAST output objects into the relevant proof data and proof states in the form of proof trees\n(Lean.Elab.InfoTree).These proof trees contain detailed context and information on a tactic-\nby-tactic level relating to the modiﬁcation of the proof state, metavariable context, and proof cor-\nrectness.After state extraction is completed and cached for efﬁcient future access, we annotate the proof text\nitself to contain the intermediate states in the form as comments.Figure 2 shows an example. This explicit reasoning aims to help the generator model construct more optimized proofs via addi-\ntional symbolic data.3.1.2 OUTPUT FORMATTING. LLM outputs often contain ancillary and syntactically invalid content, especially before and after\nthe actual proof.Additionally, by applying additional structure to the LLM outputs, we may hope to\ngenerate more structured proofs.To analyze this hypothesis, we introduce two additional output for-\nmats to the standard str output: flat and structured.The former enforces a tactic sequence\noutput as a list of strings, and the latter enforces a proof tree output as a tree of strings.3.1.3 SAMPLING METHOD\n\nWe also introduce different methods of sampling between many (sequential or parallel) LLM infer-\nence calls, involving best-of-n and iterative reﬁnement implementations, as well as combinations\nthereof.3\n\n\fWithout Chain-of-States\n\nWith Chain-of-States\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u) := by\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u)\n\n:= by\n\nrintro x (hxs, xti | hxs, xui)\n· use xs; left; exact xt\n.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!-/\n. use xs; right; exact xu\n/-\nGoals Solved!-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.More speciﬁcally, our scoring function is given by the 2-ary\ncomparison function S, whose arguments are output objects y, y′.max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.This comparison function can be extended to evaluate the best output of any ﬁnite n via induction.This best-of-n technique is implemented as a curried function such that each of the n calls can be\nhandled by any arbitrary sampling method, or just a single standard prompt at user discretion.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.The reﬁnement process relies on user-deﬁned pa-\nrameters n and prev_num to specify the number of iterations and the number of previous iteration\ninfo to forward, respectively.Each iteration carries information on the last prev_num iterations,\nincluding input, output, metric score, correctness, and error messages.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.Similar to the best-of-n tech-\nnique, it relies on an argument n for the number of reﬁnement steps, and is curried such that each\nreﬁnement step can be handled by any other prompting function.However, unlike best-of-n, there is\nno opportunity for parallelism as each iteration is dependent on information from the previous call.Combination Sampling and Compound Prompt Functions Compound prompt functions utilize\nthe curried nature of the implementations of best-of-n and reﬁnement to nest these techniques within\none another.For example:\n\n4\n\n\fbest_of_n((refinement,m),n) is a compound sampling method that run a best-of-n,\nwhere each call is a m-step reﬁnement.refinement((best_of_n,m),n) is a compound sampling method that runs a n-step reﬁne-\nment, where each call is a best-of-m call to the LLM.Note that with each of these compound prompt functions, there are always a total of mn iterations.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.More speciﬁcally, example retrieval selects the most relevant user-generated examples of proof op-\ntimization on a speciﬁc metric.Namely, each metric is loaded with a cached (vector) database\npopulated with human-made examples of preoptimized and postoptimized pairs of Lean theorems.The number of examples that are retrieved is user-speciﬁed.Document retrieval extracts information using MMR from a pair of ﬁxed (vector) databases.The\ndatabases store semantically chunked data from the Theorem Proving in Lean (TPiL) handbook –\ncontaining syntax guides and tactic explanations – and the Mathlib mathematics libary – containing\nthousands of theorems and lemmas.The chunking is handled by a recursive character splitter, which\nsplits the TPiL markdown ﬁles at on its headers and Mathlib ﬁles at the start of theorems, examples,\nlemmas, and deﬁnitions – with chunk sizes of 1000 characters with a 200 character overlap.The Mathlib retriever ﬁnds the top k documents that score the highest MMR score against the current\ntheorem, the TPiL retriever ﬁnds the top k documents that score the highest MMR score against the\ncurrent theorem in context and all current error messages.This retrieval process helps in generating\nmore contextually accurate prompts that allow the language model to better correct its own errors as\nwell as ﬁnd useful lemmas to reference.4 EXPERIMENTS\n\nWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics and compare its results to those of the base GPT-4o and GPT-4o-mini\nmodels.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.4.1 SETUP\n\nOur experimentation is split into three distinct stages.We ﬁrst perform ablation testing on the\nImProver model parameters (§3.1) to ensure that ImProver’s parameter speciﬁcation is the optimal\none with respect to correctness and metric optimization score.We then evaluate this optimal param-\neter combination on datasets of varying complexity and analyze the performance and results thereof.Lastly, we note the performance of ImProver in NTP applications in comparison to the base GPT-4o\nand GPT-4o-mini models.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.We use exercise solutions from set theory, elementary number theory, group\ntheory, topology, differential calculus, and integration & measure theory.This dataset contains the-\norems at an undergraduate-level of complexity.For our main results, we evaluated on 72 theorems\nfrom exercise solutions from MIL chapters 4, 5, 8, 9, and 10.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.This is a\ndataset of internationally-renowned competitive math problems, many of which are readable, yet\nquite verbose.This dataset contains theorems of a competitive format, and although they contain\n\n5\n\n\fconcepts only at a high-school level, the logical complexity of internationally-renowned competi-\ntion results is far above that.For our main results, we used all 26 theorems and lemmas from the\nCompﬁles database of complete solutions to the International Mathematics Olympiad (IMO) and\nthe American Mathematics Olympiad (USAMO) from 2016-2024.These proofs are extremely\nefﬁcient, concise, and generalized - which often comes at the cost of readability and understandabil-\nity.These results and theorems often are at the cutting edge of research.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.This is the most difﬁcult dataset. Models.Our base generator uses GPT-4o (OpenAI et al., 2024).Since no prior methods currently\nexist for automated proof optimization, we consider a prompted GPT-4o without the improvements\ndescribed in (§3.1) as our baseline.Additionally, for a given metric, we write a prompt that brieﬂy\ndescribes the metric and the proof optimization task.We also provide instructions, context, and in-\nformation depending on the features selected, and add the theorem and proof to the prompt.Speciﬁc\nprompt information is detailed in (§A)\n\nPerformance metrics.First, we deﬁne improvement for length as percentage change in length, µlen(y0)−µlen(y)\n× 100.For\nreadability, we use the difference, µread(y)−µread(yo).If no correct output is generated by the model\nfor a speciﬁc theorem, improvement is deﬁned to be zero.We deﬁne nonempty improvement as\nthe improvement restricted to theorems for which some output has nonzero improvement.µlen(y0)\n\nIntuitively, improvement is the expected improvement in metric score from the input to output, ac-\ncounting for errors in the generation.The nonempty improvement score is the expected improvement\nin metric score, given that there are no errors in the generation.Similar improvement scores can be\ndeﬁned for other metrics using a binary function of the metric assigned to the original and optimized\nproofs.Additionally, the accuracy is the percentage of theorems in the dataset which the model was able to\ngenerate a correct output for.The improved accuracy is the percentage of theorems in the dataset\nwhich the model was able to generate a correct output for, as well as improve the metric to be\nnonzero.4.1.1 ABLATIONS\n\nWhen performing our ablation studies, we used a ﬁxed dataset (MIL) and metric (length) and varied\nthe parameters of all the features to ﬁnd the optimal combination.As such, we evaluate using a\nfactorial testing method. Testing Groups.We deﬁne the following testing groups with the speciﬁed parameter combina-\ntions:\n\nGPT-4o-mini/GPT-4o: This varies the GPT-4o model, outputting a string with no other features.Output and CoS: We evaluate the effects of different output formatting styles (string, string\nlist, string tree) and CoS (True, False), with the model ﬁxed as GPT-4o, with no other\nfeatures enabled.Sampling Method: Here, we evaluate the effects of best-of-n and reﬁnement for a ﬁxed n = 5.Additionally we test on the reﬁnement cases if forwarding the most recent iteration result, or all\nprevious iteration results is the best, and if we should keep the best out of the iterations, or the most\n\n6\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nMetric\n\nModel\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n3.7\n20.96\n\n2.21\n9.34\n\n6.25\n30.54\n\n4.18\n13.45\n\n15.15\n55.29\n\n8.02\n30.53\n\n18.58\n56.56\n\n14.48\n30.97\n\n26.36%\n100.0%\n\n18.75%\n100.0%\n\n8.31%\n35.44%\n\n6.13 %\n24.56%\n\n37.5%\n100.0%\n\n28.85%\n100.0%\n\n14.42%\n50.0%\n\n11.54%\n34.21%\n\nTable 2: MIL Proof optimization results.Improvement Nonempty Improvement Accuracy\n\nImproved Acc. recent.The model is ﬁxed as GPT-4o, CoS, output formatting, and examples are ﬁxed as the best\ncombination from the previous test, and no other features enabled.We test n =\n3, 5, 7, 10, 15 on GPT-4o and GPT-4o-mini, as well as n = 20 for GPT-4o-mini (as it is of a far\nlower token cost).CoS, output formatting, examples, and sampling method are ﬁxed as the best\ncombination from the previous test, and no other features enabled.Combos and RAG: We evaluate combination methods refinement(best_of_m',m) and\nbest_of_m'(refinement(m)), for m 6= m′ with mm′ equal to the optimal value m from\nthe previous test.We also test the effect of enabling document retrieval.Model, CoS, output for-\nmatting, examples, n, and sampling method are ﬁxed as the best combination from the previous\ntest.Ablation data. We evaluate our ablations on a subset of MIL.However, due to the increase in\nmodel calls for larger n values, we switch a representative sample of this subset for some test groups.Namely,\n\nGPT-4o-mini, GPT-4o, Output and Cos, Example Retrieval, and Sampling Method are tested\non the 133 theorems in the solutions of C03_Logic, C04_Sets_and_Functions, and\nC05_Elementary_Number_Theory.n and Model are tested on 55 theorems from a representative sample of the aforementioned, and\nCombos and RAG are tested on a representative sample of 32 theorems from the aforementioned.4.2 RESULTS\n\nImProver is capable of optimizing proofs in all settings.From Table 2, Table 3, and Table 4,\nwe can see that ImProver is capable of optimizing proofs on all datasets for both the length and\nreadability metrics.Furthermore, Table 1 shows that across all metrics, ImProver signiﬁcantly\noutperforms GPT-4o on proof optimization tasks on every experimental measure – aggregated from\nall datasets.Additionally, from Table 2, Table 3, and Table 4, we can see that ImProver outperforms\nGPT-4o on each dataset as well.We proceed to analyze this data and its implications. Length optimization.First focusing on the length metric, we see that ImProver outperforms GPT-\n4o with respect to the improvement score by 566% (aggregated over all datasets).Additionally, we\nare guaranteed that ImProver produces a correct output, although that output may just be the same\nas the input.However, 35.44% of the time, it generates a correct output that is not the same length\nas the input, and in that case, we expect an average of a 55.29% reduction in length.Comparing\nthis with GPT-4o, we conclude that not only can ImProver optimize at a higher level on arbitrary\ntheorems, but its ability to generate nontrivial correct outputs is far greater in comparison to GPT-4o.Table 3: Compﬁles Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n2.75\n18.86\n\n0.39\n5.74\n\n0.0\n6.19\n\n0.0\n4.63\n\n30.7\n54.48\n\n3.38\n24.89\n\n0.0\n53.65\n\n0.0\n33.19\n\n11.54%\n100.0%\n\n14.1%\n100.0%\n\n5.13%\n34.62%\n\n1.28%\n19.23%\n\n16.67%\n100.0%\n\n4.65%\n100.0%\n\n0.0%\n11.54%\n\n0.0%\n11.63%\n\nMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 4: Mathlib Proof optimization results. Readability optimization.Readability optimization is similar, with ImProver outperforming\nGPT-4o by 423%.Moreover, the accuracy, improved accuracy, and nonempty improvement dis-\nparities for readability parallel those of the length tests.However, it should be noted that for both\nGPT-4o and ImProver, the accuracy and improved accuracy scores were markedly smaller for read-\nability than length optimization.This suggests that for both models, it was generally more “difﬁcult”\nto generate a correct output, and moreover, generate a correct output with a better metric score\nthan the input, for readability optimization than length optimization.In other words, optimizing for\nreadability is more difﬁcult for the underlying generator than optimizing for length.However, we\nspeculate with higher-quality prompts, descriptions of the metric, and examples, this disparity can\nbe minimized.Regardless, we note that different metrics can be less likely to be correctly optimized,\nand that model performance is correlated with the metric it seeks to optimize – both for GPT-4o and\nImProver.Optimization varies based on dataset difﬁculty.Additionally noting Table 2, Table 3, and\nTable 4, we observe that the improvement score for both metrics for both GPT-4o and ImProver\nis highest for the MIL dataset, lower for Compﬁles, and the lowest on the Mathlib theorems.This\nsuggests that the expected improvement in metric score decreases with higher difﬁcultly – with\nundergraduate-level theorems having a signiﬁcantly higher expected improvement than research-\nlevel theorems.Similarly, the accuracy and improved accuracy scores for both metrics and\nmodels decreased with higher difﬁculty datasets (disregarding ImProver’s accuracy scores, as they\nare ensured to be 100%).This suggests that although the base GPT-4o generator is less likely to\ngenerate a correct output for higher difﬁculty datasets, the improvements that ImProver makes to\nthe base generator allows it to maintain its improvement in the metric score whenever a correct out-\nput is generated.As such, we can speculate that the bottleneck in the improvement score is not the\nmodel’s ability to optimize the proof for a metric, but rather its ability to generate a new correct\nproof at all.We measure the baseline results from the GPT-\n4o and GPT-4o-mini models, noting that GPT-4o is the better-scoring model (with respect to the\nimprovement score).Thus, ﬁxing this model, we vary the output formatting type and if CoS is\nenabled, and determine that outputting flat with CoS enabled maximizes the improvement score.8\n\n\fTable 5: Ablation results.Each cell in the ablation tests shows best / worst, which are the best\nand worst parameter combinations in the test group.The ImProver speciﬁcation outputs the input\ntheorem when no correct proof is generated, which results in an accuracy of 100% on MIL.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o-mini\nGPT-4o\n+ Output and CoS\n+ Example Retrieval\n+ Sampling Method\n+ n and Model\n+ Combos and RAG\nImProver\n\n0\n7.03\n8.04 / 6.31\n9.34 / 5.67\n15.35 / 9.34\n23.51 / 3.65\n34.88 / 28.25\n\n0\n19.67\n12.38 / 14.17\n14.7 / 8.44\n18.44 / 14.7\n26.28 / 4.63\n57.56 / 33.48\n\n3.62%\n35.77%\n\n0%\n15.33%\n\n64.96% / 44.53% 21.17% / 16.06%\n63.5% / 67.15%\n21.9% / 16.79%\n36.5% / 21.9%\n83.21% / 63.5%\n89.47% / 78.95% 45.61% / 8.77%\n60.61% / 84.38% 54.55% / 53.12%\n\n34.88\n\n57.56\n\n100%\n\n54.55%\n\nTable 6: CoS Readability Ablation results.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o\nImProver, CoS Disabled\nImProver\n\n4.97\n9.23\n16.69\n\n15.89\n24.61\n31.42\n\n37.5%\n100.0%\n100.0%\n\n12.5%\n28.12%\n46.88%\n\nFixing these parameters, we now vary the number of examples retrieved, noting that prompting\nwith 10 examples maximizes the improvement score.Fixing this parameter, we vary the sampling\nmethods (excluding compound methods and ﬁxing n = 5) and observe that best-of-n is the best\nparameter combination.Now, as GPT-4o-mini is signiﬁcantly less computationally expensive than\nits GPT-4o counterpart, we test both models with the sample method ﬁxed to best-of-n, and vary\nn = 1, 3, 5, 7, 10, 15, and for GPT-4o-mini, also n = 20.We conclude that GPT-4o with n = 15\nis the most effective.Fixing these parameters, we consider all mixed compound sampling methods\nwith and without document retrieval enabled, concluding that a 5-step reﬁnement with best-of-3 on\neach iteration, with RAG enabled, is the optimal combination.Thus, as we can see from Table 5, the optimal parameter combination comes from gpt-4o outputting\nas a string list with CoS, RAG, 10 examples, 5-step reﬁnement with each iteration being a\nbest-of-3 evaluation.Changing any one of these parameters them leads to a reduction in performance.Additional ablation data can be found at (§B.1).Readability and Chain-of-States (CoS) Ablation.We additionally examine the effects of dis-\nabling CoS on readability optimization tasks, as the previous study focused on length optimization\ntasks, and we speculate that CoS has a high impact on the performance of readability optimization\ntasks, as the proof states that are embedded due to CoS seem to be a critical aspect to generating the\nexplicit declarations that the readability metric measures.We conﬁrm this result by considering Table 6 and observe that simply enabling CoS nearly doubles\nthe improvement score, and signiﬁcantly improves the nonempty improvement score, suggesting that\nCoS has a high degree of impact on optimizing for the readability metric, as conjectured.4.2.2 NEURAL THEOREM PROVING EVALUATION\n\nWe evaluate ImProver’s neural theorem proving (NTP) performance using the completion metric.We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input\nproof.Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-4o.ImProver substantially outperforms GPT-4o across all topics, with an 80% increase in accuracy\ncompared to the base model, showing that proof optimization systems are easily extendable to NTP\nsystems.9\n\n\fTable 7: Proof generation results.Each cell shows percent accuracy.Consider Figure 1, a lemma from the 2022 IMO Question\n2 (Compﬁles) that we optimize for length.Example 2: MIL. Consider Figure 3, a result from MIL that we optimize for readability.Original (human-written)\n\nImProver (readability-optimized)\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ncard G = card H * card K)\n\n: K ≃* G / H := by\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\n· rw [← ker_eq_bot_iff, (QuotientGroup.mk'\n\nH).ker_restrict K]\nsimp [h]\n\n· symm\n\nexact aux_card_eq h'\n\ncard G = card H * card K)\n:= by\n\n: K ≃* G / H\n\nhave injectivity : Function.Injective\n\n((QuotientGroup.mk' H).restrict K) := by\nrw [← ker_eq_bot_iff, (QuotientGroup.mk'\nH).ker_restrict K]\nsimp [h]\n\nhave card_eq : card (G / H) = card K := by\n\nexact aux_card_eq h'\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\nexact injectivity\nsymm\nexact card_eq\n\nFigure 3: Optimizing a group-theoretic result from MIL Chapter 8 Section 1 for readability.We\ndeﬁne a proof to be readable if it is written in a declarative style, which consists of intermediate\nconjectures (have · · · statements).ImProver introduces two intermediate conjectures into the proof.This original proof carried a score of 0, as it does not contain any declarative statements.10\n\n\fOriginal (human-written)\n\nImProver (completeness-optimized)\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n: S ≤ T) : comap ϕ S ≤ comap ϕ T := by\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n\n: S ≤ T) : comap ϕ S ≤ comap ϕ T\n\n:= by\n\nsorry\n\nintro g\nsimp only [mem_comap]\nintro hS\nexact hST hS\n\nFigure 4: Solving a group theorem exercise from MIL Chapter 8 Section 1 for readability.5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.However, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance\non black-box LLM’s.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.As such, we believe that ImProver sets the stage for further\nwork on proof optimization to advance the study and use of AI in mathematics.ACKNOWLEDGEMENTS\n\nRiyaz Ahuja thanks the L3 Lab and Hoskinson Center for Formal Mathematics for their support.Sean Welleck thanks Convergent Research, the Lean FRO, and the OpenAI Researcher Access Pro-\ngram for their support.In Matt Kaufmann\nand Lawrence C. Paulson (eds.99–114, Berlin, Heidelberg,\n2010. Springer Berlin Heidelberg.Jaime Carbonell and Jade Goldstein.The use of mmr, diversity-based reranking for reordering doc-\numents and producing summaries.In Proceedings of the 21st Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ’98, pp.335–336,\nISBN 1581130155. doi:\nNew York, NY, USA, 1998.Association for Computing Machinery. 10.1145/290941.291025.URL https://doi.org/10.1145/290941.291025.Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.Teaching large language models\n\nto self-debug, 2023.https://github.com/dwrensha/compfiles, 2024.Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun.Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu.Proof artifact co-\nIn International Conference on Learning\n\ntraining for theorem proving with language models.Representations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.Jiewen Hu, Thomas Zhu, and Sean Welleck.minictx: Neural theorem proving with (long-)contexts,\n\n2024.Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu.Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs.for neural\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=J4pX8Q8cxHH. theorem proving. 2024. 2024.Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie\n\nSi.), Proceedings of the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp.14605–14631, Toronto, Canada, July 2023.Association for Computational Linguistics.Self-reﬁne:\nIterative reﬁnement with self-feedback.ACM, January 2020. doi:\n10.1145/3372885.3373824.URL http://dx.doi.org/10.1145/3372885.3373824.Leonardo de Moura and Sebastian Ullrich.The lean 4 theorem prover and programming lan-\nIn Automated Deduction – CADE 28: 28th International Conference on Automated\nguage.Deduction, Virtual Event, July 12–15, 2021, Proceedings, pp.625–635, Berlin, Heidelberg,\n2021. Springer-Verlag.ISBN 978-3-030-79875-8. doi: 10.1007/978-3-030-79876-5_37.URL\nhttps://doi.org/10.1007/978-3-030-79876-5_37.Gpt-4 technical\nreport, 2024. URL https://arxiv.org/abs/2303.08774.13\n\n\fStanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\n\nSutskever.Formal mathematics statement curriculum learning, 2022.Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, and Swarat Chaudhuri.An in-\n\ncontext learning agent for formal theorem-proving, 2024.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H.\nthought prompting elicits reasoning\nChi, Quoc V Le, and Denny Zhou.in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=_VjQlMeSB_J.Chain of\n\nFreek Wiedijk. Formal proof sketches.378–393, Berlin, Heidelberg, 2004.Springer Berlin\nHeidelberg. ISBN 978-3-540-24849-1.Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil,\nRyan Prenger, and Anima Anandkumar.LeanDojo: Theorem proving with retrieval-augmented\nlanguage models.In Neural Information Processing Systems (NeurIPS), 2023.URL https://openreview.net/forum?id=ZTCxT2t2Ru.14\n\n\fA PROMPTS\n\nA.1 TEMPLATE\n\nIn this appendix, we note the prompts used by ImProver both for general LLM prompting, as well\nas the metric-speciﬁc prompts.For the main prompt sent to the LLM on each sample, we build a prompt string using a chat prompt\ntemplate that is then invoked at runtime to ﬁll in the variables.The prompt template is a conversation of the format:\n\nPlaceholder: All metric prompts with a ‘System’ role\nSystem: You will be given the proof context (i.e.the lean ﬁle contents/imports leading up\nto the theorem declaration) wrapped by <CONTEXT>...</CONTEXT>.You will be given the previous num_prev input/output pairs as well as their metric (met-\nric.name) score and correctness score, as well as any error messages, for your reference to\nimprove upon.Each of these previous results will be wrapped with <PREV I=0></PREV\nI=0>,...,<PREV I=num_prev-1></PREV I=num_prev-1>, with I=num_prev-1 being the\nmost recent result.Remember to use lean 4 syntax, which has signiﬁcant changes from the lean 3 syntax.To\nassist with the syntax relating to the current theorem and current error messages, you will\nbe given num_syntax_docs documents to refer to for ﬁxing these syntax issues.Each of\nthese documents will be wrapped with <SYNTAX_DOC>...</SYNTAX_DOC>.You will also receive num_mathlib_docs documents relevant to the current theorem to\nhelp with formulating your modiﬁed proof.Each of these will be wrapped with <CON-\nTENT_DOC>...</CONTENT_DOC>\nYou will also receive num_examples examples of input-output pairs of proofs that\nwere optimized for the metric metric.The current theorem will be\nwrapped in <CURRENT>...</CURRENT>\nSystem: Output format instructions\nPlaceholder: All retrieved syntax documentation\nPlaceholder: All retrieved mathlib documentation\nPlaceholder: All retrieved examples\nUser: <CONTEXT> context </CONTEXT>\nPlaceholder: Previous results and inputs/outputs\nPlaceholder: All metric prompts with a ‘User’ role\nUser: <CURRENT> theorem </CURRENT>\n\nThis prompt is then invoked and sent to the language model by ﬁlling in all the variables and place-\nholders.Notably, when we invoke the chain given by chain|llm|parser, we throttle the invoca-\ntion with a randomized exponential rate limit throttling to account for API rate limits, especially in\nhighly-parallelized requests like when benchmarking over a large number of theorems.A.2 METRIC PROMPTS\n\nLength Metric\n\nSystem: You are an AI assistant who shortens Lean 4 proofs while ensuring their correct-\nness.You will aim to reduce the number of lines of the tactic proof while ensuring that it\nproperly compiles in Lean 4.15\n\n\fUser: Shorten the current theorem (wrapped in <CURRENT>...</CURRENT>) to be as\nshort in length—measured in the number of lines of the proof—as possible, while also\nensuring that the output is still syntactically correct.\"Readability Metric\n\nCompletion Metric\n\nSystem: You are an AI assistant who rewrites Lean 4 proofs to be more readable while\nensuring their correctness.We measure readablity by considering the ratio of the number\nofexplicitly typed have tactics against the total number of tactics in the proof, as this is\nproportional to whether a proof is declarative in style, and thus, readable.System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.You will receive a Lean 4 proof you must\nmodify to eliminate any errors so that it compiles correctly and eliminate any “sorry”s with\nfull proofs.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a\nformal, complete, and correct Lean 4 proof by ﬁlling in its tactic proof.B ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this section, we provide more detailed information on the experimental setup and results used to\nevaluate ImProver.B.1 ABLATION DETAILS\n\nWe now proceed to show detailed results from our ablation testing.Table 8: Output and Chain-of-States Ablations\n\nOutput Format\n\nCoS\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.string\nTrue\nstring\nFalse\nstring list True\nstring list\nFalse\nstring tree\nTrue\nstring tree\nFalse\n\n7.53\n7.03\n8.04\n7.04\n7.62\n6.31\n\n16.12\n19.67\n12.38\n13.58\n15.34\n14.17\n\n46.72%\n35.77%\n64.96%\n51.82%\n49.64%\n44.53%\n\n16.79%\n15.33%\n21.17%\n18.98%\n18.25%\n16.06%\n\nBy Table 8, we see that the optimal combination in this testing group is a string list output\nformat with CoS enabled.Fix these values for all future tests.Examples\n\nTable 9: Example Retrieval Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.0\n3\n5\n7\n10\n\n5.67\n8.49\n8.38\n7.56\n9.34\n\n67.15%\n62.04%\n64.96%\n62.77%\n63.5%\n\n16.79%\n19.71%\n21.17%\n19.71%\n21.9%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the number of examples.By Table 9,\nwe see that the optimal combination in this testing group is 10 examples.Fix this value for all future\ntests.8.44\n13.68\n12.9\n12.04\n14.7\n\n16\n\n\fTable 10: Sampling Method Ablations\n\nMethod\n\nForward Keep Best\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.None\nreﬁnement\nreﬁnement\nreﬁnement\nreﬁnement\nbest-of-n\n\nN/A\n1\n5\n1\n5\nN/A\n\nN/A\nFalse\nFalse\nTrue\nTrue\nN/A\n\n9.34\n14.76\n12.5\n14.95\n13.15\n15.35\n\n14.7\n30.63\n20.88\n14.95\n13.15\n18.44\n\n63.5%\n48.18%\n59.85%\n100.0%\n100.0%\n83.21%\n\n21.9%\n30.66%\n30.66%\n30.66%\n29.93%\n36.5%\n\nNote that forward and keep-best values are parameters for reﬁnement of how many previous itera-\ntions to forward, and whether to keep the most recent or the best iteration in subsequent reﬁnement\nsteps.By\nTable 10, we see that the optimal combination in this testing group is best-of-n.Fix this value\nfor all future tests.Table 11: Model and n Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.Model\n\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\n\nn\n\n3\n5\n7\n10\n15\n3\n5\n7\n10\n15\n20\n\n19.66\n20.12\n22.44\n21.73\n23.51\n3.65\n5.12\n3.65\n4.99\n4.35\n4.87\n\n24.36\n24.97\n27.21\n25.28\n26.28\n4.63\n6.21\n4.34\n5.69\n5.06\n5.56\n\n80.7%\n80.56%\n82.46%\n85.96%\n89.47%\n78.95%\n82.46%\n84.21%\n87.72%\n85.96%\n87.72%\n\n38.6%\n36.11%\n42.11%\n40.35%\n45.61%\n8.77%\n10.53%\n8.77%\n12.28%\n12.28%\n14.04%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the value of n and model.By\nTable 11, we see that the optimal combination in this testing group is GPT-4o with n = 15.Fix this\nvalue for all future tests.Table 12: RAG and Combination Sampling Method Ablations\n\nRAG Improvement Nonempty Improve. Accuracy\n\nImproved Acc.Combination\n\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nbest-of-n\nbest-of-n\n\nm\n\nm′\n\n5\n3\n5\n3\n3\n5\n3\n5\n5\n3\n5\n3\n3\n5\n5\n3\nN/A 15\nN/A 15\n\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n33.78\n31.23\n31.85\n31.35\n51.32\n50.1\n57.56\n49.75\n32.71\n33.48\n\n100.0%\n100.0%\n100.0%\n100.0%\n63.64%\n65.62%\n60.61%\n59.38%\n90.62%\n84.38%\n\n50.0%\n46.88%\n50.0%\n50.0%\n48.48%\n53.12%\n54.55%\n43.75%\n56.25%\n53.12%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the combination methods and if\nRAG is enabled.B.2 ADDITIONAL QUALITATIVE EXAMPLES\n\nIn this section, we provide qualitative examples demonstrating the improvements ImProver achieves\nin proof optimization.33.78\n31.23\n31.85\n31.35\n32.66\n32.88\n34.88\n29.54\n29.64\n28.25\n\n17\n\n\fCompﬁles: Length Optimization See (§4.3)\n\nCompﬁles: Readability Optimization Consider Figure 5, in which a lemma from the 2019 IMO\nproblem 1 (from the Compﬁles dataset) is optimized for readability.This introduces multiple new\nhypotheses, which generalize a linear_property of the functions, and then reuses and instanti-\nates that (and others, too) hypothesis throughout the proof, creating a signiﬁcantly more declaritive,\nmodular, and therefore readable proof.Mathlib: Readability Optimization Consider Figure 9, a theorem from Mathlib that we optimize\nfor readability.This original proof carried a score of 0, as it does not contain any declarative statements.It is concise\nand efﬁcient, however, it is difﬁcult to understand and read.18\n\n\fOriginal (human-written)\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\next x; constructor\n· rintro (hxs, xnti | hxt, xnsi)\n\nImProver (length-optimized)\n\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\nexact Set.ext fun x => hfun h => h.elim\n\n(fun hxs, xnti => hOr.inl xs, fun h_, xti =\n> xnt xti) (fun hxt, xnsi => hOr.inr xt,\nfun hxs, _i => xns xsi),\n\nfun hh, nxsti => h.elim (fun xs => Or.inl h\nxs, fun xt => nxst hxs, xtii) (fun xt =>\nOr.inr hxt, fun xs => nxst hxs, xtii)i\n\n· constructor\n\nleft\nexact xs\nrintro h_, xti\ncontradiction\n\n.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.This intermediate tactic greatly helps in the understandability and clarity of the proof.Original (human-written)\n\nImProver (readability-optimized)\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n:=\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]; rfl\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\n:=\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nhave h1 : ∀ x y : Y, Subsingleton\n\n(Path.Homotopic.Quotient x y) ↔ ∀ {p1 p2\n: Path x y}, Path.Homotopic p1 p2 := by\nintro x y\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]\nrfl\n\nsimp only [h1]\n\nFigure 9: Optimizing a theorem from Mathlib/FundamentalGroupoid/SimplyConnected\nfor readability\n\n19",
          "time": 561.8994400501251,
          "input_token_size": 16880,
          "output_token_size": 107,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_2ec24144",
        "created_at": 1749869951.8731425,
        "processing_time": 565.1924257278442,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.829852"
      }
    }
  },
  {
    "46051f6c": {
      "question": "Where were the experimental results of ImProver tested?",
      "answer": "Answer is not found.",
      "provenance": [],
      "metadata": {
        "question_id": "paper_46051f6c",
        "created_at": 1750198692.448536,
        "processing_time": 1.1737618446350098,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "default",
        "sentence_count": 999999,
        "provenance_count": 0,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 999999,
            "provenance_count": 0,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.830851"
      }
    }
  },
  {
    "4c840760": {
      "question": "What challenges do human-written proofs present for proof assistants like Lean?",
      "answer": "1. Imprecision and ambiguity in human-written proofs.\n2. Low readability and excessive detail in formal proofs.\n3. Difficulty in generating correct outputs when optimizing for various metrics.\n4. Variability in proof styles that may not align with optimal use of limited human-written proofs.\n5. Challenges in maintaining correctness while optimizing for length or readability.\n6. High cost and slow runtime of proof optimization systems.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance": "4\n2\n0\n2\n \nt\nc\nO\n \n7\n \n \n]\nI\n\nA\n. s\nc\n[\n \n \n1\nv\n3\n5\n7\n4\n0\n.0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.To this end, we study a new problem of automated proof\noptimization: rewriting a proof so that it is correct and optimizes for an arbitrary\ncriterion, such as length or readability.As a ﬁrst method for automated proof opti-\nmization, we present ImProver, a large-language-model agent that rewrites proofs\nto optimize arbitrary user-deﬁned metrics in Lean.We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.In\npractice, however, informal, natural-language proofs are prone to imprecision, ambiguity, and error.Using a formal language such as Lean (Moura & Ullrich, 2021) removes ambiguity and precision\nand enables a proof assistant to verify correctness down to the primitives of a formal axiomatic\nsystem.Formal proofs, however, can be hard to read and often suffer from low reusability or excessive detail.For example, formal proofs in Lean’s extensive mathematical library, Mathlib (mathlib Community,\n2020), are generally designed to be concise and very general, often at the expense of readability.Formal proofs in an expository text, in contrast, may include detailed calculations steps, making\nthem readable but verbose.Machine learning systems trained on such proofs learn to mimic these\nvaried conventions (Hu et al., 2024), which may not be the optimal use of the limited supply of\nhuman-written proofs.As a result, we would like to be able to automatically refactor proofs to meet\na secondary objective such as length or readability.To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is\ncorrect and optimizes a criterion such as length or readability.We ﬁnd that naively applying LLMs\nto proof optimization falls short, often resulting in incorrect or poorly optimized proofs.We develop\nvarious improvements that can be applied on top of a black-box language model, including Chain-\nof-States prompting–an analogy to chain-of-thought prompting (Wei et al., 2022) that shows inter-\nmediate proof states–along with error-correction and retrieval.We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.1\n\n\fWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more readable, and more declarative in style.1\n\nOriginal (human-written)\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\nImProver (length-optimized)\n\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nrw [h1u _ (Classical.choose_spec _)]\nobtain hw, h1e', h1u'i := h1 y\nrw [h1u' _ ((h2 _ _).mpr h1e)]\nexact h1u' _ (Classical.choose_spec _)\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nhave h2' : Classical.choose (h1 x).exists =\n\ny :=\nh1u _ (Classical.choose_spec (h1\nx).exists)\n\nrw [h2']\nobtain hw, h1e', h1u'i := h1 y\nhave h4 := Classical.choose_spec (h1\n\nhave hxw : x = w := by\n\ny).exists\n\napply h1u'\nrw [h2]\nexact h1e\n\nrw [hxw]\nexact h1u' _ h4\n\nFigure 1: ImProver automatically rewrites formal proofs to optimize a criterion such as length\nor readability while remaining correct.In this example, ImProver optimizes a human-written\nlemma (right) from the 2022 International Math Olympiad (Question 2, solution from Comp-\nﬁles (David Renshaw, 2024)) for length.ImProver’s optimized proof is correct and more concise.2 RELATED WORK\n\nRecently there has been wide interest in automating theorem proving in interactive proof assistants;\nsee (Lu et al., 2023; Li et al., 2024) for surveys.A typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs\nsuch as Lean’s Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024).A model learns from the distribution of proofs in the cor-\npus, such as Mathlib-style proofs.Recently, the AlphaProof (AlphaProof & Teams, 2024) system\nwas shown to produce proofs with an arcane, non-human structure and syntax.We consider the\nnew problem of rewriting a proof to optimize a metric, such as rewriting a proof into a more read-\nable or more concise one.Proof optimization is more general than theorem proving, since we can\nalso rewrite an empty proof to optimize correctness.Finally, there is a rich literature on the var-\nied styles of (human) formal proofs (e.g., (Autexier & Dietrich, 2010; Wiedijk, 2004)).Our model,\nImProver, builds on neural theorem proving techniques including full proof generation (Jiang et al.,\n2023; First et al., 2023), conditioning on example proofs (Jiang et al., 2023), retrieval (Yang et al.,\n2023; Thakur et al., 2024), and preceding ﬁle context (First et al., 2023; Hu et al., 2024), as well as\nerror correction (Madaan et al., 2023; Chen et al., 2023) and documentation retrieval (Zhou et al.,\n2023) from code generation.ImProver brings these code generation techniques, along with new\nChain-of-States prompting and meta-programmed contextual information, into a uniﬁed proof opti-\nmization agent.3 AUTOMATED PROOF OPTIMIZATION WITH ImProver\n\nAutomated Proof Optimization.Given a theorem statement x, additional context c, and an initial\nproof y0, proof optimization consists of generating a new proof y that is correct and minimizes (or\nmaximizes) a metric µ(x, c, y0, y) → R.\n\n1Code is available at https://github.com/riyazahuja/ImProver.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.Note that shorter proofs often\nrepresent more efﬁcient proofs.Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.Concretely, we evaluate this using the ratio of number of explicitly typed\nhave tactics to total number of tactic invocations.Completion Metric: The completion of a proof simply describes its correctness.This is a trivial\nmetric which measures the number of errors present.The completion metric is used for concretely\nviewing proof optimization as a generalization of neural theorem proving.3.1\n\nIMPROVER\n\nWe develop several improvements that can be applied to a black-box LLM generator yout ∼\nG(·|xin), such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these pa-\nrameters.The explicit prompts and templates that are sent to the LLM can be found in (§A).3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.The intermediate states often contain valuable information (e.g., an expression\nafter it has been simpliﬁed) that is not present in the tactics.To allow the model to reason about\nthese intermediate goals and hypotheses, we use tools from Lean metaprogramming to automatically\nannotate each proof state as a comment prior to each tactic.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.The implementation of this extraction system is mod-\neled from the work (Kim Morrison, 2024).Speciﬁcally, in the compiler’s elaboration and eval-\nuation stages – where the parsed theorem code is ﬁrst converted into concrete syntax trees (in\npractice, Syntax objects) and abstract syntax trees (Expr objects) – we convert the CST and\nAST output objects into the relevant proof data and proof states in the form of proof trees\n(Lean.Elab.InfoTree).These proof trees contain detailed context and information on a tactic-\nby-tactic level relating to the modiﬁcation of the proof state, metavariable context, and proof cor-\nrectness.After state extraction is completed and cached for efﬁcient future access, we annotate the proof text\nitself to contain the intermediate states in the form as comments.Figure 2 shows an example. This explicit reasoning aims to help the generator model construct more optimized proofs via addi-\ntional symbolic data.3.1.2 OUTPUT FORMATTING. LLM outputs often contain ancillary and syntactically invalid content, especially before and after\nthe actual proof.Additionally, by applying additional structure to the LLM outputs, we may hope to\ngenerate more structured proofs.To analyze this hypothesis, we introduce two additional output for-\nmats to the standard str output: flat and structured.The former enforces a tactic sequence\noutput as a list of strings, and the latter enforces a proof tree output as a tree of strings.3.1.3 SAMPLING METHOD\n\nWe also introduce different methods of sampling between many (sequential or parallel) LLM infer-\nence calls, involving best-of-n and iterative reﬁnement implementations, as well as combinations\nthereof.3\n\n\fWithout Chain-of-States\n\nWith Chain-of-States\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u) := by\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u)\n\n:= by\n\nrintro x (hxs, xti | hxs, xui)\n· use xs; left; exact xt\n.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!-/\n. use xs; right; exact xu\n/-\nGoals Solved!-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.More speciﬁcally, our scoring function is given by the 2-ary\ncomparison function S, whose arguments are output objects y, y′.max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.This comparison function can be extended to evaluate the best output of any ﬁnite n via induction.This best-of-n technique is implemented as a curried function such that each of the n calls can be\nhandled by any arbitrary sampling method, or just a single standard prompt at user discretion.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.The reﬁnement process relies on user-deﬁned pa-\nrameters n and prev_num to specify the number of iterations and the number of previous iteration\ninfo to forward, respectively.Each iteration carries information on the last prev_num iterations,\nincluding input, output, metric score, correctness, and error messages.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.Similar to the best-of-n tech-\nnique, it relies on an argument n for the number of reﬁnement steps, and is curried such that each\nreﬁnement step can be handled by any other prompting function.However, unlike best-of-n, there is\nno opportunity for parallelism as each iteration is dependent on information from the previous call.Combination Sampling and Compound Prompt Functions Compound prompt functions utilize\nthe curried nature of the implementations of best-of-n and reﬁnement to nest these techniques within\none another.For example:\n\n4\n\n\fbest_of_n((refinement,m),n) is a compound sampling method that run a best-of-n,\nwhere each call is a m-step reﬁnement.refinement((best_of_n,m),n) is a compound sampling method that runs a n-step reﬁne-\nment, where each call is a best-of-m call to the LLM.Note that with each of these compound prompt functions, there are always a total of mn iterations.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.More speciﬁcally, example retrieval selects the most relevant user-generated examples of proof op-\ntimization on a speciﬁc metric.Namely, each metric is loaded with a cached (vector) database\npopulated with human-made examples of preoptimized and postoptimized pairs of Lean theorems.The number of examples that are retrieved is user-speciﬁed.Document retrieval extracts information using MMR from a pair of ﬁxed (vector) databases.The\ndatabases store semantically chunked data from the Theorem Proving in Lean (TPiL) handbook –\ncontaining syntax guides and tactic explanations – and the Mathlib mathematics libary – containing\nthousands of theorems and lemmas.The chunking is handled by a recursive character splitter, which\nsplits the TPiL markdown ﬁles at on its headers and Mathlib ﬁles at the start of theorems, examples,\nlemmas, and deﬁnitions – with chunk sizes of 1000 characters with a 200 character overlap.The Mathlib retriever ﬁnds the top k documents that score the highest MMR score against the current\ntheorem, the TPiL retriever ﬁnds the top k documents that score the highest MMR score against the\ncurrent theorem in context and all current error messages.This retrieval process helps in generating\nmore contextually accurate prompts that allow the language model to better correct its own errors as\nwell as ﬁnd useful lemmas to reference.4 EXPERIMENTS\n\nWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics and compare its results to those of the base GPT-4o and GPT-4o-mini\nmodels.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.4.1 SETUP\n\nOur experimentation is split into three distinct stages.We ﬁrst perform ablation testing on the\nImProver model parameters (§3.1) to ensure that ImProver’s parameter speciﬁcation is the optimal\none with respect to correctness and metric optimization score.We then evaluate this optimal param-\neter combination on datasets of varying complexity and analyze the performance and results thereof.Lastly, we note the performance of ImProver in NTP applications in comparison to the base GPT-4o\nand GPT-4o-mini models.Datasets. We evaluate ImProver on subsets of the following datasets.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.We use exercise solutions from set theory, elementary number theory, group\ntheory, topology, differential calculus, and integration & measure theory.This dataset contains the-\norems at an undergraduate-level of complexity.For our main results, we evaluated on 72 theorems\nfrom exercise solutions from MIL chapters 4, 5, 8, 9, and 10.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.This is a\ndataset of internationally-renowned competitive math problems, many of which are readable, yet\nquite verbose.This dataset contains theorems of a competitive format, and although they contain\n\n5\n\n\fconcepts only at a high-school level, the logical complexity of internationally-renowned competi-\ntion results is far above that.For our main results, we used all 26 theorems and lemmas from the\nCompﬁles database of complete solutions to the International Mathematics Olympiad (IMO) and\nthe American Mathematics Olympiad (USAMO) from 2016-2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.These proofs are extremely\nefﬁcient, concise, and generalized - which often comes at the cost of readability and understandabil-\nity.These results and theorems often are at the cutting edge of research.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.This is the most difﬁcult dataset. Models.Our base generator uses GPT-4o (OpenAI et al., 2024).Since no prior methods currently\nexist for automated proof optimization, we consider a prompted GPT-4o without the improvements\ndescribed in (§3.1) as our baseline.Additionally, for a given metric, we write a prompt that brieﬂy\ndescribes the metric and the proof optimization task.We also provide instructions, context, and in-\nformation depending on the features selected, and add the theorem and proof to the prompt.Speciﬁc\nprompt information is detailed in (§A)\n\nPerformance metrics.Since proof optimization is a new task, we deﬁne four performance metrics\nfor measuring aspects of correctness and improvement.First, we deﬁne improvement for length as percentage change in length, µlen(y0)−µlen(y)\n× 100.For\nreadability, we use the difference, µread(y)−µread(yo).If no correct output is generated by the model\nfor a speciﬁc theorem, improvement is deﬁned to be zero.We deﬁne nonempty improvement as\nthe improvement restricted to theorems for which some output has nonzero improvement.µlen(y0)\n\nIntuitively, improvement is the expected improvement in metric score from the input to output, ac-\ncounting for errors in the generation.The nonempty improvement score is the expected improvement\nin metric score, given that there are no errors in the generation.Similar improvement scores can be\ndeﬁned for other metrics using a binary function of the metric assigned to the original and optimized\nproofs.Additionally, the accuracy is the percentage of theorems in the dataset which the model was able to\ngenerate a correct output for.The improved accuracy is the percentage of theorems in the dataset\nwhich the model was able to generate a correct output for, as well as improve the metric to be\nnonzero.4.1.1 ABLATIONS\n\nWhen performing our ablation studies, we used a ﬁxed dataset (MIL) and metric (length) and varied\nthe parameters of all the features to ﬁnd the optimal combination.However, as there are over 8640\npossible combinations, it is inefﬁcient to test all combinations at once.As such, we evaluate using a\nfactorial testing method. Testing Groups.We deﬁne the following testing groups with the speciﬁed parameter combina-\ntions:\n\nGPT-4o-mini/GPT-4o: This varies the GPT-4o model, outputting a string with no other features.Output and CoS: We evaluate the effects of different output formatting styles (string, string\nlist, string tree) and CoS (True, False), with the model ﬁxed as GPT-4o, with no other\nfeatures enabled.Example Retrieval: We evaluate the effects of increasing the number of examples provided (multi-\nshot prompting) in the range of 0, 3, 5, 7, and 10, with the model ﬁxed as GPT-4o, CoS and output\nformatting ﬁxed as the best combination from the previous test, and no other features enabled.Sampling Method: Here, we evaluate the effects of best-of-n and reﬁnement for a ﬁxed n = 5.Additionally we test on the reﬁnement cases if forwarding the most recent iteration result, or all\nprevious iteration results is the best, and if we should keep the best out of the iterations, or the most\n\n6\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 1: Average Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nMetric\n\nModel\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n3.7\n20.96\n\n2.21\n9.34\n\n6.25\n30.54\n\n4.18\n13.45\n\n15.15\n55.29\n\n8.02\n30.53\n\n18.58\n56.56\n\n14.48\n30.97\n\n26.36%\n100.0%\n\n18.75%\n100.0%\n\n8.31%\n35.44%\n\n6.13 %\n24.56%\n\n37.5%\n100.0%\n\n28.85%\n100.0%\n\n14.42%\n50.0%\n\n11.54%\n34.21%\n\nTable 2: MIL Proof optimization results.Improvement Nonempty Improvement Accuracy\n\nImproved Acc. recent.The model is ﬁxed as GPT-4o, CoS, output formatting, and examples are ﬁxed as the best\ncombination from the previous test, and no other features enabled.n and Model: Here, we evaluate the effects of larger n values and different models.We test n =\n3, 5, 7, 10, 15 on GPT-4o and GPT-4o-mini, as well as n = 20 for GPT-4o-mini (as it is of a far\nlower token cost).CoS, output formatting, examples, and sampling method are ﬁxed as the best\ncombination from the previous test, and no other features enabled.Combos and RAG: We evaluate combination methods refinement(best_of_m',m) and\nbest_of_m'(refinement(m)), for m 6= m′ with mm′ equal to the optimal value m from\nthe previous test.We also test the effect of enabling document retrieval.Model, CoS, output for-\nmatting, examples, n, and sampling method are ﬁxed as the best combination from the previous\ntest.Ablation data. We evaluate our ablations on a subset of MIL.However, due to the increase in\nmodel calls for larger n values, we switch a representative sample of this subset for some test groups.Namely,\n\nGPT-4o-mini, GPT-4o, Output and Cos, Example Retrieval, and Sampling Method are tested\non the 133 theorems in the solutions of C03_Logic, C04_Sets_and_Functions, and\nC05_Elementary_Number_Theory.n and Model are tested on 55 theorems from a representative sample of the aforementioned, and\nCombos and RAG are tested on a representative sample of 32 theorems from the aforementioned.4.2 RESULTS\n\nImProver is capable of optimizing proofs in all settings.From Table 2, Table 3, and Table 4,\nwe can see that ImProver is capable of optimizing proofs on all datasets for both the length and\nreadability metrics.Furthermore, Table 1 shows that across all metrics, ImProver signiﬁcantly\noutperforms GPT-4o on proof optimization tasks on every experimental measure – aggregated from\nall datasets.Additionally, from Table 2, Table 3, and Table 4, we can see that ImProver outperforms\nGPT-4o on each dataset as well.We proceed to analyze this data and its implications. Length optimization.First focusing on the length metric, we see that ImProver outperforms GPT-\n4o with respect to the improvement score by 566% (aggregated over all datasets).Additionally, we\nare guaranteed that ImProver produces a correct output, although that output may just be the same\nas the input.However, 35.44% of the time, it generates a correct output that is not the same length\nas the input, and in that case, we expect an average of a 55.29% reduction in length.Comparing\nthis with GPT-4o, we conclude that not only can ImProver optimize at a higher level on arbitrary\ntheorems, but its ability to generate nontrivial correct outputs is far greater in comparison to GPT-4o.7\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 3: Compﬁles Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n2.75\n18.86\n\n0.39\n5.74\n\n0.0\n6.19\n\n0.0\n4.63\n\n30.7\n54.48\n\n3.38\n24.89\n\n0.0\n53.65\n\n0.0\n33.19\n\n11.54%\n100.0%\n\n14.1%\n100.0%\n\n5.13%\n34.62%\n\n1.28%\n19.23%\n\n16.67%\n100.0%\n\n4.65%\n100.0%\n\n0.0%\n11.54%\n\n0.0%\n11.63%\n\nMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 4: Mathlib Proof optimization results. Readability optimization.Readability optimization is similar, with ImProver outperforming\nGPT-4o by 423%.Moreover, the accuracy, improved accuracy, and nonempty improvement dis-\nparities for readability parallel those of the length tests.However, it should be noted that for both\nGPT-4o and ImProver, the accuracy and improved accuracy scores were markedly smaller for read-\nability than length optimization.This suggests that for both models, it was generally more “difﬁcult”\nto generate a correct output, and moreover, generate a correct output with a better metric score\nthan the input, for readability optimization than length optimization.In other words, optimizing for\nreadability is more difﬁcult for the underlying generator than optimizing for length.However, we\nspeculate with higher-quality prompts, descriptions of the metric, and examples, this disparity can\nbe minimized.Regardless, we note that different metrics can be less likely to be correctly optimized,\nand that model performance is correlated with the metric it seeks to optimize – both for GPT-4o and\nImProver.Optimization varies based on dataset difﬁculty.Additionally noting Table 2, Table 3, and\nTable 4, we observe that the improvement score for both metrics for both GPT-4o and ImProver\nis highest for the MIL dataset, lower for Compﬁles, and the lowest on the Mathlib theorems.This\nsuggests that the expected improvement in metric score decreases with higher difﬁcultly – with\nundergraduate-level theorems having a signiﬁcantly higher expected improvement than research-\nlevel theorems.However, it should be noted that for both metrics, the nonempty improvement of\nImProver stayed consistent, whereas for GPT-4o, it followed the aforementioned trend of decreas-\ning with difﬁculty.Similarly, the accuracy and improved accuracy scores for both metrics and\nmodels decreased with higher difﬁculty datasets (disregarding ImProver’s accuracy scores, as they\nare ensured to be 100%).This suggests that although the base GPT-4o generator is less likely to\ngenerate a correct output for higher difﬁculty datasets, the improvements that ImProver makes to\nthe base generator allows it to maintain its improvement in the metric score whenever a correct out-\nput is generated.As such, we can speculate that the bottleneck in the improvement score is not the\nmodel’s ability to optimize the proof for a metric, but rather its ability to generate a new correct\nproof at all.As such, we conjecture that with more capable generator models, the accuracy – and\nthus, the improvement score – in optimization tasks will continue to increase, until the improvement\nscores match the nonempty improvement.Overall, we conclude that although the performance of both ImProver and GPT-4o decreases on\nlength and readability optimization on more difﬁcult datasets, ImProver signiﬁcantly outperforms\nGPT-4o on all datasets for length and readability optimization.4.2.1 ABLATION TESTING\n\nWe perform ablation studies using a subset of the MIL dataset as discussed in §4.1.1.The results\nof this factorial study are aggregated in Table 5.We measure the baseline results from the GPT-\n4o and GPT-4o-mini models, noting that GPT-4o is the better-scoring model (with respect to the\nimprovement score).Thus, ﬁxing this model, we vary the output formatting type and if CoS is\nenabled, and determine that outputting flat with CoS enabled maximizes the improvement score.8\n\n\fTable 5: Ablation results.Each cell in the ablation tests shows best / worst, which are the best\nand worst parameter combinations in the test group.The ImProver speciﬁcation outputs the input\ntheorem when no correct proof is generated, which results in an accuracy of 100% on MIL.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o-mini\nGPT-4o\n+ Output and CoS\n+ Example Retrieval\n+ Sampling Method\n+ n and Model\n+ Combos and RAG\nImProver\n\n0\n7.03\n8.04 / 6.31\n9.34 / 5.67\n15.35 / 9.34\n23.51 / 3.65\n34.88 / 28.25\n\n0\n19.67\n12.38 / 14.17\n14.7 / 8.44\n18.44 / 14.7\n26.28 / 4.63\n57.56 / 33.48\n\n3.62%\n35.77%\n\n0%\n15.33%\n\n64.96% / 44.53% 21.17% / 16.06%\n63.5% / 67.15%\n21.9% / 16.79%\n36.5% / 21.9%\n83.21% / 63.5%\n89.47% / 78.95% 45.61% / 8.77%\n60.61% / 84.38% 54.55% / 53.12%\n\n34.88\n\n57.56\n\n100%\n\n54.55%\n\nTable 6: CoS Readability Ablation results.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o\nImProver, CoS Disabled\nImProver\n\n4.97\n9.23\n16.69\n\n15.89\n24.61\n31.42\n\n37.5%\n100.0%\n100.0%\n\n12.5%\n28.12%\n46.88%\n\nFixing these parameters, we now vary the number of examples retrieved, noting that prompting\nwith 10 examples maximizes the improvement score.Fixing this parameter, we vary the sampling\nmethods (excluding compound methods and ﬁxing n = 5) and observe that best-of-n is the best\nparameter combination.Now, as GPT-4o-mini is signiﬁcantly less computationally expensive than\nits GPT-4o counterpart, we test both models with the sample method ﬁxed to best-of-n, and vary\nn = 1, 3, 5, 7, 10, 15, and for GPT-4o-mini, also n = 20.We conclude that GPT-4o with n = 15\nis the most effective.Fixing these parameters, we consider all mixed compound sampling methods\nwith and without document retrieval enabled, concluding that a 5-step reﬁnement with best-of-3 on\neach iteration, with RAG enabled, is the optimal combination.Thus, as we can see from Table 5, the optimal parameter combination comes from gpt-4o outputting\nas a string list with CoS, RAG, 10 examples, 5-step reﬁnement with each iteration being a\nbest-of-3 evaluation.Changing any one of these parameters them leads to a reduction in performance.Additional ablation data can be found at (§B.1).Readability and Chain-of-States (CoS) Ablation.We additionally examine the effects of dis-\nabling CoS on readability optimization tasks, as the previous study focused on length optimization\ntasks, and we speculate that CoS has a high impact on the performance of readability optimization\ntasks, as the proof states that are embedded due to CoS seem to be a critical aspect to generating the\nexplicit declarations that the readability metric measures.We conﬁrm this result by considering Table 6 and observe that simply enabling CoS nearly doubles\nthe improvement score, and signiﬁcantly improves the nonempty improvement score, suggesting that\nCoS has a high degree of impact on optimizing for the readability metric, as conjectured.However,\nwe also note a signiﬁcant increase in improved accuracy, which suggests that embedding the chain\nof states also improves the ability of the model to generate nontrivial correct outputs, implying that\nthe symbolic information contained in the states are critical to effectively modifying the structure\nand content of a proof.4.2.2 NEURAL THEOREM PROVING EVALUATION\n\nWe evaluate ImProver’s neural theorem proving (NTP) performance using the completion metric.We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input\nproof.Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-4o.ImProver substantially outperforms GPT-4o across all topics, with an 80% increase in accuracy\ncompared to the base model, showing that proof optimization systems are easily extendable to NTP\nsystems.9\n\n\fTable 7: Proof generation results.Each cell shows percent accuracy.MIL\n\nSet Theory Group Theory\n\nOverall\n\nGPT-4o\nImProver\n\n18.18%\n45.45%\n\n25%\n33.33%\n\n21.73%\n39.13%\n\n4.3 QUALITATIVE RESULTS\n\nNext, we discuss qualitative examples showing the improvements from ImProver in proof optimiza-\ntion.These examples show the balance between correctness and the desired optimization metric,\nshowing how ImProver can produce more concise or readable proofs depending on the use case.Additional examples can be found in (§B.2)\n\nExample 1: Compﬁles Optimization.Consider Figure 1, a lemma from the 2022 IMO Question\n2 (Compﬁles) that we optimize for length.The original proof consisted of 12 tactic steps and multiple intermediate calculations.After applying\nImProver for length optimization, the proof was halved to only 6 tactics, eliminating redundant steps\nwhile maintaining correctness.In comparison, the shortened proof makes multiple nontrivial optimizations, such as eliminating the\nh2’ and h4 and hxw hypotheses, as well as fully generating proof terms for speciﬁc rewrites and\nother tactics.Example 2: MIL. Consider Figure 3, a result from MIL that we optimize for readability.Original (human-written)\n\nImProver (readability-optimized)\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ncard G = card H * card K)\n\n: K ≃* G / H := by\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\n· rw [← ker_eq_bot_iff, (QuotientGroup.mk'\n\nH).ker_restrict K]\nsimp [h]\n\n· symm\n\nexact aux_card_eq h'\n\ncard G = card H * card K)\n:= by\n\n: K ≃* G / H\n\nhave injectivity : Function.Injective\n\n((QuotientGroup.mk' H).restrict K) := by\nrw [← ker_eq_bot_iff, (QuotientGroup.mk'\nH).ker_restrict K]\nsimp [h]\n\nhave card_eq : card (G / H) = card K := by\n\nexact aux_card_eq h'\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\nexact injectivity\nsymm\nexact card_eq\n\nFigure 3: Optimizing a group-theoretic result from MIL Chapter 8 Section 1 for readability.We\ndeﬁne a proof to be readable if it is written in a declarative style, which consists of intermediate\nconjectures (have · · · statements).ImProver introduces two intermediate conjectures into the proof.This original proof carried a score of 0, as it does not contain any declarative statements.In compar-\nison, after applying ImProver, we transformed the proof to be more declarative, with many more\nintermediate steps with explicit have tactics for improved clarity.Additionally observe how the\nmodel deﬁnes hypotheses for use in the latter half of the proof; these predeﬁned hypotheses could\neasily be converted into standalone lemmas for reuse.Example 3: Full Proof Generation.We analyze the application of ImProver to neural theorem\nproving in the MIL example from Figure 4.This theorem relating to group theory originally has no proof, however, ImProver generates one\nfrom scratch.This generated proof is veriﬁed to be correct by Lean, utilizing all the included hy-\npotheses as well as a retrieved mathlib theorem.10\n\n\fOriginal (human-written)\n\nImProver (completeness-optimized)\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n: S ≤ T) : comap ϕ S ≤ comap ϕ T := by\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n\n: S ≤ T) : comap ϕ S ≤ comap ϕ T\n\n:= by\n\nsorry\n\nintro g\nsimp only [mem_comap]\nintro hS\nexact hST hS\n\nFigure 4: Solving a group theorem exercise from MIL Chapter 8 Section 1 for readability.5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.By incorporating CoS, RAG, and other features, ImProver signiﬁcantly outperforms base\nlanguage models in proof optimization over undergraduate, competition, and research-level prob-\nlems.However, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance\non black-box LLM’s.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.As such, we believe that ImProver sets the stage for further\nwork on proof optimization to advance the study and use of AI in mathematics.ACKNOWLEDGEMENTS\n\nRiyaz Ahuja thanks the L3 Lab and Hoskinson Center for Formal Mathematics for their support.Sean Welleck thanks Convergent Research, the Lean FRO, and the OpenAI Researcher Access Pro-\ngram for their support.11\n\n\fREFERENCES\n\nand\n\nAlphaGeometry\n\nAlphaProof\nstandard\nhttps://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\n2024.\n\nsilver-medal\nproblems.achieves\nolympiad\n\nmathematical\n\ninternational\n\nsolving\n\nTeams.AI\n\nSerge Autexier and Dominik Dietrich.A tactic language for declarative proofs.In Matt Kaufmann\nand Lawrence C. Paulson (eds.), Interactive Theorem Proving, pp.99–114, Berlin, Heidelberg,\n2010. Springer Berlin Heidelberg.Jaime Carbonell and Jade Goldstein.The use of mmr, diversity-based reranking for reordering doc-\numents and producing summaries.In Proceedings of the 21st Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ’98, pp.335–336,\nISBN 1581130155. doi:\nNew York, NY, USA, 1998.Association for Computing Machinery. 10.1145/290941.291025.URL https://doi.org/10.1145/290941.291025.Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.Teaching large language models\n\nto self-debug, 2023.URL https://arxiv.org/abs/2304.05128. David Renshaw. compﬁles.https://github.com/dwrensha/compfiles, 2024.Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun.Baldur: Whole-proof generation and\n\nrepair with large language models, 2023.Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu.Proof artifact co-\nIn International Conference on Learning\n\ntraining for theorem proving with language models.Representations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.Jiewen Hu, Thomas Zhu, and Sean Welleck.minictx: Neural theorem proving with (long-)contexts,\n\n2024.URL https://arxiv.org/abs/2408.03350.Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu.Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs.In The Eleventh International Conference on Learning\nRepresentations, 2023.URL https://openreview.net/forum?id=SMa9EAovKMC.Kim Morrison. lean-training-data.https://github.com/kim-em/lean-training-data,\n\nGuillaume Lample, Timothee Lacroix, Marie anne Lachaux, Aurelien Rodriguez, Amaury\nHypertree proof search\nHayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.for neural\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=J4pX8Q8cxHH. theorem proving. 2024. 2024.Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie\n\nSi.A survey on deep learning for theorem proving, 2024.Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang.A survey of deep\nlearning for mathematical reasoning.In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp.14605–14631, Toronto, Canada, July 2023.Association for Computational Linguistics.10.18653/v1/2023.acl-long.817.URL\nhttps://aclanthology.org/2023.acl-long.817.doi:\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-reﬁne:\nIterative reﬁnement with self-feedback.In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.URL https://openreview.net/forum?id=S37hOerQLB.12\n\nleanprover-community. mathematics_in_lean.https://github.com/leanprover-community/mathematics_in_lea\n\n\fThe mathlib Community.The lean mathematical library.In Proceedings of the 9th ACM SIGPLAN\nInternational Conference on Certiﬁed Programs and Proofs, POPL ’20.ACM, January 2020. doi:\n10.1145/3372885.3373824.URL http://dx.doi.org/10.1145/3372885.3373824.Leonardo de Moura and Sebastian Ullrich.The lean 4 theorem prover and programming lan-\nIn Automated Deduction – CADE 28: 28th International Conference on Automated\nguage.Deduction, Virtual Event, July 12–15, 2021, Proceedings, pp.625–635, Berlin, Heidelberg,\n2021. Springer-Verlag.ISBN 978-3-030-79875-8. doi: 10.1007/978-3-030-79876-5_37.URL\nhttps://doi.org/10.1007/978-3-030-79876-5_37.OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red\nAvila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Moham-\nmad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher\nBerner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brit-\ntany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek\nChen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,\nHyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas\nDegry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,\nAdrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada\nFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel,\nTarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff\nHarris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jo-\nmoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,\nNitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik\nKim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, An-\ndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,\nStephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim\nMalfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew\nMayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil,\nDavid Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin,\nVinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeon-\nwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley\nPantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov,\nAndrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde\nde Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea\nPower, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick\nRyder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David\nSchnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah\nShoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama,\nIan Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie\nSummers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin\nTootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón\nUribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welin-\nder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,\nHannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah\nYoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,\nShengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.Gpt-4 technical\nreport, 2024. URL https://arxiv.org/abs/2303.08774.Stanislas Polu and Ilya Sutskever.Generative language modeling for automated theorem proving,\n\n2020.13\n\n\fStanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\n\nSutskever.Formal mathematics statement curriculum learning, 2022.Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, and Swarat Chaudhuri.An in-\n\ncontext learning agent for formal theorem-proving, 2024.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H.\nthought prompting elicits reasoning\nChi, Quoc V Le, and Denny Zhou.in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=_VjQlMeSB_J.Chain of\n\nFreek Wiedijk. Formal proof sketches.In Stefano Berardi, Mario Coppo, and Ferruccio Damiani\n(eds.), Types for Proofs and Programs, pp.378–393, Berlin, Heidelberg, 2004.Springer Berlin\nHeidelberg. ISBN 978-3-540-24849-1.Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil,\nRyan Prenger, and Anima Anandkumar.LeanDojo: Theorem proving with retrieval-augmented\nlanguage models.In Neural Information Processing Systems (NeurIPS), 2023.Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig.Docprompting: Gener-\nating code by retrieving the docs.In The Eleventh International Conference on Learning Repre-\nsentations, 2023.URL https://openreview.net/forum?id=ZTCxT2t2Ru.14\n\n\fA PROMPTS\n\nA.1 TEMPLATE\n\nIn this appendix, we note the prompts used by ImProver both for general LLM prompting, as well\nas the metric-speciﬁc prompts.For the main prompt sent to the LLM on each sample, we build a prompt string using a chat prompt\ntemplate that is then invoked at runtime to ﬁll in the variables.Namely, these variables include the set of metric prompts, previous results, input theorem, context,\na syntax documents, Mathlib documents, and examples.The prompt template is a conversation of the format:\n\nPlaceholder: All metric prompts with a ‘System’ role\nSystem: You will be given the proof context (i.e.the lean ﬁle contents/imports leading up\nto the theorem declaration) wrapped by <CONTEXT>...</CONTEXT>.You will be given the previous num_prev input/output pairs as well as their metric (met-\nric.name) score and correctness score, as well as any error messages, for your reference to\nimprove upon.Each of these previous results will be wrapped with <PREV I=0></PREV\nI=0>,...,<PREV I=num_prev-1></PREV I=num_prev-1>, with I=num_prev-1 being the\nmost recent result.Remember to use lean 4 syntax, which has signiﬁcant changes from the lean 3 syntax.To\nassist with the syntax relating to the current theorem and current error messages, you will\nbe given num_syntax_docs documents to refer to for ﬁxing these syntax issues.Each of\nthese documents will be wrapped with <SYNTAX_DOC>...</SYNTAX_DOC>.You will also receive num_mathlib_docs documents relevant to the current theorem to\nhelp with formulating your modiﬁed proof.Each of these will be wrapped with <CON-\nTENT_DOC>...</CONTENT_DOC>\nYou will also receive num_examples examples of input-output pairs of proofs that\nwere optimized for the metric metric.Each of these will be wrapped with <EXAM-\nPLE>...</EXAMPLE>\nYou will be given the tactic states as comments for reference.The current theorem will be\nwrapped in <CURRENT>...</CURRENT>\nSystem: Output format instructions\nPlaceholder: All retrieved syntax documentation\nPlaceholder: All retrieved mathlib documentation\nPlaceholder: All retrieved examples\nUser: <CONTEXT> context </CONTEXT>\nPlaceholder: Previous results and inputs/outputs\nPlaceholder: All metric prompts with a ‘User’ role\nUser: <CURRENT> theorem </CURRENT>\n\nThis prompt is then invoked and sent to the language model by ﬁlling in all the variables and place-\nholders.Notably, when we invoke the chain given by chain|llm|parser, we throttle the invoca-\ntion with a randomized exponential rate limit throttling to account for API rate limits, especially in\nhighly-parallelized requests like when benchmarking over a large number of theorems.A.2 METRIC PROMPTS\n\nLength Metric\n\nSystem: You are an AI assistant who shortens Lean 4 proofs while ensuring their correct-\nness.You will aim to reduce the number of lines of the tactic proof while ensuring that it\nproperly compiles in Lean 4.15\n\n\fUser: Shorten the current theorem (wrapped in <CURRENT>...</CURRENT>) to be as\nshort in length—measured in the number of lines of the proof—as possible, while also\nensuring that the output is still syntactically correct.\"Readability Metric\n\nCompletion Metric\n\nSystem: You are an AI assistant who rewrites Lean 4 proofs to be more readable while\nensuring their correctness.We measure readablity by considering the ratio of the number\nofexplicitly typed have tactics against the total number of tactics in the proof, as this is\nproportional to whether a proof is declarative in style, and thus, readable.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is\nmore readable and declarative and modular.System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.You will receive a Lean 4 proof you must\nmodify to eliminate any errors so that it compiles correctly and eliminate any “sorry”s with\nfull proofs.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a\nformal, complete, and correct Lean 4 proof by ﬁlling in its tactic proof.B ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this section, we provide more detailed information on the experimental setup and results used to\nevaluate ImProver.B.1 ABLATION DETAILS\n\nWe now proceed to show detailed results from our ablation testing.Table 8: Output and Chain-of-States Ablations\n\nOutput Format\n\nCoS\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.string\nTrue\nstring\nFalse\nstring list True\nstring list\nFalse\nstring tree\nTrue\nstring tree\nFalse\n\n7.53\n7.03\n8.04\n7.04\n7.62\n6.31\n\n16.12\n19.67\n12.38\n13.58\n15.34\n14.17\n\n46.72%\n35.77%\n64.96%\n51.82%\n49.64%\n44.53%\n\n16.79%\n15.33%\n21.17%\n18.98%\n18.25%\n16.06%\n\nBy Table 8, we see that the optimal combination in this testing group is a string list output\nformat with CoS enabled.Fix these values for all future tests.Examples\n\nTable 9: Example Retrieval Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.0\n3\n5\n7\n10\n\n5.67\n8.49\n8.38\n7.56\n9.34\n\n67.15%\n62.04%\n64.96%\n62.77%\n63.5%\n\n16.79%\n19.71%\n21.17%\n19.71%\n21.9%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the number of examples.By Table 9,\nwe see that the optimal combination in this testing group is 10 examples.Fix this value for all future\ntests.8.44\n13.68\n12.9\n12.04\n14.7\n\n16\n\n\fTable 10: Sampling Method Ablations\n\nMethod\n\nForward Keep Best\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.None\nreﬁnement\nreﬁnement\nreﬁnement\nreﬁnement\nbest-of-n\n\nN/A\n1\n5\n1\n5\nN/A\n\nN/A\nFalse\nFalse\nTrue\nTrue\nN/A\n\n9.34\n14.76\n12.5\n14.95\n13.15\n15.35\n\n14.7\n30.63\n20.88\n14.95\n13.15\n18.44\n\n63.5%\n48.18%\n59.85%\n100.0%\n100.0%\n83.21%\n\n21.9%\n30.66%\n30.66%\n30.66%\n29.93%\n36.5%\n\nNote that forward and keep-best values are parameters for reﬁnement of how many previous itera-\ntions to forward, and whether to keep the most recent or the best iteration in subsequent reﬁnement\nsteps.Now, with the previous optimal parameters ﬁxed, run the ablation on the sample method.By\nTable 10, we see that the optimal combination in this testing group is best-of-n.Fix this value\nfor all future tests.Table 11: Model and n Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.Model\n\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\n\nn\n\n3\n5\n7\n10\n15\n3\n5\n7\n10\n15\n20\n\n19.66\n20.12\n22.44\n21.73\n23.51\n3.65\n5.12\n3.65\n4.99\n4.35\n4.87\n\n24.36\n24.97\n27.21\n25.28\n26.28\n4.63\n6.21\n4.34\n5.69\n5.06\n5.56\n\n80.7%\n80.56%\n82.46%\n85.96%\n89.47%\n78.95%\n82.46%\n84.21%\n87.72%\n85.96%\n87.72%\n\n38.6%\n36.11%\n42.11%\n40.35%\n45.61%\n8.77%\n10.53%\n8.77%\n12.28%\n12.28%\n14.04%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the value of n and model.By\nTable 11, we see that the optimal combination in this testing group is GPT-4o with n = 15.Fix this\nvalue for all future tests.Table 12: RAG and Combination Sampling Method Ablations\n\nRAG Improvement Nonempty Improve. Accuracy\n\nImproved Acc.Combination\n\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nbest-of-n\nbest-of-n\n\nm\n\nm′\n\n5\n3\n5\n3\n3\n5\n3\n5\n5\n3\n5\n3\n3\n5\n5\n3\nN/A 15\nN/A 15\n\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n33.78\n31.23\n31.85\n31.35\n51.32\n50.1\n57.56\n49.75\n32.71\n33.48\n\n100.0%\n100.0%\n100.0%\n100.0%\n63.64%\n65.62%\n60.61%\n59.38%\n90.62%\n84.38%\n\n50.0%\n46.88%\n50.0%\n50.0%\n48.48%\n53.12%\n54.55%\n43.75%\n56.25%\n53.12%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the combination methods and if\nRAG is enabled.By Table 12, we see that the optimal combination in this testing group is a 5-step\nreﬁnement with each iteration being a best-of-3 call, with RAG enabled.B.2 ADDITIONAL QUALITATIVE EXAMPLES\n\nIn this section, we provide qualitative examples demonstrating the improvements ImProver achieves\nin proof optimization.33.78\n31.23\n31.85\n31.35\n32.66\n32.88\n34.88\n29.54\n29.64\n28.25\n\n17\n\n\fCompﬁles: Length Optimization See (§4.3)\n\nCompﬁles: Readability Optimization Consider Figure 5, in which a lemma from the 2019 IMO\nproblem 1 (from the Compﬁles dataset) is optimized for readability.This introduces multiple new\nhypotheses, which generalize a linear_property of the functions, and then reuses and instanti-\nates that (and others, too) hypothesis throughout the proof, creating a signiﬁcantly more declaritive,\nmodular, and therefore readable proof.Original (human-written)\n\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\nrefine hf 1, fun a => ?_i\nchange g a = g 1 * a\nrw [mul_comm, ← smul_eq_mul, ←\n\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nImProver (readability-optimized)\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\n\nhave linear_property : ∀ a, f a = g a := by\n\nintro a\nrfl\n\nhave g_smul : ∀ a, g a = g 1 * a := by\n\nintro a\nrw [mul_comm, ← smul_eq_mul, ←\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nrefine hf 1, fun a => ?_i\nhave f_eq_g : f a = g a := linear_property a\nhave g_a_eq : g a = g 1 * a := g_smul a\nrw [f_eq_g, linear_property 1, g_a_eq]\n\nFigure 5: Optimizing a lemma from IMO 2019 P1 for readability\n\nMIL: Length Optimization Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, eliminating simp calls and introducing proof terms\ninto the structure of the proof to shorten it from 9 tactic invocations to 7.Original (human-written)\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\n\n:= by\n\next x\nsimp only [mem_map]\nconstructor\n· rintro hy, y_in, hyi\n\nexact hϕ y, hy, y_in, rfli, hyi\n\n· rintro hy, hz, z_in, hzi, hyi\n\nuse z, z_in\ncalc ψ.comp ϕ z = ψ (ϕ z) := rfl\n_\n= ψ y := by congr\n\nImProver (length-optimized)\n\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\nby\next x\nsimp only [mem_map]\nconstructor\nrintro hy, y_in, hyi; exact hϕ y, hy, y_in,\n\n:=\n\nrfli, hyi\n\nrintro hy, hz, z_in, hzi, hyi; exact hz,\nz_in, (congr_arg ψ hz).trans hyi\n\nFigure 6: Optimizing a lemma from the solutions of MIL CH08 S01 for length\n\nMIL: Length Optimization 2 Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, converting a full tactic proof into a single proof term\nto shorten it from 28 tactic invocations to 1.Note that the model does not have access to the Lean\ncommands that symbolically generate proof terms, and therefore generates and estimates the proof\nterm entirely by itself.MIL: Readability Optimization See (§4.3)\n\nMathlib: Length Optimization Consider Figure 8, which optimizes a theorem in algebraic topol-\nogy from mathlib for length, eliminating simp calls and combining tactics to shorten it from 3 tactic\ninvocations to 1.Mathlib: Readability Optimization Consider Figure 9, a theorem from Mathlib that we optimize\nfor readability.This original proof carried a score of 0, as it does not contain any declarative statements.It is concise\nand efﬁcient, however, it is difﬁcult to understand and read.18\n\n\fOriginal (human-written)\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\next x; constructor\n· rintro (hxs, xnti | hxt, xnsi)\n\nImProver (length-optimized)\n\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\nexact Set.ext fun x => hfun h => h.elim\n\n(fun hxs, xnti => hOr.inl xs, fun h_, xti =\n> xnt xti) (fun hxt, xnsi => hOr.inr xt,\nfun hxs, _i => xns xsi),\n\nfun hh, nxsti => h.elim (fun xs => Or.inl h\nxs, fun xt => nxst hxs, xtii) (fun xt =>\nOr.inr hxt, fun xs => nxst hxs, xtii)i\n\n· constructor\n\nleft\nexact xs\nrintro h_, xti\ncontradiction\n\n.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.This intermediate tactic greatly helps in the understandability and clarity of the proof.Original (human-written)\n\nImProver (readability-optimized)\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n:=\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]; rfl\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\n:=\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nhave h1 : ∀ x y : Y, Subsingleton\n\n(Path.Homotopic.Quotient x y) ↔ ∀ {p1 p2\n: Path x y}, Path.Homotopic p1 p2 := by\nintro x y\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]\nrfl\n\nsimp only [h1]\n\nFigure 9: Optimizing a theorem from Mathlib/FundamentalGroupoid/SimplyConnected\nfor readability\n\n19",
          "time": 903.8778467178345,
          "input_token_size": 16876,
          "output_token_size": 70,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_4c840760",
        "created_at": 1749627241.0961614,
        "processing_time": 909.4665141105652,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.830851"
      }
    }
  },
  {
    "4e97676f": {
      "question": "What are the key components of the ImProver's methodology?",
      "answer": "1. Automated proof optimization\n2. Length metric\n3. Readability metric\n4. Completion metric\n5. Chain-of-States prompting\n6. Output formatting\n7. Sampling method\n8. Error correction and refinement\n9. Combination sampling and compound prompt functions\n10. Retrieval-augmented generation (RAG)",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            84,
            87,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            139,
            146,
            147,
            151,
            152,
            153,
            154,
            155,
            157,
            158,
            159,
            160,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            180,
            181,
            182,
            183,
            185,
            189,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            212,
            213,
            214,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            267,
            268,
            269,
            270,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            298,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            308,
            309,
            310,
            311,
            312,
            314,
            315,
            316,
            317,
            321,
            322,
            324,
            325,
            326,
            327,
            329,
            330,
            332,
            333,
            334,
            336,
            337,
            338,
            339,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            360,
            361,
            362,
            363,
            364,
            366,
            367,
            368,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance": "4\n2\n0\n2\n \nt\nc\nO\n \n7\n \n \n]\nI\n\nA\n. s\nc\n[\n \n \n1\nv\n3\n5\n7\n4\n0\n.0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.To this end, we study a new problem of automated proof\noptimization: rewriting a proof so that it is correct and optimizes for an arbitrary\ncriterion, such as length or readability.As a ﬁrst method for automated proof opti-\nmization, we present ImProver, a large-language-model agent that rewrites proofs\nto optimize arbitrary user-deﬁned metrics in Lean.We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.In\npractice, however, informal, natural-language proofs are prone to imprecision, ambiguity, and error.Using a formal language such as Lean (Moura & Ullrich, 2021) removes ambiguity and precision\nand enables a proof assistant to verify correctness down to the primitives of a formal axiomatic\nsystem.Formal proofs, however, can be hard to read and often suffer from low reusability or excessive detail.For example, formal proofs in Lean’s extensive mathematical library, Mathlib (mathlib Community,\n2020), are generally designed to be concise and very general, often at the expense of readability.Formal proofs in an expository text, in contrast, may include detailed calculations steps, making\nthem readable but verbose.Machine learning systems trained on such proofs learn to mimic these\nvaried conventions (Hu et al., 2024), which may not be the optimal use of the limited supply of\nhuman-written proofs.As a result, we would like to be able to automatically refactor proofs to meet\na secondary objective such as length or readability.To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is\ncorrect and optimizes a criterion such as length or readability.We ﬁnd that naively applying LLMs\nto proof optimization falls short, often resulting in incorrect or poorly optimized proofs.We develop\nvarious improvements that can be applied on top of a black-box language model, including Chain-\nof-States prompting–an analogy to chain-of-thought prompting (Wei et al., 2022) that shows inter-\nmediate proof states–along with error-correction and retrieval.We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.1\n\n\fWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more readable, and more declarative in style.1\n\nOriginal (human-written)\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\nImProver (length-optimized)\n\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nrw [h1u _ (Classical.choose_spec _)]\nobtain hw, h1e', h1u'i := h1 y\nrw [h1u' _ ((h2 _ _).mpr h1e)]\nexact h1u' _ (Classical.choose_spec _)\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nhave h2' : Classical.choose (h1 x).exists =\n\ny :=\nh1u _ (Classical.choose_spec (h1\nx).exists)\n\nrw [h2']\nobtain hw, h1e', h1u'i := h1 y\nhave h4 := Classical.choose_spec (h1\n\nhave hxw : x = w := by\n\ny).exists\n\napply h1u'\nrw [h2]\nexact h1e\n\nrw [hxw]\nexact h1u' _ h4\n\nFigure 1: ImProver automatically rewrites formal proofs to optimize a criterion such as length\nor readability while remaining correct.In this example, ImProver optimizes a human-written\nlemma (right) from the 2022 International Math Olympiad (Question 2, solution from Comp-\nﬁles (David Renshaw, 2024)) for length.ImProver’s optimized proof is correct and more concise.2 RELATED WORK\n\nRecently there has been wide interest in automating theorem proving in interactive proof assistants;\nsee (Lu et al., 2023; Li et al., 2024) for surveys.A typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs\nsuch as Lean’s Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024).A model learns from the distribution of proofs in the cor-\npus, such as Mathlib-style proofs.Recently, the AlphaProof (AlphaProof & Teams, 2024) system\nwas shown to produce proofs with an arcane, non-human structure and syntax.We consider the\nnew problem of rewriting a proof to optimize a metric, such as rewriting a proof into a more read-\nable or more concise one.Proof optimization is more general than theorem proving, since we can\nalso rewrite an empty proof to optimize correctness.Finally, there is a rich literature on the var-\nied styles of (human) formal proofs (e.g., (Autexier & Dietrich, 2010; Wiedijk, 2004)).Our model,\nImProver, builds on neural theorem proving techniques including full proof generation (Jiang et al.,\n2023; First et al., 2023), conditioning on example proofs (Jiang et al., 2023), retrieval (Yang et al.,\n2023; Thakur et al., 2024), and preceding ﬁle context (First et al., 2023; Hu et al., 2024), as well as\nerror correction (Madaan et al., 2023; Chen et al., 2023) and documentation retrieval (Zhou et al.,\n2023) from code generation.ImProver brings these code generation techniques, along with new\nChain-of-States prompting and meta-programmed contextual information, into a uniﬁed proof opti-\nmization agent.3 AUTOMATED PROOF OPTIMIZATION WITH ImProver\n\nAutomated Proof Optimization.Given a theorem statement x, additional context c, and an initial\nproof y0, proof optimization consists of generating a new proof y that is correct and minimizes (or\nmaximizes) a metric µ(x, c, y0, y) → R.\n\n1Code is available at https://github.com/riyazahuja/ImProver.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.Note that shorter proofs often\nrepresent more efﬁcient proofs.Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.Concretely, we evaluate this using the ratio of number of explicitly typed\nhave tactics to total number of tactic invocations.Completion Metric: The completion of a proof simply describes its correctness.This is a trivial\nmetric which measures the number of errors present.The completion metric is used for concretely\nviewing proof optimization as a generalization of neural theorem proving.3.1\n\nIMPROVER\n\nWe develop several improvements that can be applied to a black-box LLM generator yout ∼\nG(·|xin), such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these pa-\nrameters.The explicit prompts and templates that are sent to the LLM can be found in (§A).3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.The intermediate states often contain valuable information (e.g., an expression\nafter it has been simpliﬁed) that is not present in the tactics.To allow the model to reason about\nthese intermediate goals and hypotheses, we use tools from Lean metaprogramming to automatically\nannotate each proof state as a comment prior to each tactic.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.The implementation of this extraction system is mod-\neled from the work (Kim Morrison, 2024).Speciﬁcally, in the compiler’s elaboration and eval-\nuation stages – where the parsed theorem code is ﬁrst converted into concrete syntax trees (in\npractice, Syntax objects) and abstract syntax trees (Expr objects) – we convert the CST and\nAST output objects into the relevant proof data and proof states in the form of proof trees\n(Lean.Elab.InfoTree).These proof trees contain detailed context and information on a tactic-\nby-tactic level relating to the modiﬁcation of the proof state, metavariable context, and proof cor-\nrectness.After state extraction is completed and cached for efﬁcient future access, we annotate the proof text\nitself to contain the intermediate states in the form as comments.Figure 2 shows an example. This explicit reasoning aims to help the generator model construct more optimized proofs via addi-\ntional symbolic data.3.1.2 OUTPUT FORMATTING. LLM outputs often contain ancillary and syntactically invalid content, especially before and after\nthe actual proof.Additionally, by applying additional structure to the LLM outputs, we may hope to\ngenerate more structured proofs.To analyze this hypothesis, we introduce two additional output for-\nmats to the standard str output: flat and structured.The former enforces a tactic sequence\noutput as a list of strings, and the latter enforces a proof tree output as a tree of strings.3.1.3 SAMPLING METHOD\n\nWe also introduce different methods of sampling between many (sequential or parallel) LLM infer-\nence calls, involving best-of-n and iterative reﬁnement implementations, as well as combinations\nthereof.3\n\n\fWithout Chain-of-States\n\nWith Chain-of-States\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u) := by\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u)\n\n:= by\n\nrintro x (hxs, xti | hxs, xui)\n· use xs; left; exact xt\n.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!-/\n. use xs; right; exact xu\n/-\nGoals Solved!-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.More speciﬁcally, our scoring function is given by the 2-ary\ncomparison function S, whose arguments are output objects y, y′.max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.This comparison function can be extended to evaluate the best output of any ﬁnite n via induction.This best-of-n technique is implemented as a curried function such that each of the n calls can be\nhandled by any arbitrary sampling method, or just a single standard prompt at user discretion.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.The reﬁnement process relies on user-deﬁned pa-\nrameters n and prev_num to specify the number of iterations and the number of previous iteration\ninfo to forward, respectively.Each iteration carries information on the last prev_num iterations,\nincluding input, output, metric score, correctness, and error messages.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.Similar to the best-of-n tech-\nnique, it relies on an argument n for the number of reﬁnement steps, and is curried such that each\nreﬁnement step can be handled by any other prompting function.However, unlike best-of-n, there is\nno opportunity for parallelism as each iteration is dependent on information from the previous call.Combination Sampling and Compound Prompt Functions Compound prompt functions utilize\nthe curried nature of the implementations of best-of-n and reﬁnement to nest these techniques within\none another.For example:\n\n4\n\n\fbest_of_n((refinement,m),n) is a compound sampling method that run a best-of-n,\nwhere each call is a m-step reﬁnement.refinement((best_of_n,m),n) is a compound sampling method that runs a n-step reﬁne-\nment, where each call is a best-of-m call to the LLM.Note that with each of these compound prompt functions, there are always a total of mn iterations.Namely, each metric is loaded with a cached (vector) database\npopulated with human-made examples of preoptimized and postoptimized pairs of Lean theorems.The\ndatabases store semantically chunked data from the Theorem Proving in Lean (TPiL) handbook –\ncontaining syntax guides and tactic explanations – and the Mathlib mathematics libary – containing\nthousands of theorems and lemmas.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.We use exercise solutions from set theory, elementary number theory, group\ntheory, topology, differential calculus, and integration & measure theory.This dataset contains the-\norems at an undergraduate-level of complexity.For our main results, we evaluated on 72 theorems\nfrom exercise solutions from MIL chapters 4, 5, 8, 9, and 10.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.This is a\ndataset of internationally-renowned competitive math problems, many of which are readable, yet\nquite verbose.This dataset contains theorems of a competitive format, and although they contain\n\n5\n\n\fconcepts only at a high-school level, the logical complexity of internationally-renowned competi-\ntion results is far above that.For our main results, we used all 26 theorems and lemmas from the\nCompﬁles database of complete solutions to the International Mathematics Olympiad (IMO) and\nthe American Mathematics Olympiad (USAMO) from 2016-2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.These proofs are extremely\nefﬁcient, concise, and generalized - which often comes at the cost of readability and understandabil-\nity.We test n =\n3, 5, 7, 10, 15 on GPT-4o and GPT-4o-mini, as well as n = 20 for GPT-4o-mini (as it is of a far\nlower token cost).Namely,\n\nGPT-4o-mini, GPT-4o, Output and Cos, Example Retrieval, and Sampling Method are tested\non the 133 theorems in the solutions of C03_Logic, C04_Sets_and_Functions, and\nC05_Elementary_Number_Theory.n and Model are tested on 55 theorems from a representative sample of the aforementioned, and\nCombos and RAG are tested on a representative sample of 32 theorems from the aforementioned.Additionally, from Table 2, Table 3, and Table 4, we can see that ImProver outperforms\nGPT-4o on each dataset as well.We proceed to analyze this data and its implications. Length optimization.First focusing on the length metric, we see that ImProver outperforms GPT-\n4o with respect to the improvement score by 566% (aggregated over all datasets).Additionally, we\nare guaranteed that ImProver produces a correct output, although that output may just be the same\nas the input.However, 35.44% of the time, it generates a correct output that is not the same length\nas the input, and in that case, we expect an average of a 55.29% reduction in length.7\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 3: Compﬁles Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n2.75\n18.86\n\n0.39\n5.74\n\n0.0\n6.19\n\n0.0\n4.63\n\n30.7\n54.48\n\n3.38\n24.89\n\n0.0\n53.65\n\n0.0\n33.19\n\n11.54%\n100.0%\n\n14.1%\n100.0%\n\n5.13%\n34.62%\n\n1.28%\n19.23%\n\n16.67%\n100.0%\n\n4.65%\n100.0%\n\n0.0%\n11.54%\n\n0.0%\n11.63%\n\nMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 4: Mathlib Proof optimization results. Readability optimization.Moreover, the accuracy, improved accuracy, and nonempty improvement dis-\nparities for readability parallel those of the length tests.However, it should be noted that for both\nGPT-4o and ImProver, the accuracy and improved accuracy scores were markedly smaller for read-\nability than length optimization.This suggests that for both models, it was generally more “difﬁcult”\nto generate a correct output, and moreover, generate a correct output with a better metric score\nthan the input, for readability optimization than length optimization.In other words, optimizing for\nreadability is more difﬁcult for the underlying generator than optimizing for length.However, we\nspeculate with higher-quality prompts, descriptions of the metric, and examples, this disparity can\nbe minimized.Regardless, we note that different metrics can be less likely to be correctly optimized,\nand that model performance is correlated with the metric it seeks to optimize – both for GPT-4o and\nImProver.Optimization varies based on dataset difﬁculty.Additionally noting Table 2, Table 3, and\nTable 4, we observe that the improvement score for both metrics for both GPT-4o and ImProver\nis highest for the MIL dataset, lower for Compﬁles, and the lowest on the Mathlib theorems.Thus, ﬁxing this model, we vary the output formatting type and if CoS is\nenabled, and determine that outputting flat with CoS enabled maximizes the improvement score.8\n\n\fTable 5: Ablation results.Each cell in the ablation tests shows best / worst, which are the best\nand worst parameter combinations in the test group.The ImProver speciﬁcation outputs the input\ntheorem when no correct proof is generated, which results in an accuracy of 100% on MIL.GPT-4o-mini\nGPT-4o\n+ Output and CoS\n+ Example Retrieval\n+ Sampling Method\n+ n and Model\n+ Combos and RAG\nImProver\n\n0\n7.03\n8.04 / 6.31\n9.34 / 5.67\n15.35 / 9.34\n23.51 / 3.65\n34.88 / 28.25\n\n0\n19.67\n12.38 / 14.17\n14.7 / 8.44\n18.44 / 14.7\n26.28 / 4.63\n57.56 / 33.48\n\n3.62%\n35.77%\n\n0%\n15.33%\n\n64.96% / 44.53% 21.17% / 16.06%\n63.5% / 67.15%\n21.9% / 16.79%\n36.5% / 21.9%\n83.21% / 63.5%\n89.47% / 78.95% 45.61% / 8.77%\n60.61% / 84.38% 54.55% / 53.12%\n\n34.88\n\n57.56\n\n100%\n\n54.55%\n\nTable 6: CoS Readability Ablation results.Now, as GPT-4o-mini is signiﬁcantly less computationally expensive than\nits GPT-4o counterpart, we test both models with the sample method ﬁxed to best-of-n, and vary\nn = 1, 3, 5, 7, 10, 15, and for GPT-4o-mini, also n = 20.Changing any one of these parameters them leads to a reduction in performance.Additional ablation data can be found at (§B.1).Readability and Chain-of-States (CoS) Ablation.We additionally examine the effects of dis-\nabling CoS on readability optimization tasks, as the previous study focused on length optimization\ntasks, and we speculate that CoS has a high impact on the performance of readability optimization\ntasks, as the proof states that are embedded due to CoS seem to be a critical aspect to generating the\nexplicit declarations that the readability metric measures.We conﬁrm this result by considering Table 6 and observe that simply enabling CoS nearly doubles\nthe improvement score, and signiﬁcantly improves the nonempty improvement score, suggesting that\nCoS has a high degree of impact on optimizing for the readability metric, as conjectured.However,\nwe also note a signiﬁcant increase in improved accuracy, which suggests that embedding the chain\nof states also improves the ability of the model to generate nontrivial correct outputs, implying that\nthe symbolic information contained in the states are critical to effectively modifying the structure\nand content of a proof.4.2.2 NEURAL THEOREM PROVING EVALUATION\n\nWe evaluate ImProver’s neural theorem proving (NTP) performance using the completion metric.We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input\nproof.Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-4o.ImProver substantially outperforms GPT-4o across all topics, with an 80% increase in accuracy\ncompared to the base model, showing that proof optimization systems are easily extendable to NTP\nsystems.9\n\n\fTable 7: Proof generation results.Each cell shows percent accuracy.MIL\n\nSet Theory Group Theory\n\nOverall\n\nGPT-4o\nImProver\n\n18.18%\n45.45%\n\n25%\n33.33%\n\n21.73%\n39.13%\n\n4.3 QUALITATIVE RESULTS\n\nNext, we discuss qualitative examples showing the improvements from ImProver in proof optimiza-\ntion.These examples show the balance between correctness and the desired optimization metric,\nshowing how ImProver can produce more concise or readable proofs depending on the use case.Additional examples can be found in (§B.2)\n\nExample 1: Compﬁles Optimization.Consider Figure 1, a lemma from the 2022 IMO Question\n2 (Compﬁles) that we optimize for length.The original proof consisted of 12 tactic steps and multiple intermediate calculations.After applying\nImProver for length optimization, the proof was halved to only 6 tactics, eliminating redundant steps\nwhile maintaining correctness.Example 2: MIL. Consider Figure 3, a result from MIL that we optimize for readability.Original (human-written)\n\nImProver (readability-optimized)\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ncard G = card H * card K)\n\n: K ≃* G / H := by\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\n· rw [← ker_eq_bot_iff, (QuotientGroup.mk'\n\nH).ker_restrict K]\nsimp [h]\n\n· symm\n\nexact aux_card_eq h'\n\ncard G = card H * card K)\n:= by\n\n: K ≃* G / H\n\nhave injectivity : Function.Injective\n\n((QuotientGroup.mk' H).restrict K) := by\nrw [← ker_eq_bot_iff, (QuotientGroup.mk'\nH).ker_restrict K]\nsimp [h]\n\nhave card_eq : card (G / H) = card K := by\n\nexact aux_card_eq h'\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\nexact injectivity\nsymm\nexact card_eq\n\nFigure 3: Optimizing a group-theoretic result from MIL Chapter 8 Section 1 for readability.We\ndeﬁne a proof to be readable if it is written in a declarative style, which consists of intermediate\nconjectures (have · · · statements).This original proof carried a score of 0, as it does not contain any declarative statements.In compar-\nison, after applying ImProver, we transformed the proof to be more declarative, with many more\nintermediate steps with explicit have tactics for improved clarity.Additionally observe how the\nmodel deﬁnes hypotheses for use in the latter half of the proof; these predeﬁned hypotheses could\neasily be converted into standalone lemmas for reuse.Example 3: Full Proof Generation.We analyze the application of ImProver to neural theorem\nproving in the MIL example from Figure 4.This theorem relating to group theory originally has no proof, however, ImProver generates one\nfrom scratch.This generated proof is veriﬁed to be correct by Lean, utilizing all the included hy-\npotheses as well as a retrieved mathlib theorem.10\n\n\fOriginal (human-written)\n\nImProver (completeness-optimized)\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n: S ≤ T) : comap ϕ S ≤ comap ϕ T := by\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n\n: S ≤ T) : comap ϕ S ≤ comap ϕ T\n\n:= by\n\nsorry\n\nintro g\nsimp only [mem_comap]\nintro hS\nexact hST hS\n\nFigure 4: Solving a group theorem exercise from MIL Chapter 8 Section 1 for readability.5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.By incorporating CoS, RAG, and other features, ImProver signiﬁcantly outperforms base\nlanguage models in proof optimization over undergraduate, competition, and research-level prob-\nlems.However, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance\non black-box LLM’s.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.As such, we believe that ImProver sets the stage for further\nwork on proof optimization to advance the study and use of AI in mathematics.ACKNOWLEDGEMENTS\n\nRiyaz Ahuja thanks the L3 Lab and Hoskinson Center for Formal Mathematics for their support.Sean Welleck thanks Convergent Research, the Lean FRO, and the OpenAI Researcher Access Pro-\ngram for their support.11\n\n\fREFERENCES\n\nand\n\nAlphaGeometry\n\nAlphaProof\nstandard\nhttps://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\n2024.\n\nsilver-medal\nproblems.achieves\nolympiad\n\nmathematical\n\ninternational\n\nsolving\n\nTeams.AI\n\nSerge Autexier and Dominik Dietrich.A tactic language for declarative proofs.In Matt Kaufmann\nand Lawrence C. Paulson (eds.), Interactive Theorem Proving, pp.99–114, Berlin, Heidelberg,\n2010. Springer Berlin Heidelberg.Jaime Carbonell and Jade Goldstein.The use of mmr, diversity-based reranking for reordering doc-\numents and producing summaries.In Proceedings of the 21st Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ’98, pp.335–336,\nISBN 1581130155. doi:\nNew York, NY, USA, 1998.Association for Computing Machinery. 10.1145/290941.291025.URL https://doi.org/10.1145/290941.291025.Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.Teaching large language models\n\nto self-debug, 2023.URL https://arxiv.org/abs/2304.05128. David Renshaw. compﬁles.https://github.com/dwrensha/compfiles, 2024.Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun.Baldur: Whole-proof generation and\n\nrepair with large language models, 2023.Representations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.Jiewen Hu, Thomas Zhu, and Sean Welleck.minictx: Neural theorem proving with (long-)contexts,\n\n2024.URL https://arxiv.org/abs/2408.03350.Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu.Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs.In The Eleventh International Conference on Learning\nRepresentations, 2023.URL https://openreview.net/forum?id=SMa9EAovKMC.Kim Morrison. lean-training-data.https://github.com/kim-em/lean-training-data,\n\nGuillaume Lample, Timothee Lacroix, Marie anne Lachaux, Aurelien Rodriguez, Amaury\nHypertree proof search\nHayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.for neural\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=J4pX8Q8cxHH. theorem proving. 2024. 2024.A survey on deep learning for theorem proving, 2024.Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang.A survey of deep\nlearning for mathematical reasoning.In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki (eds.14605–14631, Toronto, Canada, July 2023.Association for Computational Linguistics.10.18653/v1/2023.acl-long.817.URL\nhttps://aclanthology.org/2023.acl-long.817.doi:\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-reﬁne:\nIterative reﬁnement with self-feedback.In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.URL https://openreview.net/forum?id=S37hOerQLB.12\n\nleanprover-community. mathematics_in_lean.https://github.com/leanprover-community/mathematics_in_lea\n\n\fThe mathlib Community.The lean mathematical library.In Proceedings of the 9th ACM SIGPLAN\nInternational Conference on Certiﬁed Programs and Proofs, POPL ’20.URL http://dx.doi.org/10.1145/3372885.3373824.Leonardo de Moura and Sebastian Ullrich.The lean 4 theorem prover and programming lan-\nIn Automated Deduction – CADE 28: 28th International Conference on Automated\nguage.Deduction, Virtual Event, July 12–15, 2021, Proceedings, pp.625–635, Berlin, Heidelberg,\n2021. Springer-Verlag.ISBN 978-3-030-79875-8. doi: 10.1007/978-3-030-79876-5_37.URL\nhttps://doi.org/10.1007/978-3-030-79876-5_37.OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red\nAvila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Moham-\nmad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher\nBerner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brit-\ntany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek\nChen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,\nHyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas\nDegry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,\nAdrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada\nFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel,\nTarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff\nHarris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jo-\nmoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,\nNitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik\nKim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, An-\ndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,\nStephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim\nMalfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew\nMayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil,\nDavid Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin,\nVinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeon-\nwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley\nPantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov,\nAndrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde\nde Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea\nPower, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick\nRyder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David\nSchnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah\nShoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama,\nIan Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie\nSummers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin\nTootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón\nUribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welin-\nder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,\nHannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah\nYoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,\nShengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.Gpt-4 technical\nreport, 2024. URL https://arxiv.org/abs/2303.08774.Stanislas Polu and Ilya Sutskever.Generative language modeling for automated theorem proving,\n\n2020.13\n\n\fStanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\n\nSutskever.Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, and Swarat Chaudhuri.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H.\nthought prompting elicits reasoning\nChi, Quoc V Le, and Denny Zhou.in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=_VjQlMeSB_J.Chain of\n\nFreek Wiedijk. Formal proof sketches.In Stefano Berardi, Mario Coppo, and Ferruccio Damiani\n(eds.), Types for Proofs and Programs, pp.Springer Berlin\nHeidelberg. ISBN 978-3-540-24849-1.Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil,\nRyan Prenger, and Anima Anandkumar.LeanDojo: Theorem proving with retrieval-augmented\nlanguage models.In Neural Information Processing Systems (NeurIPS), 2023.Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig.In The Eleventh International Conference on Learning Repre-\nsentations, 2023.URL https://openreview.net/forum?id=ZTCxT2t2Ru.14\n\n\fA PROMPTS\n\nA.1 TEMPLATE\n\nIn this appendix, we note the prompts used by ImProver both for general LLM prompting, as well\nas the metric-speciﬁc prompts.For the main prompt sent to the LLM on each sample, we build a prompt string using a chat prompt\ntemplate that is then invoked at runtime to ﬁll in the variables.You will be given the previous num_prev input/output pairs as well as their metric (met-\nric.name) score and correctness score, as well as any error messages, for your reference to\nimprove upon.Each of these previous results will be wrapped with <PREV I=0></PREV\nI=0>,...,<PREV I=num_prev-1></PREV I=num_prev-1>, with I=num_prev-1 being the\nmost recent result.To\nassist with the syntax relating to the current theorem and current error messages, you will\nbe given num_syntax_docs documents to refer to for ﬁxing these syntax issues.Each of\nthese documents will be wrapped with <SYNTAX_DOC>...</SYNTAX_DOC>.You will also receive num_mathlib_docs documents relevant to the current theorem to\nhelp with formulating your modiﬁed proof.Each of these will be wrapped with <CON-\nTENT_DOC>...</CONTENT_DOC>\nYou will also receive num_examples examples of input-output pairs of proofs that\nwere optimized for the metric metric.The current theorem will be\nwrapped in <CURRENT>...</CURRENT>\nSystem: Output format instructions\nPlaceholder: All retrieved syntax documentation\nPlaceholder: All retrieved mathlib documentation\nPlaceholder: All retrieved examples\nUser: <CONTEXT> context </CONTEXT>\nPlaceholder: Previous results and inputs/outputs\nPlaceholder: All metric prompts with a ‘User’ role\nUser: <CURRENT> theorem </CURRENT>\n\nThis prompt is then invoked and sent to the language model by ﬁlling in all the variables and place-\nholders.Notably, when we invoke the chain given by chain|llm|parser, we throttle the invoca-\ntion with a randomized exponential rate limit throttling to account for API rate limits, especially in\nhighly-parallelized requests like when benchmarking over a large number of theorems.You will aim to reduce the number of lines of the tactic proof while ensuring that it\nproperly compiles in Lean 4.15\n\n\fUser: Shorten the current theorem (wrapped in <CURRENT>...</CURRENT>) to be as\nshort in length—measured in the number of lines of the proof—as possible, while also\nensuring that the output is still syntactically correct.\"Readability Metric\n\nCompletion Metric\n\nSystem: You are an AI assistant who rewrites Lean 4 proofs to be more readable while\nensuring their correctness.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is\nmore readable and declarative and modular.System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.You will receive a Lean 4 proof you must\nmodify to eliminate any errors so that it compiles correctly and eliminate any “sorry”s with\nfull proofs.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a\nformal, complete, and correct Lean 4 proof by ﬁlling in its tactic proof.string\nTrue\nstring\nFalse\nstring list True\nstring list\nFalse\nstring tree\nTrue\nstring tree\nFalse\n\n7.53\n7.03\n8.04\n7.04\n7.62\n6.31\n\n16.12\n19.67\n12.38\n13.58\n15.34\n14.17\n\n46.72%\n35.77%\n64.96%\n51.82%\n49.64%\n44.53%\n\n16.79%\n15.33%\n21.17%\n18.98%\n18.25%\n16.06%\n\nBy Table 8, we see that the optimal combination in this testing group is a string list output\nformat with CoS enabled.Fix these values for all future tests.Examples\n\nTable 9: Example Retrieval Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.0\n3\n5\n7\n10\n\n5.67\n8.49\n8.38\n7.56\n9.34\n\n67.15%\n62.04%\n64.96%\n62.77%\n63.5%\n\n16.79%\n19.71%\n21.17%\n19.71%\n21.9%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the number of examples.By Table 9,\nwe see that the optimal combination in this testing group is 10 examples.Fix this value for all future\ntests.8.44\n13.68\n12.9\n12.04\n14.7\n\n16\n\n\fTable 10: Sampling Method Ablations\n\nMethod\n\nForward Keep Best\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.None\nreﬁnement\nreﬁnement\nreﬁnement\nreﬁnement\nbest-of-n\n\nN/A\n1\n5\n1\n5\nN/A\n\nN/A\nFalse\nFalse\nTrue\nTrue\nN/A\n\n9.34\n14.76\n12.5\n14.95\n13.15\n15.35\n\n14.7\n30.63\n20.88\n14.95\n13.15\n18.44\n\n63.5%\n48.18%\n59.85%\n100.0%\n100.0%\n83.21%\n\n21.9%\n30.66%\n30.66%\n30.66%\n29.93%\n36.5%\n\nNote that forward and keep-best values are parameters for reﬁnement of how many previous itera-\ntions to forward, and whether to keep the most recent or the best iteration in subsequent reﬁnement\nsteps.Now, with the previous optimal parameters ﬁxed, run the ablation on the sample method.By\nTable 10, we see that the optimal combination in this testing group is best-of-n.Fix this value\nfor all future tests.Table 11: Model and n Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.Model\n\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\n\nn\n\n3\n5\n7\n10\n15\n3\n5\n7\n10\n15\n20\n\n19.66\n20.12\n22.44\n21.73\n23.51\n3.65\n5.12\n3.65\n4.99\n4.35\n4.87\n\n24.36\n24.97\n27.21\n25.28\n26.28\n4.63\n6.21\n4.34\n5.69\n5.06\n5.56\n\n80.7%\n80.56%\n82.46%\n85.96%\n89.47%\n78.95%\n82.46%\n84.21%\n87.72%\n85.96%\n87.72%\n\n38.6%\n36.11%\n42.11%\n40.35%\n45.61%\n8.77%\n10.53%\n8.77%\n12.28%\n12.28%\n14.04%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the value of n and model.By\nTable 11, we see that the optimal combination in this testing group is GPT-4o with n = 15.Fix this\nvalue for all future tests.Table 12: RAG and Combination Sampling Method Ablations\n\nRAG Improvement Nonempty Improve. Accuracy\n\nImproved Acc.By Table 12, we see that the optimal combination in this testing group is a 5-step\nreﬁnement with each iteration being a best-of-3 call, with RAG enabled.B.2 ADDITIONAL QUALITATIVE EXAMPLES\n\nIn this section, we provide qualitative examples demonstrating the improvements ImProver achieves\nin proof optimization.33.78\n31.23\n31.85\n31.35\n32.66\n32.88\n34.88\n29.54\n29.64\n28.25\n\n17\n\n\fCompﬁles: Length Optimization See (§4.3)\n\nCompﬁles: Readability Optimization Consider Figure 5, in which a lemma from the 2019 IMO\nproblem 1 (from the Compﬁles dataset) is optimized for readability.This introduces multiple new\nhypotheses, which generalize a linear_property of the functions, and then reuses and instanti-\nates that (and others, too) hypothesis throughout the proof, creating a signiﬁcantly more declaritive,\nmodular, and therefore readable proof.Original (human-written)\n\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\nrefine hf 1, fun a => ?_i\nchange g a = g 1 * a\nrw [mul_comm, ← smul_eq_mul, ←\n\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nImProver (readability-optimized)\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\n\nhave linear_property : ∀ a, f a = g a := by\n\nintro a\nrfl\n\nhave g_smul : ∀ a, g a = g 1 * a := by\n\nintro a\nrw [mul_comm, ← smul_eq_mul, ←\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nrefine hf 1, fun a => ?_i\nhave f_eq_g : f a = g a := linear_property a\nhave g_a_eq : g a = g 1 * a := g_smul a\nrw [f_eq_g, linear_property 1, g_a_eq]\n\nFigure 5: Optimizing a lemma from IMO 2019 P1 for readability\n\nMIL: Length Optimization Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, eliminating simp calls and introducing proof terms\ninto the structure of the proof to shorten it from 9 tactic invocations to 7.Note that the model does not have access to the Lean\ncommands that symbolically generate proof terms, and therefore generates and estimates the proof\nterm entirely by itself.MIL: Readability Optimization See (§4.3)\n\nMathlib: Length Optimization Consider Figure 8, which optimizes a theorem in algebraic topol-\nogy from mathlib for length, eliminating simp calls and combining tactics to shorten it from 3 tactic\ninvocations to 1.Mathlib: Readability Optimization Consider Figure 9, a theorem from Mathlib that we optimize\nfor readability.It is concise\nand efﬁcient, however, it is difﬁcult to understand and read.18\n\n\fOriginal (human-written)\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\next x; constructor\n· rintro (hxs, xnti | hxt, xnsi)\n\nImProver (length-optimized)\n\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\nexact Set.ext fun x => hfun h => h.elim\n\n(fun hxs, xnti => hOr.inl xs, fun h_, xti =\n> xnt xti) (fun hxt, xnsi => hOr.inr xt,\nfun hxs, _i => xns xsi),\n\nfun hh, nxsti => h.elim (fun xs => Or.inl h\nxs, fun xt => nxst hxs, xtii) (fun xt =>\nOr.inr hxt, fun xs => nxst hxs, xtii)i\n\n· constructor\n\nleft\nexact xs\nrintro h_, xti\ncontradiction\n\n.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.This intermediate tactic greatly helps in the understandability and clarity of the proof.Original (human-written)\n\nImProver (readability-optimized)\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n:=\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]; rfl\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\n:=\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nhave h1 : ∀ x y : Y, Subsingleton\n\n(Path.Homotopic.Quotient x y) ↔ ∀ {p1 p2\n: Path x y}, Path.Homotopic p1 p2 := by\nintro x y\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]\nrfl\n\nsimp only [h1]\n\nFigure 9: Optimizing a theorem from Mathlib/FundamentalGroupoid/SimplyConnected\nfor readability\n\n19",
          "time": 744.1963822841644,
          "input_token_size": 16876,
          "output_token_size": 118,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_4e97676f",
        "created_at": 1749627255.8703718,
        "processing_time": 748.9348497390747,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.831852"
      }
    }
  },
  {
    "51bdcbc6": {
      "question": "How does the complexity of datasets affect ImProver's performance?",
      "answer": "Answer is not found.",
      "provenance": [],
      "metadata": {
        "question_id": "paper_51bdcbc6",
        "created_at": 1749769295.614784,
        "processing_time": 1.2511827945709229,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "default",
        "sentence_count": 999999,
        "provenance_count": 0,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 999999,
            "provenance_count": 0,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.831852"
      }
    }
  },
  {
    "5411ab1e": {
      "question": "What challenges are associated with using large language models for proof optimization?",
      "answer": "1. Naive application of LLMs often results in incorrect or poorly optimized proofs.\n2. Difficulty in generating correct outputs for higher difficulty datasets.\n3. High cost and slow runtime associated with using LLMs.\n4. Variability in optimization effectiveness based on the chosen metric (length vs. readability).\n5. Challenges in maintaining correctness while optimizing for different criteria.\n6. Dependence on the quality of prompts and examples for effective optimization.\n7. Complexity of formal proofs can lead to low readability and reusability.\n8. The need for error correction and iterative refinement complicates the optimization process.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance": "4\n2\n0\n2\n \nt\nc\nO\n \n7\n \n \n]\nI\n\nA\n. s\nc\n[\n \n \n1\nv\n3\n5\n7\n4\n0\n.0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.To this end, we study a new problem of automated proof\noptimization: rewriting a proof so that it is correct and optimizes for an arbitrary\ncriterion, such as length or readability.As a ﬁrst method for automated proof opti-\nmization, we present ImProver, a large-language-model agent that rewrites proofs\nto optimize arbitrary user-deﬁned metrics in Lean.We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.In\npractice, however, informal, natural-language proofs are prone to imprecision, ambiguity, and error.Using a formal language such as Lean (Moura & Ullrich, 2021) removes ambiguity and precision\nand enables a proof assistant to verify correctness down to the primitives of a formal axiomatic\nsystem.Formal proofs, however, can be hard to read and often suffer from low reusability or excessive detail.For example, formal proofs in Lean’s extensive mathematical library, Mathlib (mathlib Community,\n2020), are generally designed to be concise and very general, often at the expense of readability.Formal proofs in an expository text, in contrast, may include detailed calculations steps, making\nthem readable but verbose.Machine learning systems trained on such proofs learn to mimic these\nvaried conventions (Hu et al., 2024), which may not be the optimal use of the limited supply of\nhuman-written proofs.As a result, we would like to be able to automatically refactor proofs to meet\na secondary objective such as length or readability.To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is\ncorrect and optimizes a criterion such as length or readability.We ﬁnd that naively applying LLMs\nto proof optimization falls short, often resulting in incorrect or poorly optimized proofs.We develop\nvarious improvements that can be applied on top of a black-box language model, including Chain-\nof-States prompting–an analogy to chain-of-thought prompting (Wei et al., 2022) that shows inter-\nmediate proof states–along with error-correction and retrieval.We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.1\n\n\fWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more readable, and more declarative in style.1\n\nOriginal (human-written)\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\nImProver (length-optimized)\n\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nrw [h1u _ (Classical.choose_spec _)]\nobtain hw, h1e', h1u'i := h1 y\nrw [h1u' _ ((h2 _ _).mpr h1e)]\nexact h1u' _ (Classical.choose_spec _)\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nhave h2' : Classical.choose (h1 x).exists =\n\ny :=\nh1u _ (Classical.choose_spec (h1\nx).exists)\n\nrw [h2']\nobtain hw, h1e', h1u'i := h1 y\nhave h4 := Classical.choose_spec (h1\n\nhave hxw : x = w := by\n\ny).exists\n\napply h1u'\nrw [h2]\nexact h1e\n\nrw [hxw]\nexact h1u' _ h4\n\nFigure 1: ImProver automatically rewrites formal proofs to optimize a criterion such as length\nor readability while remaining correct.In this example, ImProver optimizes a human-written\nlemma (right) from the 2022 International Math Olympiad (Question 2, solution from Comp-\nﬁles (David Renshaw, 2024)) for length.ImProver’s optimized proof is correct and more concise.2 RELATED WORK\n\nRecently there has been wide interest in automating theorem proving in interactive proof assistants;\nsee (Lu et al., 2023; Li et al., 2024) for surveys.A typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs\nsuch as Lean’s Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024).A model learns from the distribution of proofs in the cor-\npus, such as Mathlib-style proofs.Recently, the AlphaProof (AlphaProof & Teams, 2024) system\nwas shown to produce proofs with an arcane, non-human structure and syntax.We consider the\nnew problem of rewriting a proof to optimize a metric, such as rewriting a proof into a more read-\nable or more concise one.Proof optimization is more general than theorem proving, since we can\nalso rewrite an empty proof to optimize correctness.Finally, there is a rich literature on the var-\nied styles of (human) formal proofs (e.g., (Autexier & Dietrich, 2010; Wiedijk, 2004)).Our model,\nImProver, builds on neural theorem proving techniques including full proof generation (Jiang et al.,\n2023; First et al., 2023), conditioning on example proofs (Jiang et al., 2023), retrieval (Yang et al.,\n2023; Thakur et al., 2024), and preceding ﬁle context (First et al., 2023; Hu et al., 2024), as well as\nerror correction (Madaan et al., 2023; Chen et al., 2023) and documentation retrieval (Zhou et al.,\n2023) from code generation.ImProver brings these code generation techniques, along with new\nChain-of-States prompting and meta-programmed contextual information, into a uniﬁed proof opti-\nmization agent.3 AUTOMATED PROOF OPTIMIZATION WITH ImProver\n\nAutomated Proof Optimization.Given a theorem statement x, additional context c, and an initial\nproof y0, proof optimization consists of generating a new proof y that is correct and minimizes (or\nmaximizes) a metric µ(x, c, y0, y) → R.\n\n1Code is available at https://github.com/riyazahuja/ImProver.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.Note that shorter proofs often\nrepresent more efﬁcient proofs.Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.Concretely, we evaluate this using the ratio of number of explicitly typed\nhave tactics to total number of tactic invocations.Completion Metric: The completion of a proof simply describes its correctness.This is a trivial\nmetric which measures the number of errors present.The completion metric is used for concretely\nviewing proof optimization as a generalization of neural theorem proving.3.1\n\nIMPROVER\n\nWe develop several improvements that can be applied to a black-box LLM generator yout ∼\nG(·|xin), such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these pa-\nrameters.The explicit prompts and templates that are sent to the LLM can be found in (§A).3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.The intermediate states often contain valuable information (e.g., an expression\nafter it has been simpliﬁed) that is not present in the tactics.To allow the model to reason about\nthese intermediate goals and hypotheses, we use tools from Lean metaprogramming to automatically\nannotate each proof state as a comment prior to each tactic.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.The implementation of this extraction system is mod-\neled from the work (Kim Morrison, 2024).Speciﬁcally, in the compiler’s elaboration and eval-\nuation stages – where the parsed theorem code is ﬁrst converted into concrete syntax trees (in\npractice, Syntax objects) and abstract syntax trees (Expr objects) – we convert the CST and\nAST output objects into the relevant proof data and proof states in the form of proof trees\n(Lean.Elab.InfoTree).These proof trees contain detailed context and information on a tactic-\nby-tactic level relating to the modiﬁcation of the proof state, metavariable context, and proof cor-\nrectness.After state extraction is completed and cached for efﬁcient future access, we annotate the proof text\nitself to contain the intermediate states in the form as comments.Figure 2 shows an example. This explicit reasoning aims to help the generator model construct more optimized proofs via addi-\ntional symbolic data.3.1.2 OUTPUT FORMATTING. LLM outputs often contain ancillary and syntactically invalid content, especially before and after\nthe actual proof.Additionally, by applying additional structure to the LLM outputs, we may hope to\ngenerate more structured proofs.To analyze this hypothesis, we introduce two additional output for-\nmats to the standard str output: flat and structured.The former enforces a tactic sequence\noutput as a list of strings, and the latter enforces a proof tree output as a tree of strings.3.1.3 SAMPLING METHOD\n\nWe also introduce different methods of sampling between many (sequential or parallel) LLM infer-\nence calls, involving best-of-n and iterative reﬁnement implementations, as well as combinations\nthereof.3\n\n\fWithout Chain-of-States\n\nWith Chain-of-States\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u) := by\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u)\n\n:= by\n\nrintro x (hxs, xti | hxs, xui)\n· use xs; left; exact xt\n.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!-/\n. use xs; right; exact xu\n/-\nGoals Solved!-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.More speciﬁcally, our scoring function is given by the 2-ary\ncomparison function S, whose arguments are output objects y, y′.max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.This comparison function can be extended to evaluate the best output of any ﬁnite n via induction.This best-of-n technique is implemented as a curried function such that each of the n calls can be\nhandled by any arbitrary sampling method, or just a single standard prompt at user discretion.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.The reﬁnement process relies on user-deﬁned pa-\nrameters n and prev_num to specify the number of iterations and the number of previous iteration\ninfo to forward, respectively.Each iteration carries information on the last prev_num iterations,\nincluding input, output, metric score, correctness, and error messages.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.Similar to the best-of-n tech-\nnique, it relies on an argument n for the number of reﬁnement steps, and is curried such that each\nreﬁnement step can be handled by any other prompting function.However, unlike best-of-n, there is\nno opportunity for parallelism as each iteration is dependent on information from the previous call.Combination Sampling and Compound Prompt Functions Compound prompt functions utilize\nthe curried nature of the implementations of best-of-n and reﬁnement to nest these techniques within\none another.For example:\n\n4\n\n\fbest_of_n((refinement,m),n) is a compound sampling method that run a best-of-n,\nwhere each call is a m-step reﬁnement.refinement((best_of_n,m),n) is a compound sampling method that runs a n-step reﬁne-\nment, where each call is a best-of-m call to the LLM.Note that with each of these compound prompt functions, there are always a total of mn iterations.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.More speciﬁcally, example retrieval selects the most relevant user-generated examples of proof op-\ntimization on a speciﬁc metric.Namely, each metric is loaded with a cached (vector) database\npopulated with human-made examples of preoptimized and postoptimized pairs of Lean theorems.The number of examples that are retrieved is user-speciﬁed.Document retrieval extracts information using MMR from a pair of ﬁxed (vector) databases.The\ndatabases store semantically chunked data from the Theorem Proving in Lean (TPiL) handbook –\ncontaining syntax guides and tactic explanations – and the Mathlib mathematics libary – containing\nthousands of theorems and lemmas.The chunking is handled by a recursive character splitter, which\nsplits the TPiL markdown ﬁles at on its headers and Mathlib ﬁles at the start of theorems, examples,\nlemmas, and deﬁnitions – with chunk sizes of 1000 characters with a 200 character overlap.The Mathlib retriever ﬁnds the top k documents that score the highest MMR score against the current\ntheorem, the TPiL retriever ﬁnds the top k documents that score the highest MMR score against the\ncurrent theorem in context and all current error messages.This retrieval process helps in generating\nmore contextually accurate prompts that allow the language model to better correct its own errors as\nwell as ﬁnd useful lemmas to reference.4 EXPERIMENTS\n\nWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics and compare its results to those of the base GPT-4o and GPT-4o-mini\nmodels.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.4.1 SETUP\n\nOur experimentation is split into three distinct stages.We ﬁrst perform ablation testing on the\nImProver model parameters (§3.1) to ensure that ImProver’s parameter speciﬁcation is the optimal\none with respect to correctness and metric optimization score.We then evaluate this optimal param-\neter combination on datasets of varying complexity and analyze the performance and results thereof.Lastly, we note the performance of ImProver in NTP applications in comparison to the base GPT-4o\nand GPT-4o-mini models.Datasets. We evaluate ImProver on subsets of the following datasets.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.We use exercise solutions from set theory, elementary number theory, group\ntheory, topology, differential calculus, and integration & measure theory.This dataset contains the-\norems at an undergraduate-level of complexity.For our main results, we evaluated on 72 theorems\nfrom exercise solutions from MIL chapters 4, 5, 8, 9, and 10.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.This is a\ndataset of internationally-renowned competitive math problems, many of which are readable, yet\nquite verbose.This dataset contains theorems of a competitive format, and although they contain\n\n5\n\n\fconcepts only at a high-school level, the logical complexity of internationally-renowned competi-\ntion results is far above that.For our main results, we used all 26 theorems and lemmas from the\nCompﬁles database of complete solutions to the International Mathematics Olympiad (IMO) and\nthe American Mathematics Olympiad (USAMO) from 2016-2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.These proofs are extremely\nefﬁcient, concise, and generalized - which often comes at the cost of readability and understandabil-\nity.These results and theorems often are at the cutting edge of research.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.This is the most difﬁcult dataset. Models.Our base generator uses GPT-4o (OpenAI et al., 2024).Since no prior methods currently\nexist for automated proof optimization, we consider a prompted GPT-4o without the improvements\ndescribed in (§3.1) as our baseline.Additionally, for a given metric, we write a prompt that brieﬂy\ndescribes the metric and the proof optimization task.We also provide instructions, context, and in-\nformation depending on the features selected, and add the theorem and proof to the prompt.Speciﬁc\nprompt information is detailed in (§A)\n\nPerformance metrics.Since proof optimization is a new task, we deﬁne four performance metrics\nfor measuring aspects of correctness and improvement.First, we deﬁne improvement for length as percentage change in length, µlen(y0)−µlen(y)\n× 100.For\nreadability, we use the difference, µread(y)−µread(yo).If no correct output is generated by the model\nfor a speciﬁc theorem, improvement is deﬁned to be zero.We deﬁne nonempty improvement as\nthe improvement restricted to theorems for which some output has nonzero improvement.µlen(y0)\n\nIntuitively, improvement is the expected improvement in metric score from the input to output, ac-\ncounting for errors in the generation.The nonempty improvement score is the expected improvement\nin metric score, given that there are no errors in the generation.Similar improvement scores can be\ndeﬁned for other metrics using a binary function of the metric assigned to the original and optimized\nproofs.Additionally, the accuracy is the percentage of theorems in the dataset which the model was able to\ngenerate a correct output for.The improved accuracy is the percentage of theorems in the dataset\nwhich the model was able to generate a correct output for, as well as improve the metric to be\nnonzero.4.1.1 ABLATIONS\n\nWhen performing our ablation studies, we used a ﬁxed dataset (MIL) and metric (length) and varied\nthe parameters of all the features to ﬁnd the optimal combination.However, as there are over 8640\npossible combinations, it is inefﬁcient to test all combinations at once.As such, we evaluate using a\nfactorial testing method. Testing Groups.We deﬁne the following testing groups with the speciﬁed parameter combina-\ntions:\n\nGPT-4o-mini/GPT-4o: This varies the GPT-4o model, outputting a string with no other features.Output and CoS: We evaluate the effects of different output formatting styles (string, string\nlist, string tree) and CoS (True, False), with the model ﬁxed as GPT-4o, with no other\nfeatures enabled.Example Retrieval: We evaluate the effects of increasing the number of examples provided (multi-\nshot prompting) in the range of 0, 3, 5, 7, and 10, with the model ﬁxed as GPT-4o, CoS and output\nformatting ﬁxed as the best combination from the previous test, and no other features enabled.Sampling Method: Here, we evaluate the effects of best-of-n and reﬁnement for a ﬁxed n = 5.Additionally we test on the reﬁnement cases if forwarding the most recent iteration result, or all\nprevious iteration results is the best, and if we should keep the best out of the iterations, or the most\n\n6\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 1: Average Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nMetric\n\nModel\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n3.7\n20.96\n\n2.21\n9.34\n\n6.25\n30.54\n\n4.18\n13.45\n\n15.15\n55.29\n\n8.02\n30.53\n\n18.58\n56.56\n\n14.48\n30.97\n\n26.36%\n100.0%\n\n18.75%\n100.0%\n\n8.31%\n35.44%\n\n6.13 %\n24.56%\n\n37.5%\n100.0%\n\n28.85%\n100.0%\n\n14.42%\n50.0%\n\n11.54%\n34.21%\n\nTable 2: MIL Proof optimization results.Improvement Nonempty Improvement Accuracy\n\nImproved Acc. recent.The model is ﬁxed as GPT-4o, CoS, output formatting, and examples are ﬁxed as the best\ncombination from the previous test, and no other features enabled.n and Model: Here, we evaluate the effects of larger n values and different models.We test n =\n3, 5, 7, 10, 15 on GPT-4o and GPT-4o-mini, as well as n = 20 for GPT-4o-mini (as it is of a far\nlower token cost).CoS, output formatting, examples, and sampling method are ﬁxed as the best\ncombination from the previous test, and no other features enabled.Combos and RAG: We evaluate combination methods refinement(best_of_m',m) and\nbest_of_m'(refinement(m)), for m 6= m′ with mm′ equal to the optimal value m from\nthe previous test.We also test the effect of enabling document retrieval.Model, CoS, output for-\nmatting, examples, n, and sampling method are ﬁxed as the best combination from the previous\ntest.Ablation data. We evaluate our ablations on a subset of MIL.However, due to the increase in\nmodel calls for larger n values, we switch a representative sample of this subset for some test groups.Namely,\n\nGPT-4o-mini, GPT-4o, Output and Cos, Example Retrieval, and Sampling Method are tested\non the 133 theorems in the solutions of C03_Logic, C04_Sets_and_Functions, and\nC05_Elementary_Number_Theory.n and Model are tested on 55 theorems from a representative sample of the aforementioned, and\nCombos and RAG are tested on a representative sample of 32 theorems from the aforementioned.4.2 RESULTS\n\nImProver is capable of optimizing proofs in all settings.From Table 2, Table 3, and Table 4,\nwe can see that ImProver is capable of optimizing proofs on all datasets for both the length and\nreadability metrics.Furthermore, Table 1 shows that across all metrics, ImProver signiﬁcantly\noutperforms GPT-4o on proof optimization tasks on every experimental measure – aggregated from\nall datasets.Additionally, from Table 2, Table 3, and Table 4, we can see that ImProver outperforms\nGPT-4o on each dataset as well.We proceed to analyze this data and its implications. Length optimization.First focusing on the length metric, we see that ImProver outperforms GPT-\n4o with respect to the improvement score by 566% (aggregated over all datasets).Additionally, we\nare guaranteed that ImProver produces a correct output, although that output may just be the same\nas the input.However, 35.44% of the time, it generates a correct output that is not the same length\nas the input, and in that case, we expect an average of a 55.29% reduction in length.Comparing\nthis with GPT-4o, we conclude that not only can ImProver optimize at a higher level on arbitrary\ntheorems, but its ability to generate nontrivial correct outputs is far greater in comparison to GPT-4o.7\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 3: Compﬁles Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n2.75\n18.86\n\n0.39\n5.74\n\n0.0\n6.19\n\n0.0\n4.63\n\n30.7\n54.48\n\n3.38\n24.89\n\n0.0\n53.65\n\n0.0\n33.19\n\n11.54%\n100.0%\n\n14.1%\n100.0%\n\n5.13%\n34.62%\n\n1.28%\n19.23%\n\n16.67%\n100.0%\n\n4.65%\n100.0%\n\n0.0%\n11.54%\n\n0.0%\n11.63%\n\nMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 4: Mathlib Proof optimization results. Readability optimization.Readability optimization is similar, with ImProver outperforming\nGPT-4o by 423%.Moreover, the accuracy, improved accuracy, and nonempty improvement dis-\nparities for readability parallel those of the length tests.However, it should be noted that for both\nGPT-4o and ImProver, the accuracy and improved accuracy scores were markedly smaller for read-\nability than length optimization.This suggests that for both models, it was generally more “difﬁcult”\nto generate a correct output, and moreover, generate a correct output with a better metric score\nthan the input, for readability optimization than length optimization.In other words, optimizing for\nreadability is more difﬁcult for the underlying generator than optimizing for length.However, we\nspeculate with higher-quality prompts, descriptions of the metric, and examples, this disparity can\nbe minimized.Regardless, we note that different metrics can be less likely to be correctly optimized,\nand that model performance is correlated with the metric it seeks to optimize – both for GPT-4o and\nImProver.Optimization varies based on dataset difﬁculty.Additionally noting Table 2, Table 3, and\nTable 4, we observe that the improvement score for both metrics for both GPT-4o and ImProver\nis highest for the MIL dataset, lower for Compﬁles, and the lowest on the Mathlib theorems.This\nsuggests that the expected improvement in metric score decreases with higher difﬁcultly – with\nundergraduate-level theorems having a signiﬁcantly higher expected improvement than research-\nlevel theorems.However, it should be noted that for both metrics, the nonempty improvement of\nImProver stayed consistent, whereas for GPT-4o, it followed the aforementioned trend of decreas-\ning with difﬁculty.Similarly, the accuracy and improved accuracy scores for both metrics and\nmodels decreased with higher difﬁculty datasets (disregarding ImProver’s accuracy scores, as they\nare ensured to be 100%).This suggests that although the base GPT-4o generator is less likely to\ngenerate a correct output for higher difﬁculty datasets, the improvements that ImProver makes to\nthe base generator allows it to maintain its improvement in the metric score whenever a correct out-\nput is generated.As such, we can speculate that the bottleneck in the improvement score is not the\nmodel’s ability to optimize the proof for a metric, but rather its ability to generate a new correct\nproof at all.As such, we conjecture that with more capable generator models, the accuracy – and\nthus, the improvement score – in optimization tasks will continue to increase, until the improvement\nscores match the nonempty improvement.Overall, we conclude that although the performance of both ImProver and GPT-4o decreases on\nlength and readability optimization on more difﬁcult datasets, ImProver signiﬁcantly outperforms\nGPT-4o on all datasets for length and readability optimization.4.2.1 ABLATION TESTING\n\nWe perform ablation studies using a subset of the MIL dataset as discussed in §4.1.1.The results\nof this factorial study are aggregated in Table 5.We measure the baseline results from the GPT-\n4o and GPT-4o-mini models, noting that GPT-4o is the better-scoring model (with respect to the\nimprovement score).Thus, ﬁxing this model, we vary the output formatting type and if CoS is\nenabled, and determine that outputting flat with CoS enabled maximizes the improvement score.8\n\n\fTable 5: Ablation results.Each cell in the ablation tests shows best / worst, which are the best\nand worst parameter combinations in the test group.The ImProver speciﬁcation outputs the input\ntheorem when no correct proof is generated, which results in an accuracy of 100% on MIL.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o-mini\nGPT-4o\n+ Output and CoS\n+ Example Retrieval\n+ Sampling Method\n+ n and Model\n+ Combos and RAG\nImProver\n\n0\n7.03\n8.04 / 6.31\n9.34 / 5.67\n15.35 / 9.34\n23.51 / 3.65\n34.88 / 28.25\n\n0\n19.67\n12.38 / 14.17\n14.7 / 8.44\n18.44 / 14.7\n26.28 / 4.63\n57.56 / 33.48\n\n3.62%\n35.77%\n\n0%\n15.33%\n\n64.96% / 44.53% 21.17% / 16.06%\n63.5% / 67.15%\n21.9% / 16.79%\n36.5% / 21.9%\n83.21% / 63.5%\n89.47% / 78.95% 45.61% / 8.77%\n60.61% / 84.38% 54.55% / 53.12%\n\n34.88\n\n57.56\n\n100%\n\n54.55%\n\nTable 6: CoS Readability Ablation results.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o\nImProver, CoS Disabled\nImProver\n\n4.97\n9.23\n16.69\n\n15.89\n24.61\n31.42\n\n37.5%\n100.0%\n100.0%\n\n12.5%\n28.12%\n46.88%\n\nFixing these parameters, we now vary the number of examples retrieved, noting that prompting\nwith 10 examples maximizes the improvement score.Fixing this parameter, we vary the sampling\nmethods (excluding compound methods and ﬁxing n = 5) and observe that best-of-n is the best\nparameter combination.Now, as GPT-4o-mini is signiﬁcantly less computationally expensive than\nits GPT-4o counterpart, we test both models with the sample method ﬁxed to best-of-n, and vary\nn = 1, 3, 5, 7, 10, 15, and for GPT-4o-mini, also n = 20.We conclude that GPT-4o with n = 15\nis the most effective.Fixing these parameters, we consider all mixed compound sampling methods\nwith and without document retrieval enabled, concluding that a 5-step reﬁnement with best-of-3 on\neach iteration, with RAG enabled, is the optimal combination.Thus, as we can see from Table 5, the optimal parameter combination comes from gpt-4o outputting\nas a string list with CoS, RAG, 10 examples, 5-step reﬁnement with each iteration being a\nbest-of-3 evaluation.Changing any one of these parameters them leads to a reduction in performance.Additional ablation data can be found at (§B.1).Readability and Chain-of-States (CoS) Ablation.We additionally examine the effects of dis-\nabling CoS on readability optimization tasks, as the previous study focused on length optimization\ntasks, and we speculate that CoS has a high impact on the performance of readability optimization\ntasks, as the proof states that are embedded due to CoS seem to be a critical aspect to generating the\nexplicit declarations that the readability metric measures.We conﬁrm this result by considering Table 6 and observe that simply enabling CoS nearly doubles\nthe improvement score, and signiﬁcantly improves the nonempty improvement score, suggesting that\nCoS has a high degree of impact on optimizing for the readability metric, as conjectured.However,\nwe also note a signiﬁcant increase in improved accuracy, which suggests that embedding the chain\nof states also improves the ability of the model to generate nontrivial correct outputs, implying that\nthe symbolic information contained in the states are critical to effectively modifying the structure\nand content of a proof.4.2.2 NEURAL THEOREM PROVING EVALUATION\n\nWe evaluate ImProver’s neural theorem proving (NTP) performance using the completion metric.We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input\nproof.Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-4o.ImProver substantially outperforms GPT-4o across all topics, with an 80% increase in accuracy\ncompared to the base model, showing that proof optimization systems are easily extendable to NTP\nsystems.9\n\n\fTable 7: Proof generation results.Each cell shows percent accuracy.MIL\n\nSet Theory Group Theory\n\nOverall\n\nGPT-4o\nImProver\n\n18.18%\n45.45%\n\n25%\n33.33%\n\n21.73%\n39.13%\n\n4.3 QUALITATIVE RESULTS\n\nNext, we discuss qualitative examples showing the improvements from ImProver in proof optimiza-\ntion.These examples show the balance between correctness and the desired optimization metric,\nshowing how ImProver can produce more concise or readable proofs depending on the use case.Additional examples can be found in (§B.2)\n\nExample 1: Compﬁles Optimization.Consider Figure 1, a lemma from the 2022 IMO Question\n2 (Compﬁles) that we optimize for length.The original proof consisted of 12 tactic steps and multiple intermediate calculations.After applying\nImProver for length optimization, the proof was halved to only 6 tactics, eliminating redundant steps\nwhile maintaining correctness.In comparison, the shortened proof makes multiple nontrivial optimizations, such as eliminating the\nh2’ and h4 and hxw hypotheses, as well as fully generating proof terms for speciﬁc rewrites and\nother tactics.Example 2: MIL. Consider Figure 3, a result from MIL that we optimize for readability.Original (human-written)\n\nImProver (readability-optimized)\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ncard G = card H * card K)\n\n: K ≃* G / H := by\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\n· rw [← ker_eq_bot_iff, (QuotientGroup.mk'\n\nH).ker_restrict K]\nsimp [h]\n\n· symm\n\nexact aux_card_eq h'\n\ncard G = card H * card K)\n:= by\n\n: K ≃* G / H\n\nhave injectivity : Function.Injective\n\n((QuotientGroup.mk' H).restrict K) := by\nrw [← ker_eq_bot_iff, (QuotientGroup.mk'\nH).ker_restrict K]\nsimp [h]\n\nhave card_eq : card (G / H) = card K := by\n\nexact aux_card_eq h'\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\nexact injectivity\nsymm\nexact card_eq\n\nFigure 3: Optimizing a group-theoretic result from MIL Chapter 8 Section 1 for readability.We\ndeﬁne a proof to be readable if it is written in a declarative style, which consists of intermediate\nconjectures (have · · · statements).ImProver introduces two intermediate conjectures into the proof.This original proof carried a score of 0, as it does not contain any declarative statements.In compar-\nison, after applying ImProver, we transformed the proof to be more declarative, with many more\nintermediate steps with explicit have tactics for improved clarity.Additionally observe how the\nmodel deﬁnes hypotheses for use in the latter half of the proof; these predeﬁned hypotheses could\neasily be converted into standalone lemmas for reuse.Example 3: Full Proof Generation.We analyze the application of ImProver to neural theorem\nproving in the MIL example from Figure 4.This theorem relating to group theory originally has no proof, however, ImProver generates one\nfrom scratch.This generated proof is veriﬁed to be correct by Lean, utilizing all the included hy-\npotheses as well as a retrieved mathlib theorem.10\n\n\fOriginal (human-written)\n\nImProver (completeness-optimized)\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n: S ≤ T) : comap ϕ S ≤ comap ϕ T := by\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n\n: S ≤ T) : comap ϕ S ≤ comap ϕ T\n\n:= by\n\nsorry\n\nintro g\nsimp only [mem_comap]\nintro hS\nexact hST hS\n\nFigure 4: Solving a group theorem exercise from MIL Chapter 8 Section 1 for readability.5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.By incorporating CoS, RAG, and other features, ImProver signiﬁcantly outperforms base\nlanguage models in proof optimization over undergraduate, competition, and research-level prob-\nlems.However, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance\non black-box LLM’s.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.As such, we believe that ImProver sets the stage for further\nwork on proof optimization to advance the study and use of AI in mathematics.ACKNOWLEDGEMENTS\n\nRiyaz Ahuja thanks the L3 Lab and Hoskinson Center for Formal Mathematics for their support.Sean Welleck thanks Convergent Research, the Lean FRO, and the OpenAI Researcher Access Pro-\ngram for their support.11\n\n\fREFERENCES\n\nand\n\nAlphaGeometry\n\nAlphaProof\nstandard\nhttps://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\n2024.\n\nsilver-medal\nproblems.achieves\nolympiad\n\nmathematical\n\ninternational\n\nsolving\n\nTeams.AI\n\nSerge Autexier and Dominik Dietrich.A tactic language for declarative proofs.In Matt Kaufmann\nand Lawrence C. Paulson (eds.), Interactive Theorem Proving, pp.99–114, Berlin, Heidelberg,\n2010. Springer Berlin Heidelberg.Jaime Carbonell and Jade Goldstein.The use of mmr, diversity-based reranking for reordering doc-\numents and producing summaries.In Proceedings of the 21st Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ’98, pp.335–336,\nISBN 1581130155. doi:\nNew York, NY, USA, 1998.Association for Computing Machinery. 10.1145/290941.291025.URL https://doi.org/10.1145/290941.291025.Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.Teaching large language models\n\nto self-debug, 2023.URL https://arxiv.org/abs/2304.05128. David Renshaw. compﬁles.https://github.com/dwrensha/compfiles, 2024.Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun.Baldur: Whole-proof generation and\n\nrepair with large language models, 2023.Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu.Proof artifact co-\nIn International Conference on Learning\n\ntraining for theorem proving with language models.Representations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.Jiewen Hu, Thomas Zhu, and Sean Welleck.minictx: Neural theorem proving with (long-)contexts,\n\n2024.URL https://arxiv.org/abs/2408.03350.Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu.Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs.In The Eleventh International Conference on Learning\nRepresentations, 2023.URL https://openreview.net/forum?id=SMa9EAovKMC.Kim Morrison. lean-training-data.https://github.com/kim-em/lean-training-data,\n\nGuillaume Lample, Timothee Lacroix, Marie anne Lachaux, Aurelien Rodriguez, Amaury\nHypertree proof search\nHayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.for neural\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=J4pX8Q8cxHH. theorem proving. 2024. 2024.Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie\n\nSi.A survey on deep learning for theorem proving, 2024.Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang.A survey of deep\nlearning for mathematical reasoning.In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp.14605–14631, Toronto, Canada, July 2023.Association for Computational Linguistics.10.18653/v1/2023.acl-long.817.URL\nhttps://aclanthology.org/2023.acl-long.817.doi:\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-reﬁne:\nIterative reﬁnement with self-feedback.In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.URL https://openreview.net/forum?id=S37hOerQLB.12\n\nleanprover-community. mathematics_in_lean.https://github.com/leanprover-community/mathematics_in_lea\n\n\fThe mathlib Community.The lean mathematical library.In Proceedings of the 9th ACM SIGPLAN\nInternational Conference on Certiﬁed Programs and Proofs, POPL ’20.ACM, January 2020. doi:\n10.1145/3372885.3373824.URL http://dx.doi.org/10.1145/3372885.3373824.Leonardo de Moura and Sebastian Ullrich.The lean 4 theorem prover and programming lan-\nIn Automated Deduction – CADE 28: 28th International Conference on Automated\nguage.Deduction, Virtual Event, July 12–15, 2021, Proceedings, pp.625–635, Berlin, Heidelberg,\n2021. Springer-Verlag.ISBN 978-3-030-79875-8. doi: 10.1007/978-3-030-79876-5_37.URL\nhttps://doi.org/10.1007/978-3-030-79876-5_37.OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red\nAvila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Moham-\nmad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher\nBerner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brit-\ntany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek\nChen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,\nHyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas\nDegry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,\nAdrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada\nFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel,\nTarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff\nHarris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jo-\nmoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,\nNitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik\nKim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, An-\ndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,\nStephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim\nMalfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew\nMayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil,\nDavid Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin,\nVinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeon-\nwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley\nPantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov,\nAndrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde\nde Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea\nPower, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick\nRyder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David\nSchnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah\nShoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama,\nIan Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie\nSummers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin\nTootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón\nUribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welin-\nder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,\nHannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah\nYoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,\nShengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.Gpt-4 technical\nreport, 2024. URL https://arxiv.org/abs/2303.08774.Stanislas Polu and Ilya Sutskever.Generative language modeling for automated theorem proving,\n\n2020.13\n\n\fStanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\n\nSutskever.Formal mathematics statement curriculum learning, 2022.Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, and Swarat Chaudhuri.An in-\n\ncontext learning agent for formal theorem-proving, 2024.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H.\nthought prompting elicits reasoning\nChi, Quoc V Le, and Denny Zhou.in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=_VjQlMeSB_J.Chain of\n\nFreek Wiedijk. Formal proof sketches.In Stefano Berardi, Mario Coppo, and Ferruccio Damiani\n(eds.), Types for Proofs and Programs, pp.378–393, Berlin, Heidelberg, 2004.Springer Berlin\nHeidelberg. ISBN 978-3-540-24849-1.Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil,\nRyan Prenger, and Anima Anandkumar.LeanDojo: Theorem proving with retrieval-augmented\nlanguage models.In Neural Information Processing Systems (NeurIPS), 2023.Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig.Docprompting: Gener-\nating code by retrieving the docs.In The Eleventh International Conference on Learning Repre-\nsentations, 2023.URL https://openreview.net/forum?id=ZTCxT2t2Ru.14\n\n\fA PROMPTS\n\nA.1 TEMPLATE\n\nIn this appendix, we note the prompts used by ImProver both for general LLM prompting, as well\nas the metric-speciﬁc prompts.For the main prompt sent to the LLM on each sample, we build a prompt string using a chat prompt\ntemplate that is then invoked at runtime to ﬁll in the variables.Namely, these variables include the set of metric prompts, previous results, input theorem, context,\na syntax documents, Mathlib documents, and examples.The prompt template is a conversation of the format:\n\nPlaceholder: All metric prompts with a ‘System’ role\nSystem: You will be given the proof context (i.e.the lean ﬁle contents/imports leading up\nto the theorem declaration) wrapped by <CONTEXT>...</CONTEXT>.You will be given the previous num_prev input/output pairs as well as their metric (met-\nric.name) score and correctness score, as well as any error messages, for your reference to\nimprove upon.Each of these previous results will be wrapped with <PREV I=0></PREV\nI=0>,...,<PREV I=num_prev-1></PREV I=num_prev-1>, with I=num_prev-1 being the\nmost recent result.Remember to use lean 4 syntax, which has signiﬁcant changes from the lean 3 syntax.To\nassist with the syntax relating to the current theorem and current error messages, you will\nbe given num_syntax_docs documents to refer to for ﬁxing these syntax issues.Each of\nthese documents will be wrapped with <SYNTAX_DOC>...</SYNTAX_DOC>.You will also receive num_mathlib_docs documents relevant to the current theorem to\nhelp with formulating your modiﬁed proof.Each of these will be wrapped with <CON-\nTENT_DOC>...</CONTENT_DOC>\nYou will also receive num_examples examples of input-output pairs of proofs that\nwere optimized for the metric metric.Each of these will be wrapped with <EXAM-\nPLE>...</EXAMPLE>\nYou will be given the tactic states as comments for reference.The current theorem will be\nwrapped in <CURRENT>...</CURRENT>\nSystem: Output format instructions\nPlaceholder: All retrieved syntax documentation\nPlaceholder: All retrieved mathlib documentation\nPlaceholder: All retrieved examples\nUser: <CONTEXT> context </CONTEXT>\nPlaceholder: Previous results and inputs/outputs\nPlaceholder: All metric prompts with a ‘User’ role\nUser: <CURRENT> theorem </CURRENT>\n\nThis prompt is then invoked and sent to the language model by ﬁlling in all the variables and place-\nholders.Notably, when we invoke the chain given by chain|llm|parser, we throttle the invoca-\ntion with a randomized exponential rate limit throttling to account for API rate limits, especially in\nhighly-parallelized requests like when benchmarking over a large number of theorems.A.2 METRIC PROMPTS\n\nLength Metric\n\nSystem: You are an AI assistant who shortens Lean 4 proofs while ensuring their correct-\nness.You will aim to reduce the number of lines of the tactic proof while ensuring that it\nproperly compiles in Lean 4.15\n\n\fUser: Shorten the current theorem (wrapped in <CURRENT>...</CURRENT>) to be as\nshort in length—measured in the number of lines of the proof—as possible, while also\nensuring that the output is still syntactically correct.\"Readability Metric\n\nCompletion Metric\n\nSystem: You are an AI assistant who rewrites Lean 4 proofs to be more readable while\nensuring their correctness.We measure readablity by considering the ratio of the number\nofexplicitly typed have tactics against the total number of tactics in the proof, as this is\nproportional to whether a proof is declarative in style, and thus, readable.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is\nmore readable and declarative and modular.System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.You will receive a Lean 4 proof you must\nmodify to eliminate any errors so that it compiles correctly and eliminate any “sorry”s with\nfull proofs.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a\nformal, complete, and correct Lean 4 proof by ﬁlling in its tactic proof.B ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this section, we provide more detailed information on the experimental setup and results used to\nevaluate ImProver.B.1 ABLATION DETAILS\n\nWe now proceed to show detailed results from our ablation testing.Table 8: Output and Chain-of-States Ablations\n\nOutput Format\n\nCoS\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.string\nTrue\nstring\nFalse\nstring list True\nstring list\nFalse\nstring tree\nTrue\nstring tree\nFalse\n\n7.53\n7.03\n8.04\n7.04\n7.62\n6.31\n\n16.12\n19.67\n12.38\n13.58\n15.34\n14.17\n\n46.72%\n35.77%\n64.96%\n51.82%\n49.64%\n44.53%\n\n16.79%\n15.33%\n21.17%\n18.98%\n18.25%\n16.06%\n\nBy Table 8, we see that the optimal combination in this testing group is a string list output\nformat with CoS enabled.Fix these values for all future tests.Examples\n\nTable 9: Example Retrieval Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.0\n3\n5\n7\n10\n\n5.67\n8.49\n8.38\n7.56\n9.34\n\n67.15%\n62.04%\n64.96%\n62.77%\n63.5%\n\n16.79%\n19.71%\n21.17%\n19.71%\n21.9%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the number of examples.By Table 9,\nwe see that the optimal combination in this testing group is 10 examples.Fix this value for all future\ntests.8.44\n13.68\n12.9\n12.04\n14.7\n\n16\n\n\fTable 10: Sampling Method Ablations\n\nMethod\n\nForward Keep Best\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.None\nreﬁnement\nreﬁnement\nreﬁnement\nreﬁnement\nbest-of-n\n\nN/A\n1\n5\n1\n5\nN/A\n\nN/A\nFalse\nFalse\nTrue\nTrue\nN/A\n\n9.34\n14.76\n12.5\n14.95\n13.15\n15.35\n\n14.7\n30.63\n20.88\n14.95\n13.15\n18.44\n\n63.5%\n48.18%\n59.85%\n100.0%\n100.0%\n83.21%\n\n21.9%\n30.66%\n30.66%\n30.66%\n29.93%\n36.5%\n\nNote that forward and keep-best values are parameters for reﬁnement of how many previous itera-\ntions to forward, and whether to keep the most recent or the best iteration in subsequent reﬁnement\nsteps.Now, with the previous optimal parameters ﬁxed, run the ablation on the sample method.By\nTable 10, we see that the optimal combination in this testing group is best-of-n.Fix this value\nfor all future tests.Table 11: Model and n Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.Model\n\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\n\nn\n\n3\n5\n7\n10\n15\n3\n5\n7\n10\n15\n20\n\n19.66\n20.12\n22.44\n21.73\n23.51\n3.65\n5.12\n3.65\n4.99\n4.35\n4.87\n\n24.36\n24.97\n27.21\n25.28\n26.28\n4.63\n6.21\n4.34\n5.69\n5.06\n5.56\n\n80.7%\n80.56%\n82.46%\n85.96%\n89.47%\n78.95%\n82.46%\n84.21%\n87.72%\n85.96%\n87.72%\n\n38.6%\n36.11%\n42.11%\n40.35%\n45.61%\n8.77%\n10.53%\n8.77%\n12.28%\n12.28%\n14.04%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the value of n and model.By\nTable 11, we see that the optimal combination in this testing group is GPT-4o with n = 15.Fix this\nvalue for all future tests.Table 12: RAG and Combination Sampling Method Ablations\n\nRAG Improvement Nonempty Improve. Accuracy\n\nImproved Acc.Combination\n\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nbest-of-n\nbest-of-n\n\nm\n\nm′\n\n5\n3\n5\n3\n3\n5\n3\n5\n5\n3\n5\n3\n3\n5\n5\n3\nN/A 15\nN/A 15\n\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n33.78\n31.23\n31.85\n31.35\n51.32\n50.1\n57.56\n49.75\n32.71\n33.48\n\n100.0%\n100.0%\n100.0%\n100.0%\n63.64%\n65.62%\n60.61%\n59.38%\n90.62%\n84.38%\n\n50.0%\n46.88%\n50.0%\n50.0%\n48.48%\n53.12%\n54.55%\n43.75%\n56.25%\n53.12%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the combination methods and if\nRAG is enabled.By Table 12, we see that the optimal combination in this testing group is a 5-step\nreﬁnement with each iteration being a best-of-3 call, with RAG enabled.B.2 ADDITIONAL QUALITATIVE EXAMPLES\n\nIn this section, we provide qualitative examples demonstrating the improvements ImProver achieves\nin proof optimization.33.78\n31.23\n31.85\n31.35\n32.66\n32.88\n34.88\n29.54\n29.64\n28.25\n\n17\n\n\fCompﬁles: Length Optimization See (§4.3)\n\nCompﬁles: Readability Optimization Consider Figure 5, in which a lemma from the 2019 IMO\nproblem 1 (from the Compﬁles dataset) is optimized for readability.This introduces multiple new\nhypotheses, which generalize a linear_property of the functions, and then reuses and instanti-\nates that (and others, too) hypothesis throughout the proof, creating a signiﬁcantly more declaritive,\nmodular, and therefore readable proof.Original (human-written)\n\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\nrefine hf 1, fun a => ?_i\nchange g a = g 1 * a\nrw [mul_comm, ← smul_eq_mul, ←\n\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nImProver (readability-optimized)\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\n\nhave linear_property : ∀ a, f a = g a := by\n\nintro a\nrfl\n\nhave g_smul : ∀ a, g a = g 1 * a := by\n\nintro a\nrw [mul_comm, ← smul_eq_mul, ←\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nrefine hf 1, fun a => ?_i\nhave f_eq_g : f a = g a := linear_property a\nhave g_a_eq : g a = g 1 * a := g_smul a\nrw [f_eq_g, linear_property 1, g_a_eq]\n\nFigure 5: Optimizing a lemma from IMO 2019 P1 for readability\n\nMIL: Length Optimization Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, eliminating simp calls and introducing proof terms\ninto the structure of the proof to shorten it from 9 tactic invocations to 7.Original (human-written)\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\n\n:= by\n\next x\nsimp only [mem_map]\nconstructor\n· rintro hy, y_in, hyi\n\nexact hϕ y, hy, y_in, rfli, hyi\n\n· rintro hy, hz, z_in, hzi, hyi\n\nuse z, z_in\ncalc ψ.comp ϕ z = ψ (ϕ z) := rfl\n_\n= ψ y := by congr\n\nImProver (length-optimized)\n\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\nby\next x\nsimp only [mem_map]\nconstructor\nrintro hy, y_in, hyi; exact hϕ y, hy, y_in,\n\n:=\n\nrfli, hyi\n\nrintro hy, hz, z_in, hzi, hyi; exact hz,\nz_in, (congr_arg ψ hz).trans hyi\n\nFigure 6: Optimizing a lemma from the solutions of MIL CH08 S01 for length\n\nMIL: Length Optimization 2 Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, converting a full tactic proof into a single proof term\nto shorten it from 28 tactic invocations to 1.Note that the model does not have access to the Lean\ncommands that symbolically generate proof terms, and therefore generates and estimates the proof\nterm entirely by itself.MIL: Readability Optimization See (§4.3)\n\nMathlib: Length Optimization Consider Figure 8, which optimizes a theorem in algebraic topol-\nogy from mathlib for length, eliminating simp calls and combining tactics to shorten it from 3 tactic\ninvocations to 1.Mathlib: Readability Optimization Consider Figure 9, a theorem from Mathlib that we optimize\nfor readability.This original proof carried a score of 0, as it does not contain any declarative statements.It is concise\nand efﬁcient, however, it is difﬁcult to understand and read.18\n\n\fOriginal (human-written)\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\next x; constructor\n· rintro (hxs, xnti | hxt, xnsi)\n\nImProver (length-optimized)\n\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\nexact Set.ext fun x => hfun h => h.elim\n\n(fun hxs, xnti => hOr.inl xs, fun h_, xti =\n> xnt xti) (fun hxt, xnsi => hOr.inr xt,\nfun hxs, _i => xns xsi),\n\nfun hh, nxsti => h.elim (fun xs => Or.inl h\nxs, fun xt => nxst hxs, xtii) (fun xt =>\nOr.inr hxt, fun xs => nxst hxs, xtii)i\n\n· constructor\n\nleft\nexact xs\nrintro h_, xti\ncontradiction\n\n.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.This intermediate tactic greatly helps in the understandability and clarity of the proof.Original (human-written)\n\nImProver (readability-optimized)\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n:=\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]; rfl\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\n:=\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nhave h1 : ∀ x y : Y, Subsingleton\n\n(Path.Homotopic.Quotient x y) ↔ ∀ {p1 p2\n: Path x y}, Path.Homotopic p1 p2 := by\nintro x y\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]\nrfl\n\nsimp only [h1]\n\nFigure 9: Optimizing a theorem from Mathlib/FundamentalGroupoid/SimplyConnected\nfor readability\n\n19",
          "time": 1221.5163176059723,
          "input_token_size": 16876,
          "output_token_size": 263,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_5411ab1e",
        "created_at": 1749769250.8037224,
        "processing_time": 1238.4015984535217,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.832851"
      }
    }
  },
  {
    "5c94718a": {
      "question": "What is the name of the tool designed for automated proof optimization?",
      "answer": "ImProver",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            20,
            21,
            22
          ],
          "provenance_ids": [
            21
          ],
          "provenance": "1\n\n\fWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more readable, and more declarative in style.1\n\nOriginal (human-written)\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\nImProver (length-optimized)\n\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\n(h1 : ∀ x, ∃!",
          "time": 5.4762678146362305,
          "input_token_size": 16503,
          "output_token_size": 21,
          "original_provenance_id": 0
        },
        {
          "provenance_id": 1,
          "input_sentence_ids": [
            8,
            9,
            10
          ],
          "provenance_ids": [
            8
          ],
          "provenance": "We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.",
          "time": 9.611883878707886,
          "input_token_size": 17806,
          "output_token_size": 32,
          "original_provenance_id": 1
        },
        {
          "provenance_id": 2,
          "input_sentence_ids": [
            5,
            6,
            7
          ],
          "provenance_ids": [
            7
          ],
          "provenance": "We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.",
          "time": 11.01710295677185,
          "input_token_size": 17973,
          "output_token_size": 35,
          "original_provenance_id": 2
        },
        {
          "provenance_id": 3,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4
          ],
          "provenance_ids": [
            1
          ],
          "provenance": "0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.",
          "time": 14.659303188323975,
          "input_token_size": 18229,
          "output_token_size": 38,
          "original_provenance_id": 3
        },
        {
          "provenance_id": 4,
          "input_sentence_ids": [
            32,
            33,
            34
          ],
          "provenance_ids": [
            34
          ],
          "provenance": "ImProver brings these code generation techniques, along with new\nChain-of-States prompting and meta-programmed contextual information, into a uniﬁed proof opti-\nmization agent.",
          "time": 19.330724716186523,
          "input_token_size": 20663,
          "output_token_size": 50,
          "original_provenance_id": 4
        }
      ],
      "metadata": {
        "question_id": "paper_5c94718a",
        "created_at": 1749665182.072706,
        "processing_time": 23.386286973953247,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev",
        "reason": "shorter_provenance",
        "sentence_count": 17,
        "provenance_count": 5,
        "alternatives": [
          {
            "source": "test_outputs_prev",
            "sentence_count": 17,
            "provenance_count": 5,
            "has_answer": true
          },
          {
            "source": "test_outputs_prev1",
            "sentence_count": 17,
            "provenance_count": 5,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.832851"
      }
    }
  },
  {
    "6964f0d7": {
      "question": "What is the primary aim of ImProver?",
      "answer": "The primary aim of ImProver is to automate proof optimization by rewriting formal proofs to optimize for arbitrary user-defined metrics such as length or readability while ensuring correctness.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            0,
            1,
            3,
            4,
            5,
            6,
            8,
            231,
            232,
            233,
            234,
            235,
            239,
            241,
            243,
            244,
            245,
            246,
            247,
            251,
            252,
            253,
            254,
            255,
            257,
            262,
            263,
            264,
            267,
            268,
            269,
            270,
            275,
            276,
            277,
            278,
            279,
            280,
            288,
            293,
            304,
            305,
            307,
            308,
            309,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            328,
            329,
            330,
            331,
            335,
            336,
            337,
            338,
            339,
            341,
            342,
            346,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance": "4\n2\n0\n2\n \nt\nc\nO\n \n7\n \n \n]\nI\n\nA\n. s\nc\n[\n \n \n1\nv\n3\n5\n7\n4\n0\n.0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.To this end, we study a new problem of automated proof\noptimization: rewriting a proof so that it is correct and optimizes for an arbitrary\ncriterion, such as length or readability.As a ﬁrst method for automated proof opti-\nmization, we present ImProver, a large-language-model agent that rewrites proofs\nto optimize arbitrary user-deﬁned metrics in Lean.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.Sean Welleck thanks Convergent Research, the Lean FRO, and the OpenAI Researcher Access Pro-\ngram for their support.11\n\n\fREFERENCES\n\nand\n\nAlphaGeometry\n\nAlphaProof\nstandard\nhttps://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\n2024.\n\nsilver-medal\nproblems.achieves\nolympiad\n\nmathematical\n\ninternational\n\nsolving\n\nTeams.AI\n\nSerge Autexier and Dominik Dietrich.A tactic language for declarative proofs.Jaime Carbonell and Jade Goldstein.In Proceedings of the 21st Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ’98, pp.Association for Computing Machinery. 10.1145/290941.291025.URL https://doi.org/10.1145/290941.291025.Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.Teaching large language models\n\nto self-debug, 2023.URL https://arxiv.org/abs/2304.05128. David Renshaw. compﬁles.Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu.Proof artifact co-\nIn International Conference on Learning\n\ntraining for theorem proving with language models.Representations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.Jiewen Hu, Thomas Zhu, and Sean Welleck.minictx: Neural theorem proving with (long-)contexts,\n\n2024.Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu.https://github.com/kim-em/lean-training-data,\n\nGuillaume Lample, Timothee Lacroix, Marie anne Lachaux, Aurelien Rodriguez, Amaury\nHypertree proof search\nHayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.for neural\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.A survey on deep learning for theorem proving, 2024.Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang.A survey of deep\nlearning for mathematical reasoning.In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki (eds.URL\nhttps://aclanthology.org/2023.acl-long.817.doi:\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-reﬁne:\nIterative reﬁnement with self-feedback.In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.URL https://openreview.net/forum?id=S37hOerQLB.12\n\nleanprover-community. mathematics_in_lean.Deduction, Virtual Event, July 12–15, 2021, Proceedings, pp.Gpt-4 technical\nreport, 2024. URL https://arxiv.org/abs/2303.08774.Chain of\n\nFreek Wiedijk. Formal proof sketches.In Stefano Berardi, Mario Coppo, and Ferruccio Damiani\n(eds.378–393, Berlin, Heidelberg, 2004.Springer Berlin\nHeidelberg. ISBN 978-3-540-24849-1.Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil,\nRyan Prenger, and Anima Anandkumar.Namely, these variables include the set of metric prompts, previous results, input theorem, context,\na syntax documents, Mathlib documents, and examples.The prompt template is a conversation of the format:\n\nPlaceholder: All metric prompts with a ‘System’ role\nSystem: You will be given the proof context (i.e.the lean ﬁle contents/imports leading up\nto the theorem declaration) wrapped by <CONTEXT>...</CONTEXT>.You will be given the previous num_prev input/output pairs as well as their metric (met-\nric.name) score and correctness score, as well as any error messages, for your reference to\nimprove upon.Each of these previous results will be wrapped with <PREV I=0></PREV\nI=0>,...,<PREV I=num_prev-1></PREV I=num_prev-1>, with I=num_prev-1 being the\nmost recent result.Remember to use lean 4 syntax, which has signiﬁcant changes from the lean 3 syntax.To\nassist with the syntax relating to the current theorem and current error messages, you will\nbe given num_syntax_docs documents to refer to for ﬁxing these syntax issues.Each of\nthese documents will be wrapped with <SYNTAX_DOC>...</SYNTAX_DOC>.Each of these will be wrapped with <EXAM-\nPLE>...</EXAMPLE>\nYou will be given the tactic states as comments for reference.The current theorem will be\nwrapped in <CURRENT>...</CURRENT>\nSystem: Output format instructions\nPlaceholder: All retrieved syntax documentation\nPlaceholder: All retrieved mathlib documentation\nPlaceholder: All retrieved examples\nUser: <CONTEXT> context </CONTEXT>\nPlaceholder: Previous results and inputs/outputs\nPlaceholder: All metric prompts with a ‘User’ role\nUser: <CURRENT> theorem </CURRENT>\n\nThis prompt is then invoked and sent to the language model by ﬁlling in all the variables and place-\nholders.Notably, when we invoke the chain given by chain|llm|parser, we throttle the invoca-\ntion with a randomized exponential rate limit throttling to account for API rate limits, especially in\nhighly-parallelized requests like when benchmarking over a large number of theorems.A.2 METRIC PROMPTS\n\nLength Metric\n\nSystem: You are an AI assistant who shortens Lean 4 proofs while ensuring their correct-\nness.We measure readablity by considering the ratio of the number\nofexplicitly typed have tactics against the total number of tactics in the proof, as this is\nproportional to whether a proof is declarative in style, and thus, readable.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is\nmore readable and declarative and modular.System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.You will receive a Lean 4 proof you must\nmodify to eliminate any errors so that it compiles correctly and eliminate any “sorry”s with\nfull proofs.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a\nformal, complete, and correct Lean 4 proof by ﬁlling in its tactic proof.B.1 ABLATION DETAILS\n\nWe now proceed to show detailed results from our ablation testing.Table 8: Output and Chain-of-States Ablations\n\nOutput Format\n\nCoS\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.0\n3\n5\n7\n10\n\n5.67\n8.49\n8.38\n7.56\n9.34\n\n67.15%\n62.04%\n64.96%\n62.77%\n63.5%\n\n16.79%\n19.71%\n21.17%\n19.71%\n21.9%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the number of examples.None\nreﬁnement\nreﬁnement\nreﬁnement\nreﬁnement\nbest-of-n\n\nN/A\n1\n5\n1\n5\nN/A\n\nN/A\nFalse\nFalse\nTrue\nTrue\nN/A\n\n9.34\n14.76\n12.5\n14.95\n13.15\n15.35\n\n14.7\n30.63\n20.88\n14.95\n13.15\n18.44\n\n63.5%\n48.18%\n59.85%\n100.0%\n100.0%\n83.21%\n\n21.9%\n30.66%\n30.66%\n30.66%\n29.93%\n36.5%\n\nNote that forward and keep-best values are parameters for reﬁnement of how many previous itera-\ntions to forward, and whether to keep the most recent or the best iteration in subsequent reﬁnement\nsteps.Now, with the previous optimal parameters ﬁxed, run the ablation on the sample method.By\nTable 10, we see that the optimal combination in this testing group is best-of-n.Fix this value\nfor all future tests.Table 11: Model and n Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.Model\n\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\n\nn\n\n3\n5\n7\n10\n15\n3\n5\n7\n10\n15\n20\n\n19.66\n20.12\n22.44\n21.73\n23.51\n3.65\n5.12\n3.65\n4.99\n4.35\n4.87\n\n24.36\n24.97\n27.21\n25.28\n26.28\n4.63\n6.21\n4.34\n5.69\n5.06\n5.56\n\n80.7%\n80.56%\n82.46%\n85.96%\n89.47%\n78.95%\n82.46%\n84.21%\n87.72%\n85.96%\n87.72%\n\n38.6%\n36.11%\n42.11%\n40.35%\n45.61%\n8.77%\n10.53%\n8.77%\n12.28%\n12.28%\n14.04%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the value of n and model.By\nTable 11, we see that the optimal combination in this testing group is GPT-4o with n = 15.Fix this\nvalue for all future tests.Table 12: RAG and Combination Sampling Method Ablations\n\nRAG Improvement Nonempty Improve. Accuracy\n\nImproved Acc.Combination\n\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nbest-of-n\nbest-of-n\n\nm\n\nm′\n\n5\n3\n5\n3\n3\n5\n3\n5\n5\n3\n5\n3\n3\n5\n5\n3\nN/A 15\nN/A 15\n\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n33.78\n31.23\n31.85\n31.35\n51.32\n50.1\n57.56\n49.75\n32.71\n33.48\n\n100.0%\n100.0%\n100.0%\n100.0%\n63.64%\n65.62%\n60.61%\n59.38%\n90.62%\n84.38%\n\n50.0%\n46.88%\n50.0%\n50.0%\n48.48%\n53.12%\n54.55%\n43.75%\n56.25%\n53.12%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the combination methods and if\nRAG is enabled.By Table 12, we see that the optimal combination in this testing group is a 5-step\nreﬁnement with each iteration being a best-of-3 call, with RAG enabled.B.2 ADDITIONAL QUALITATIVE EXAMPLES\n\nIn this section, we provide qualitative examples demonstrating the improvements ImProver achieves\nin proof optimization.33.78\n31.23\n31.85\n31.35\n32.66\n32.88\n34.88\n29.54\n29.64\n28.25\n\n17\n\n\fCompﬁles: Length Optimization See (§4.3)\n\nCompﬁles: Readability Optimization Consider Figure 5, in which a lemma from the 2019 IMO\nproblem 1 (from the Compﬁles dataset) is optimized for readability.This introduces multiple new\nhypotheses, which generalize a linear_property of the functions, and then reuses and instanti-\nates that (and others, too) hypothesis throughout the proof, creating a signiﬁcantly more declaritive,\nmodular, and therefore readable proof.Original (human-written)\n\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\nrefine hf 1, fun a => ?_i\nchange g a = g 1 * a\nrw [mul_comm, ← smul_eq_mul, ←\n\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nImProver (readability-optimized)\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\n\nhave linear_property : ∀ a, f a = g a := by\n\nintro a\nrfl\n\nhave g_smul : ∀ a, g a = g 1 * a := by\n\nintro a\nrw [mul_comm, ← smul_eq_mul, ←\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nrefine hf 1, fun a => ?_i\nhave f_eq_g : f a = g a := linear_property a\nhave g_a_eq : g a = g 1 * a := g_smul a\nrw [f_eq_g, linear_property 1, g_a_eq]\n\nFigure 5: Optimizing a lemma from IMO 2019 P1 for readability\n\nMIL: Length Optimization Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, eliminating simp calls and introducing proof terms\ninto the structure of the proof to shorten it from 9 tactic invocations to 7.Original (human-written)\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\n\n:= by\n\next x\nsimp only [mem_map]\nconstructor\n· rintro hy, y_in, hyi\n\nexact hϕ y, hy, y_in, rfli, hyi\n\n· rintro hy, hz, z_in, hzi, hyi\n\nuse z, z_in\ncalc ψ.comp ϕ z = ψ (ϕ z) := rfl\n_\n= ψ y := by congr\n\nImProver (length-optimized)\n\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\nby\next x\nsimp only [mem_map]\nconstructor\nrintro hy, y_in, hyi; exact hϕ y, hy, y_in,\n\n:=\n\nrfli, hyi\n\nrintro hy, hz, z_in, hzi, hyi; exact hz,\nz_in, (congr_arg ψ hz).trans hyi\n\nFigure 6: Optimizing a lemma from the solutions of MIL CH08 S01 for length\n\nMIL: Length Optimization 2 Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, converting a full tactic proof into a single proof term\nto shorten it from 28 tactic invocations to 1.Note that the model does not have access to the Lean\ncommands that symbolically generate proof terms, and therefore generates and estimates the proof\nterm entirely by itself.MIL: Readability Optimization See (§4.3)\n\nMathlib: Length Optimization Consider Figure 8, which optimizes a theorem in algebraic topol-\nogy from mathlib for length, eliminating simp calls and combining tactics to shorten it from 3 tactic\ninvocations to 1.Mathlib: Readability Optimization Consider Figure 9, a theorem from Mathlib that we optimize\nfor readability.This original proof carried a score of 0, as it does not contain any declarative statements.It is concise\nand efﬁcient, however, it is difﬁcult to understand and read.18\n\n\fOriginal (human-written)\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\next x; constructor\n· rintro (hxs, xnti | hxt, xnsi)\n\nImProver (length-optimized)\n\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\nexact Set.ext fun x => hfun h => h.elim\n\n(fun hxs, xnti => hOr.inl xs, fun h_, xti =\n> xnt xti) (fun hxt, xnsi => hOr.inr xt,\nfun hxs, _i => xns xsi),\n\nfun hh, nxsti => h.elim (fun xs => Or.inl h\nxs, fun xt => nxst hxs, xtii) (fun xt =>\nOr.inr hxt, fun xs => nxst hxs, xtii)i\n\n· constructor\n\nleft\nexact xs\nrintro h_, xti\ncontradiction\n\n.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.This intermediate tactic greatly helps in the understandability and clarity of the proof.Original (human-written)\n\nImProver (readability-optimized)\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n:=\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]; rfl\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\n:=\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nhave h1 : ∀ x y : Y, Subsingleton\n\n(Path.Homotopic.Quotient x y) ↔ ∀ {p1 p2\n: Path x y}, Path.Homotopic p1 p2 := by\nintro x y\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]\nrfl\n\nsimp only [h1]\n\nFigure 9: Optimizing a theorem from Mathlib/FundamentalGroupoid/SimplyConnected\nfor readability\n\n19",
          "time": 260.54138350486755,
          "input_token_size": 16870,
          "output_token_size": 44,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_6964f0d7",
        "created_at": 1750198568.0620193,
        "processing_time": 263.6327407360077,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.833851"
      }
    }
  },
  {
    "7576f446": {
      "question": "How does ImProver optimize formal mathematical proofs?",
      "answer": "ImProver optimizes formal mathematical proofs by rewriting them to meet user-defined criteria such as length, readability, and correctness. It employs techniques like Chain-of-States prompting, error correction, retrieval-augmented generation, and various sampling methods to enhance the optimization process.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93
          ],
          "provenance_ids": [
            0,
            1,
            17,
            19,
            20,
            24,
            36,
            37,
            38,
            39,
            40,
            42,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            61,
            62,
            63,
            64,
            65,
            66,
            72,
            80,
            82,
            83,
            84,
            85,
            86
          ],
          "provenance": "4\n2\n0\n2\n \nt\nc\nO\n \n7\n \n \n]\nI\n\nA\n. s\nc\n[\n \n \n1\nv\n3\n5\n7\n4\n0\n.0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is\ncorrect and optimizes a criterion such as length or readability.We develop\nvarious improvements that can be applied on top of a black-box language model, including Chain-\nof-States prompting–an analogy to chain-of-thought prompting (Wei et al., 2022) that shows inter-\nmediate proof states–along with error-correction and retrieval.We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.In this example, ImProver optimizes a human-written\nlemma (right) from the 2022 International Math Olympiad (Question 2, solution from Comp-\nﬁles (David Renshaw, 2024)) for length.Given a theorem statement x, additional context c, and an initial\nproof y0, proof optimization consists of generating a new proof y that is correct and minimizes (or\nmaximizes) a metric µ(x, c, y0, y) → R.\n\n1Code is available at https://github.com/riyazahuja/ImProver.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.Note that shorter proofs often\nrepresent more efﬁcient proofs.Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.Completion Metric: The completion of a proof simply describes its correctness.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.The implementation of this extraction system is mod-\neled from the work (Kim Morrison, 2024).Speciﬁcally, in the compiler’s elaboration and eval-\nuation stages – where the parsed theorem code is ﬁrst converted into concrete syntax trees (in\npractice, Syntax objects) and abstract syntax trees (Expr objects) – we convert the CST and\nAST output objects into the relevant proof data and proof states in the form of proof trees\n(Lean.Elab.InfoTree).These proof trees contain detailed context and information on a tactic-\nby-tactic level relating to the modiﬁcation of the proof state, metavariable context, and proof cor-\nrectness.After state extraction is completed and cached for efﬁcient future access, we annotate the proof text\nitself to contain the intermediate states in the form as comments.Figure 2 shows an example. This explicit reasoning aims to help the generator model construct more optimized proofs via addi-\ntional symbolic data.3.1.2 OUTPUT FORMATTING. LLM outputs often contain ancillary and syntactically invalid content, especially before and after\nthe actual proof.Additionally, by applying additional structure to the LLM outputs, we may hope to\ngenerate more structured proofs.To analyze this hypothesis, we introduce two additional output for-\nmats to the standard str output: flat and structured.3.1.3 SAMPLING METHOD\n\nWe also introduce different methods of sampling between many (sequential or parallel) LLM infer-\nence calls, involving best-of-n and iterative reﬁnement implementations, as well as combinations\nthereof.3\n\n\fWithout Chain-of-States\n\nWith Chain-of-States\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u) := by\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u)\n\n:= by\n\nrintro x (hxs, xti | hxs, xui)\n· use xs; left; exact xt\n.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!-/\n. use xs; right; exact xu\n/-\nGoals Solved!-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.refinement((best_of_n,m),n) is a compound sampling method that runs a n-step reﬁne-\nment, where each call is a best-of-m call to the LLM.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.More speciﬁcally, example retrieval selects the most relevant user-generated examples of proof op-\ntimization on a speciﬁc metric.Namely, each metric is loaded with a cached (vector) database\npopulated with human-made examples of preoptimized and postoptimized pairs of Lean theorems.The number of examples that are retrieved is user-speciﬁed.Document retrieval extracts information using MMR from a pair of ﬁxed (vector) databases.",
          "time": 129.07600903511047,
          "input_token_size": 15982,
          "output_token_size": 212,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_7576f446",
        "created_at": 1749627233.5057604,
        "processing_time": 136.50885009765625,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev",
        "reason": "shorter_provenance",
        "sentence_count": 94,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev",
            "sentence_count": 94,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.834854"
      }
    }
  },
  {
    "81dd149b": {
      "question": "Which formal proof verification system does ImProver work with?",
      "answer": "Lean",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            20,
            21,
            22
          ],
          "provenance_ids": [
            20,
            22
          ],
          "provenance": "We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nrw [h1u _ (Classical.choose_spec _)]\nobtain hw, h1e', h1u'i := h1 y\nrw [h1u' _ ((h2 _ _).mpr h1e)]\nexact h1u' _ (Classical.choose_spec _)\n\n(h1 : ∀ x, ∃!",
          "time": 5.6663031578063965,
          "input_token_size": 16496,
          "output_token_size": 7,
          "original_provenance_id": 0
        },
        {
          "provenance_id": 1,
          "input_sentence_ids": [
            5,
            6,
            7
          ],
          "provenance_ids": [
            7
          ],
          "provenance": "We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.",
          "time": 10.118720769882202,
          "input_token_size": 17960,
          "output_token_size": 13,
          "original_provenance_id": 1
        },
        {
          "provenance_id": 2,
          "input_sentence_ids": [
            47,
            48,
            49,
            50,
            51
          ],
          "provenance_ids": [
            48,
            49,
            51
          ],
          "provenance": "The intermediate states often contain valuable information (e.g., an expression\nafter it has been simpliﬁed) that is not present in the tactics.To allow the model to reason about\nthese intermediate goals and hypotheses, we use tools from Lean metaprogramming to automatically\nannotate each proof state as a comment prior to each tactic.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.",
          "time": 17.784254550933838,
          "input_token_size": 24030,
          "output_token_size": 21,
          "original_provenance_id": 2
        },
        {
          "provenance_id": 3,
          "input_sentence_ids": [
            70,
            71,
            72,
            73,
            74,
            75
          ],
          "provenance_ids": [
            70,
            71,
            72,
            74,
            75
          ],
          "provenance": "This best-of-n technique is implemented as a curried function such that each of the n calls can be\nhandled by any arbitrary sampling method, or just a single standard prompt at user discretion.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.Each iteration carries information on the last prev_num iterations,\nincluding input, output, metric score, correctness, and error messages.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.",
          "time": 22.922247171401978,
          "input_token_size": 26003,
          "output_token_size": 28,
          "original_provenance_id": 3
        },
        {
          "provenance_id": 4,
          "input_sentence_ids": [
            223,
            224,
            225
          ],
          "provenance_ids": [
            224
          ],
          "provenance": "5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.",
          "time": 34.30055022239685,
          "input_token_size": 54160,
          "output_token_size": 42,
          "original_provenance_id": 4
        }
      ],
      "metadata": {
        "question_id": "paper_81dd149b",
        "created_at": 1749665189.4284434,
        "processing_time": 36.77856683731079,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev",
        "reason": "shorter_provenance",
        "sentence_count": 20,
        "provenance_count": 5,
        "alternatives": [
          {
            "source": "test_outputs_prev",
            "sentence_count": 20,
            "provenance_count": 5,
            "has_answer": true
          },
          {
            "source": "test_outputs_prev1",
            "sentence_count": 26,
            "provenance_count": 5,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "84cee75a": {
      "question": "What university are the authors affiliated with?",
      "answer": "Carnegie Mellon University",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4
          ],
          "provenance_ids": [
            1
          ],
          "provenance": "0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.",
          "time": 6.232700824737549,
          "input_token_size": 16089,
          "output_token_size": 31,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_84cee75a",
        "created_at": 1749869880.098623,
        "processing_time": 15.75581431388855,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "shorter_provenance",
        "sentence_count": 5,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 5,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "8e9232eb": {
      "question": "What is the main purpose of the ImProver tool?",
      "answer": "The main purpose of the ImProver tool is to automate the optimization of formal proofs in Lean, focusing on criteria such as length, readability, and correctness.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance": "4\n2\n0\n2\n \nt\nc\nO\n \n7\n \n \n]\nI\n\nA\n. s\nc\n[\n \n \n1\nv\n3\n5\n7\n4\n0\n.0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.To this end, we study a new problem of automated proof\noptimization: rewriting a proof so that it is correct and optimizes for an arbitrary\ncriterion, such as length or readability.As a ﬁrst method for automated proof opti-\nmization, we present ImProver, a large-language-model agent that rewrites proofs\nto optimize arbitrary user-deﬁned metrics in Lean.We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.In\npractice, however, informal, natural-language proofs are prone to imprecision, ambiguity, and error.Using a formal language such as Lean (Moura & Ullrich, 2021) removes ambiguity and precision\nand enables a proof assistant to verify correctness down to the primitives of a formal axiomatic\nsystem.Formal proofs, however, can be hard to read and often suffer from low reusability or excessive detail.For example, formal proofs in Lean’s extensive mathematical library, Mathlib (mathlib Community,\n2020), are generally designed to be concise and very general, often at the expense of readability.Formal proofs in an expository text, in contrast, may include detailed calculations steps, making\nthem readable but verbose.Machine learning systems trained on such proofs learn to mimic these\nvaried conventions (Hu et al., 2024), which may not be the optimal use of the limited supply of\nhuman-written proofs.As a result, we would like to be able to automatically refactor proofs to meet\na secondary objective such as length or readability.We ﬁnd that naively applying LLMs\nto proof optimization falls short, often resulting in incorrect or poorly optimized proofs.We develop\nvarious improvements that can be applied on top of a black-box language model, including Chain-\nof-States prompting–an analogy to chain-of-thought prompting (Wei et al., 2022) that shows inter-\nmediate proof states–along with error-correction and retrieval.We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.1\n\n\fWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more readable, and more declarative in style.1\n\nOriginal (human-written)\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\nImProver (length-optimized)\n\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nrw [h1u _ (Classical.choose_spec _)]\nobtain hw, h1e', h1u'i := h1 y\nrw [h1u' _ ((h2 _ _).mpr h1e)]\nexact h1u' _ (Classical.choose_spec _)\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nhave h2' : Classical.choose (h1 x).exists =\n\ny :=\nh1u _ (Classical.choose_spec (h1\nx).exists)\n\nrw [h2']\nobtain hw, h1e', h1u'i := h1 y\nhave h4 := Classical.choose_spec (h1\n\nhave hxw : x = w := by\n\ny).exists\n\napply h1u'\nrw [h2]\nexact h1e\n\nrw [hxw]\nexact h1u' _ h4\n\nFigure 1: ImProver automatically rewrites formal proofs to optimize a criterion such as length\nor readability while remaining correct.In this example, ImProver optimizes a human-written\nlemma (right) from the 2022 International Math Olympiad (Question 2, solution from Comp-\nﬁles (David Renshaw, 2024)) for length.ImProver’s optimized proof is correct and more concise.2 RELATED WORK\n\nRecently there has been wide interest in automating theorem proving in interactive proof assistants;\nsee (Lu et al., 2023; Li et al., 2024) for surveys.A typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs\nsuch as Lean’s Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024).A model learns from the distribution of proofs in the cor-\npus, such as Mathlib-style proofs.Recently, the AlphaProof (AlphaProof & Teams, 2024) system\nwas shown to produce proofs with an arcane, non-human structure and syntax.We consider the\nnew problem of rewriting a proof to optimize a metric, such as rewriting a proof into a more read-\nable or more concise one.Proof optimization is more general than theorem proving, since we can\nalso rewrite an empty proof to optimize correctness.Finally, there is a rich literature on the var-\nied styles of (human) formal proofs (e.g., (Autexier & Dietrich, 2010; Wiedijk, 2004)).Our model,\nImProver, builds on neural theorem proving techniques including full proof generation (Jiang et al.,\n2023; First et al., 2023), conditioning on example proofs (Jiang et al., 2023), retrieval (Yang et al.,\n2023; Thakur et al., 2024), and preceding ﬁle context (First et al., 2023; Hu et al., 2024), as well as\nerror correction (Madaan et al., 2023; Chen et al., 2023) and documentation retrieval (Zhou et al.,\n2023) from code generation.ImProver brings these code generation techniques, along with new\nChain-of-States prompting and meta-programmed contextual information, into a uniﬁed proof opti-\nmization agent.3 AUTOMATED PROOF OPTIMIZATION WITH ImProver\n\nAutomated Proof Optimization.Given a theorem statement x, additional context c, and an initial\nproof y0, proof optimization consists of generating a new proof y that is correct and minimizes (or\nmaximizes) a metric µ(x, c, y0, y) → R.\n\n1Code is available at https://github.com/riyazahuja/ImProver.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.Note that shorter proofs often\nrepresent more efﬁcient proofs.Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.Concretely, we evaluate this using the ratio of number of explicitly typed\nhave tactics to total number of tactic invocations.Completion Metric: The completion of a proof simply describes its correctness.This is a trivial\nmetric which measures the number of errors present.The completion metric is used for concretely\nviewing proof optimization as a generalization of neural theorem proving.3.1\n\nIMPROVER\n\nWe develop several improvements that can be applied to a black-box LLM generator yout ∼\nG(·|xin), such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these pa-\nrameters.The explicit prompts and templates that are sent to the LLM can be found in (§A).3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.The intermediate states often contain valuable information (e.g., an expression\nafter it has been simpliﬁed) that is not present in the tactics.To allow the model to reason about\nthese intermediate goals and hypotheses, we use tools from Lean metaprogramming to automatically\nannotate each proof state as a comment prior to each tactic.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.The implementation of this extraction system is mod-\neled from the work (Kim Morrison, 2024).Speciﬁcally, in the compiler’s elaboration and eval-\nuation stages – where the parsed theorem code is ﬁrst converted into concrete syntax trees (in\npractice, Syntax objects) and abstract syntax trees (Expr objects) – we convert the CST and\nAST output objects into the relevant proof data and proof states in the form of proof trees\n(Lean.Elab.InfoTree).These proof trees contain detailed context and information on a tactic-\nby-tactic level relating to the modiﬁcation of the proof state, metavariable context, and proof cor-\nrectness.After state extraction is completed and cached for efﬁcient future access, we annotate the proof text\nitself to contain the intermediate states in the form as comments.Figure 2 shows an example. This explicit reasoning aims to help the generator model construct more optimized proofs via addi-\ntional symbolic data.3.1.2 OUTPUT FORMATTING. LLM outputs often contain ancillary and syntactically invalid content, especially before and after\nthe actual proof.Additionally, by applying additional structure to the LLM outputs, we may hope to\ngenerate more structured proofs.To analyze this hypothesis, we introduce two additional output for-\nmats to the standard str output: flat and structured.The former enforces a tactic sequence\noutput as a list of strings, and the latter enforces a proof tree output as a tree of strings.3.1.3 SAMPLING METHOD\n\nWe also introduce different methods of sampling between many (sequential or parallel) LLM infer-\nence calls, involving best-of-n and iterative reﬁnement implementations, as well as combinations\nthereof.3\n\n\fWithout Chain-of-States\n\nWith Chain-of-States\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u) := by\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u)\n\n:= by\n\nrintro x (hxs, xti | hxs, xui)\n· use xs; left; exact xt\n.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!-/\n. use xs; right; exact xu\n/-\nGoals Solved!-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.More speciﬁcally, our scoring function is given by the 2-ary\ncomparison function S, whose arguments are output objects y, y′.max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.This comparison function can be extended to evaluate the best output of any ﬁnite n via induction.This best-of-n technique is implemented as a curried function such that each of the n calls can be\nhandled by any arbitrary sampling method, or just a single standard prompt at user discretion.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.The reﬁnement process relies on user-deﬁned pa-\nrameters n and prev_num to specify the number of iterations and the number of previous iteration\ninfo to forward, respectively.Each iteration carries information on the last prev_num iterations,\nincluding input, output, metric score, correctness, and error messages.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.Similar to the best-of-n tech-\nnique, it relies on an argument n for the number of reﬁnement steps, and is curried such that each\nreﬁnement step can be handled by any other prompting function.However, unlike best-of-n, there is\nno opportunity for parallelism as each iteration is dependent on information from the previous call.Combination Sampling and Compound Prompt Functions Compound prompt functions utilize\nthe curried nature of the implementations of best-of-n and reﬁnement to nest these techniques within\none another.For example:\n\n4\n\n\fbest_of_n((refinement,m),n) is a compound sampling method that run a best-of-n,\nwhere each call is a m-step reﬁnement.refinement((best_of_n,m),n) is a compound sampling method that runs a n-step reﬁne-\nment, where each call is a best-of-m call to the LLM.Note that with each of these compound prompt functions, there are always a total of mn iterations.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.More speciﬁcally, example retrieval selects the most relevant user-generated examples of proof op-\ntimization on a speciﬁc metric.Namely, each metric is loaded with a cached (vector) database\npopulated with human-made examples of preoptimized and postoptimized pairs of Lean theorems.The number of examples that are retrieved is user-speciﬁed.Document retrieval extracts information using MMR from a pair of ﬁxed (vector) databases.The\ndatabases store semantically chunked data from the Theorem Proving in Lean (TPiL) handbook –\ncontaining syntax guides and tactic explanations – and the Mathlib mathematics libary – containing\nthousands of theorems and lemmas.The chunking is handled by a recursive character splitter, which\nsplits the TPiL markdown ﬁles at on its headers and Mathlib ﬁles at the start of theorems, examples,\nlemmas, and deﬁnitions – with chunk sizes of 1000 characters with a 200 character overlap.The Mathlib retriever ﬁnds the top k documents that score the highest MMR score against the current\ntheorem, the TPiL retriever ﬁnds the top k documents that score the highest MMR score against the\ncurrent theorem in context and all current error messages.This retrieval process helps in generating\nmore contextually accurate prompts that allow the language model to better correct its own errors as\nwell as ﬁnd useful lemmas to reference.4 EXPERIMENTS\n\nWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics and compare its results to those of the base GPT-4o and GPT-4o-mini\nmodels.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.4.1 SETUP\n\nOur experimentation is split into three distinct stages.We ﬁrst perform ablation testing on the\nImProver model parameters (§3.1) to ensure that ImProver’s parameter speciﬁcation is the optimal\none with respect to correctness and metric optimization score.We then evaluate this optimal param-\neter combination on datasets of varying complexity and analyze the performance and results thereof.Lastly, we note the performance of ImProver in NTP applications in comparison to the base GPT-4o\nand GPT-4o-mini models.Datasets. We evaluate ImProver on subsets of the following datasets.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.We use exercise solutions from set theory, elementary number theory, group\ntheory, topology, differential calculus, and integration & measure theory.This dataset contains the-\norems at an undergraduate-level of complexity.For our main results, we evaluated on 72 theorems\nfrom exercise solutions from MIL chapters 4, 5, 8, 9, and 10.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.This dataset contains theorems of a competitive format, and although they contain\n\n5\n\n\fconcepts only at a high-school level, the logical complexity of internationally-renowned competi-\ntion results is far above that.For our main results, we used all 26 theorems and lemmas from the\nCompﬁles database of complete solutions to the International Mathematics Olympiad (IMO) and\nthe American Mathematics Olympiad (USAMO) from 2016-2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.These proofs are extremely\nefﬁcient, concise, and generalized - which often comes at the cost of readability and understandabil-\nity.These results and theorems often are at the cutting edge of research.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.This is the most difﬁcult dataset. Models.Our base generator uses GPT-4o (OpenAI et al., 2024).Since no prior methods currently\nexist for automated proof optimization, we consider a prompted GPT-4o without the improvements\ndescribed in (§3.1) as our baseline.Additionally, for a given metric, we write a prompt that brieﬂy\ndescribes the metric and the proof optimization task.We also provide instructions, context, and in-\nformation depending on the features selected, and add the theorem and proof to the prompt.Speciﬁc\nprompt information is detailed in (§A)\n\nPerformance metrics.Since proof optimization is a new task, we deﬁne four performance metrics\nfor measuring aspects of correctness and improvement.First, we deﬁne improvement for length as percentage change in length, µlen(y0)−µlen(y)\n× 100.For\nreadability, we use the difference, µread(y)−µread(yo).If no correct output is generated by the model\nfor a speciﬁc theorem, improvement is deﬁned to be zero.We deﬁne nonempty improvement as\nthe improvement restricted to theorems for which some output has nonzero improvement.µlen(y0)\n\nIntuitively, improvement is the expected improvement in metric score from the input to output, ac-\ncounting for errors in the generation.The nonempty improvement score is the expected improvement\nin metric score, given that there are no errors in the generation.Similar improvement scores can be\ndeﬁned for other metrics using a binary function of the metric assigned to the original and optimized\nproofs.Additionally, the accuracy is the percentage of theorems in the dataset which the model was able to\ngenerate a correct output for.The improved accuracy is the percentage of theorems in the dataset\nwhich the model was able to generate a correct output for, as well as improve the metric to be\nnonzero.4.1.1 ABLATIONS\n\nWhen performing our ablation studies, we used a ﬁxed dataset (MIL) and metric (length) and varied\nthe parameters of all the features to ﬁnd the optimal combination.However, as there are over 8640\npossible combinations, it is inefﬁcient to test all combinations at once.As such, we evaluate using a\nfactorial testing method. Testing Groups.We deﬁne the following testing groups with the speciﬁed parameter combina-\ntions:\n\nGPT-4o-mini/GPT-4o: This varies the GPT-4o model, outputting a string with no other features.Output and CoS: We evaluate the effects of different output formatting styles (string, string\nlist, string tree) and CoS (True, False), with the model ﬁxed as GPT-4o, with no other\nfeatures enabled.Example Retrieval: We evaluate the effects of increasing the number of examples provided (multi-\nshot prompting) in the range of 0, 3, 5, 7, and 10, with the model ﬁxed as GPT-4o, CoS and output\nformatting ﬁxed as the best combination from the previous test, and no other features enabled.Sampling Method: Here, we evaluate the effects of best-of-n and reﬁnement for a ﬁxed n = 5.Additionally we test on the reﬁnement cases if forwarding the most recent iteration result, or all\nprevious iteration results is the best, and if we should keep the best out of the iterations, or the most\n\n6\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 1: Average Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nMetric\n\nModel\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n3.7\n20.96\n\n2.21\n9.34\n\n6.25\n30.54\n\n4.18\n13.45\n\n15.15\n55.29\n\n8.02\n30.53\n\n18.58\n56.56\n\n14.48\n30.97\n\n26.36%\n100.0%\n\n18.75%\n100.0%\n\n8.31%\n35.44%\n\n6.13 %\n24.56%\n\n37.5%\n100.0%\n\n28.85%\n100.0%\n\n14.42%\n50.0%\n\n11.54%\n34.21%\n\nTable 2: MIL Proof optimization results.Improvement Nonempty Improvement Accuracy\n\nImproved Acc. recent.The model is ﬁxed as GPT-4o, CoS, output formatting, and examples are ﬁxed as the best\ncombination from the previous test, and no other features enabled.n and Model: Here, we evaluate the effects of larger n values and different models.We test n =\n3, 5, 7, 10, 15 on GPT-4o and GPT-4o-mini, as well as n = 20 for GPT-4o-mini (as it is of a far\nlower token cost).CoS, output formatting, examples, and sampling method are ﬁxed as the best\ncombination from the previous test, and no other features enabled.Combos and RAG: We evaluate combination methods refinement(best_of_m',m) and\nbest_of_m'(refinement(m)), for m 6= m′ with mm′ equal to the optimal value m from\nthe previous test.We also test the effect of enabling document retrieval.Model, CoS, output for-\nmatting, examples, n, and sampling method are ﬁxed as the best combination from the previous\ntest.Ablation data. We evaluate our ablations on a subset of MIL.However, due to the increase in\nmodel calls for larger n values, we switch a representative sample of this subset for some test groups.Namely,\n\nGPT-4o-mini, GPT-4o, Output and Cos, Example Retrieval, and Sampling Method are tested\non the 133 theorems in the solutions of C03_Logic, C04_Sets_and_Functions, and\nC05_Elementary_Number_Theory.n and Model are tested on 55 theorems from a representative sample of the aforementioned, and\nCombos and RAG are tested on a representative sample of 32 theorems from the aforementioned.4.2 RESULTS\n\nImProver is capable of optimizing proofs in all settings.From Table 2, Table 3, and Table 4,\nwe can see that ImProver is capable of optimizing proofs on all datasets for both the length and\nreadability metrics.Furthermore, Table 1 shows that across all metrics, ImProver signiﬁcantly\noutperforms GPT-4o on proof optimization tasks on every experimental measure – aggregated from\nall datasets.Additionally, from Table 2, Table 3, and Table 4, we can see that ImProver outperforms\nGPT-4o on each dataset as well.We proceed to analyze this data and its implications. Length optimization.First focusing on the length metric, we see that ImProver outperforms GPT-\n4o with respect to the improvement score by 566% (aggregated over all datasets).Additionally, we\nare guaranteed that ImProver produces a correct output, although that output may just be the same\nas the input.However, 35.44% of the time, it generates a correct output that is not the same length\nas the input, and in that case, we expect an average of a 55.29% reduction in length.Comparing\nthis with GPT-4o, we conclude that not only can ImProver optimize at a higher level on arbitrary\ntheorems, but its ability to generate nontrivial correct outputs is far greater in comparison to GPT-4o.7\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 3: Compﬁles Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n2.75\n18.86\n\n0.39\n5.74\n\n0.0\n6.19\n\n0.0\n4.63\n\n30.7\n54.48\n\n3.38\n24.89\n\n0.0\n53.65\n\n0.0\n33.19\n\n11.54%\n100.0%\n\n14.1%\n100.0%\n\n5.13%\n34.62%\n\n1.28%\n19.23%\n\n16.67%\n100.0%\n\n4.65%\n100.0%\n\n0.0%\n11.54%\n\n0.0%\n11.63%\n\nMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 4: Mathlib Proof optimization results. Readability optimization.Readability optimization is similar, with ImProver outperforming\nGPT-4o by 423%.Moreover, the accuracy, improved accuracy, and nonempty improvement dis-\nparities for readability parallel those of the length tests.This suggests that for both models, it was generally more “difﬁcult”\nto generate a correct output, and moreover, generate a correct output with a better metric score\nthan the input, for readability optimization than length optimization.In other words, optimizing for\nreadability is more difﬁcult for the underlying generator than optimizing for length.However, we\nspeculate with higher-quality prompts, descriptions of the metric, and examples, this disparity can\nbe minimized.Regardless, we note that different metrics can be less likely to be correctly optimized,\nand that model performance is correlated with the metric it seeks to optimize – both for GPT-4o and\nImProver.Optimization varies based on dataset difﬁculty.Additionally noting Table 2, Table 3, and\nTable 4, we observe that the improvement score for both metrics for both GPT-4o and ImProver\nis highest for the MIL dataset, lower for Compﬁles, and the lowest on the Mathlib theorems.This\nsuggests that the expected improvement in metric score decreases with higher difﬁcultly – with\nundergraduate-level theorems having a signiﬁcantly higher expected improvement than research-\nlevel theorems.However, it should be noted that for both metrics, the nonempty improvement of\nImProver stayed consistent, whereas for GPT-4o, it followed the aforementioned trend of decreas-\ning with difﬁculty.Similarly, the accuracy and improved accuracy scores for both metrics and\nmodels decreased with higher difﬁculty datasets (disregarding ImProver’s accuracy scores, as they\nare ensured to be 100%).This suggests that although the base GPT-4o generator is less likely to\ngenerate a correct output for higher difﬁculty datasets, the improvements that ImProver makes to\nthe base generator allows it to maintain its improvement in the metric score whenever a correct out-\nput is generated.As such, we can speculate that the bottleneck in the improvement score is not the\nmodel’s ability to optimize the proof for a metric, but rather its ability to generate a new correct\nproof at all.As such, we conjecture that with more capable generator models, the accuracy – and\nthus, the improvement score – in optimization tasks will continue to increase, until the improvement\nscores match the nonempty improvement.Overall, we conclude that although the performance of both ImProver and GPT-4o decreases on\nlength and readability optimization on more difﬁcult datasets, ImProver signiﬁcantly outperforms\nGPT-4o on all datasets for length and readability optimization.4.2.1 ABLATION TESTING\n\nWe perform ablation studies using a subset of the MIL dataset as discussed in §4.1.1.The results\nof this factorial study are aggregated in Table 5.We measure the baseline results from the GPT-\n4o and GPT-4o-mini models, noting that GPT-4o is the better-scoring model (with respect to the\nimprovement score).Thus, ﬁxing this model, we vary the output formatting type and if CoS is\nenabled, and determine that outputting flat with CoS enabled maximizes the improvement score.8\n\n\fTable 5: Ablation results.Each cell in the ablation tests shows best / worst, which are the best\nand worst parameter combinations in the test group.The ImProver speciﬁcation outputs the input\ntheorem when no correct proof is generated, which results in an accuracy of 100% on MIL.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o-mini\nGPT-4o\n+ Output and CoS\n+ Example Retrieval\n+ Sampling Method\n+ n and Model\n+ Combos and RAG\nImProver\n\n0\n7.03\n8.04 / 6.31\n9.34 / 5.67\n15.35 / 9.34\n23.51 / 3.65\n34.88 / 28.25\n\n0\n19.67\n12.38 / 14.17\n14.7 / 8.44\n18.44 / 14.7\n26.28 / 4.63\n57.56 / 33.48\n\n3.62%\n35.77%\n\n0%\n15.33%\n\n64.96% / 44.53% 21.17% / 16.06%\n63.5% / 67.15%\n21.9% / 16.79%\n36.5% / 21.9%\n83.21% / 63.5%\n89.47% / 78.95% 45.61% / 8.77%\n60.61% / 84.38% 54.55% / 53.12%\n\n34.88\n\n57.56\n\n100%\n\n54.55%\n\nTable 6: CoS Readability Ablation results.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o\nImProver, CoS Disabled\nImProver\n\n4.97\n9.23\n16.69\n\n15.89\n24.61\n31.42\n\n37.5%\n100.0%\n100.0%\n\n12.5%\n28.12%\n46.88%\n\nFixing these parameters, we now vary the number of examples retrieved, noting that prompting\nwith 10 examples maximizes the improvement score.Fixing this parameter, we vary the sampling\nmethods (excluding compound methods and ﬁxing n = 5) and observe that best-of-n is the best\nparameter combination.Now, as GPT-4o-mini is signiﬁcantly less computationally expensive than\nits GPT-4o counterpart, we test both models with the sample method ﬁxed to best-of-n, and vary\nn = 1, 3, 5, 7, 10, 15, and for GPT-4o-mini, also n = 20.We conclude that GPT-4o with n = 15\nis the most effective.Fixing these parameters, we consider all mixed compound sampling methods\nwith and without document retrieval enabled, concluding that a 5-step reﬁnement with best-of-3 on\neach iteration, with RAG enabled, is the optimal combination.Thus, as we can see from Table 5, the optimal parameter combination comes from gpt-4o outputting\nas a string list with CoS, RAG, 10 examples, 5-step reﬁnement with each iteration being a\nbest-of-3 evaluation.Changing any one of these parameters them leads to a reduction in performance.Additional ablation data can be found at (§B.1).Readability and Chain-of-States (CoS) Ablation.We additionally examine the effects of dis-\nabling CoS on readability optimization tasks, as the previous study focused on length optimization\ntasks, and we speculate that CoS has a high impact on the performance of readability optimization\ntasks, as the proof states that are embedded due to CoS seem to be a critical aspect to generating the\nexplicit declarations that the readability metric measures.We conﬁrm this result by considering Table 6 and observe that simply enabling CoS nearly doubles\nthe improvement score, and signiﬁcantly improves the nonempty improvement score, suggesting that\nCoS has a high degree of impact on optimizing for the readability metric, as conjectured.However,\nwe also note a signiﬁcant increase in improved accuracy, which suggests that embedding the chain\nof states also improves the ability of the model to generate nontrivial correct outputs, implying that\nthe symbolic information contained in the states are critical to effectively modifying the structure\nand content of a proof.4.2.2 NEURAL THEOREM PROVING EVALUATION\n\nWe evaluate ImProver’s neural theorem proving (NTP) performance using the completion metric.We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input\nproof.Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-4o.ImProver substantially outperforms GPT-4o across all topics, with an 80% increase in accuracy\ncompared to the base model, showing that proof optimization systems are easily extendable to NTP\nsystems.9\n\n\fTable 7: Proof generation results.Each cell shows percent accuracy.MIL\n\nSet Theory Group Theory\n\nOverall\n\nGPT-4o\nImProver\n\n18.18%\n45.45%\n\n25%\n33.33%\n\n21.73%\n39.13%\n\n4.3 QUALITATIVE RESULTS\n\nNext, we discuss qualitative examples showing the improvements from ImProver in proof optimiza-\ntion.These examples show the balance between correctness and the desired optimization metric,\nshowing how ImProver can produce more concise or readable proofs depending on the use case.Additional examples can be found in (§B.2)\n\nExample 1: Compﬁles Optimization.Consider Figure 1, a lemma from the 2022 IMO Question\n2 (Compﬁles) that we optimize for length.The original proof consisted of 12 tactic steps and multiple intermediate calculations.After applying\nImProver for length optimization, the proof was halved to only 6 tactics, eliminating redundant steps\nwhile maintaining correctness.In comparison, the shortened proof makes multiple nontrivial optimizations, such as eliminating the\nh2’ and h4 and hxw hypotheses, as well as fully generating proof terms for speciﬁc rewrites and\nother tactics.Example 2: MIL. Consider Figure 3, a result from MIL that we optimize for readability.Original (human-written)\n\nImProver (readability-optimized)\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ncard G = card H * card K)\n\n: K ≃* G / H := by\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\n· rw [← ker_eq_bot_iff, (QuotientGroup.mk'\n\nH).ker_restrict K]\nsimp [h]\n\n· symm\n\nexact aux_card_eq h'\n\ncard G = card H * card K)\n:= by\n\n: K ≃* G / H\n\nhave injectivity : Function.Injective\n\n((QuotientGroup.mk' H).restrict K) := by\nrw [← ker_eq_bot_iff, (QuotientGroup.mk'\nH).ker_restrict K]\nsimp [h]\n\nhave card_eq : card (G / H) = card K := by\n\nexact aux_card_eq h'\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\nexact injectivity\nsymm\nexact card_eq\n\nFigure 3: Optimizing a group-theoretic result from MIL Chapter 8 Section 1 for readability.We\ndeﬁne a proof to be readable if it is written in a declarative style, which consists of intermediate\nconjectures (have · · · statements).ImProver introduces two intermediate conjectures into the proof.This original proof carried a score of 0, as it does not contain any declarative statements.In compar-\nison, after applying ImProver, we transformed the proof to be more declarative, with many more\nintermediate steps with explicit have tactics for improved clarity.Additionally observe how the\nmodel deﬁnes hypotheses for use in the latter half of the proof; these predeﬁned hypotheses could\neasily be converted into standalone lemmas for reuse.Example 3: Full Proof Generation.We analyze the application of ImProver to neural theorem\nproving in the MIL example from Figure 4.This theorem relating to group theory originally has no proof, however, ImProver generates one\nfrom scratch.This generated proof is veriﬁed to be correct by Lean, utilizing all the included hy-\npotheses as well as a retrieved mathlib theorem.10\n\n\fOriginal (human-written)\n\nImProver (completeness-optimized)\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n: S ≤ T) : comap ϕ S ≤ comap ϕ T := by\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n\n: S ≤ T) : comap ϕ S ≤ comap ϕ T\n\n:= by\n\nsorry\n\nintro g\nsimp only [mem_comap]\nintro hS\nexact hST hS\n\nFigure 4: Solving a group theorem exercise from MIL Chapter 8 Section 1 for readability.5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.By incorporating CoS, RAG, and other features, ImProver signiﬁcantly outperforms base\nlanguage models in proof optimization over undergraduate, competition, and research-level prob-\nlems.However, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance\non black-box LLM’s.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.As such, we believe that ImProver sets the stage for further\nwork on proof optimization to advance the study and use of AI in mathematics.ACKNOWLEDGEMENTS\n\nRiyaz Ahuja thanks the L3 Lab and Hoskinson Center for Formal Mathematics for their support.Sean Welleck thanks Convergent Research, the Lean FRO, and the OpenAI Researcher Access Pro-\ngram for their support.11\n\n\fREFERENCES\n\nand\n\nAlphaGeometry\n\nAlphaProof\nstandard\nhttps://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\n2024.\n\nsilver-medal\nproblems.achieves\nolympiad\n\nmathematical\n\ninternational\n\nsolving\n\nTeams.AI\n\nSerge Autexier and Dominik Dietrich.A tactic language for declarative proofs.In Matt Kaufmann\nand Lawrence C. Paulson (eds.), Interactive Theorem Proving, pp.99–114, Berlin, Heidelberg,\n2010. Springer Berlin Heidelberg.Jaime Carbonell and Jade Goldstein.The use of mmr, diversity-based reranking for reordering doc-\numents and producing summaries.In Proceedings of the 21st Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ’98, pp.335–336,\nISBN 1581130155. doi:\nNew York, NY, USA, 1998.Association for Computing Machinery. 10.1145/290941.291025.URL https://doi.org/10.1145/290941.291025.Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.Teaching large language models\n\nto self-debug, 2023.URL https://arxiv.org/abs/2304.05128. David Renshaw. compﬁles.https://github.com/dwrensha/compfiles, 2024.Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun.Baldur: Whole-proof generation and\n\nrepair with large language models, 2023.Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu.Proof artifact co-\nIn International Conference on Learning\n\ntraining for theorem proving with language models.Representations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.Jiewen Hu, Thomas Zhu, and Sean Welleck.minictx: Neural theorem proving with (long-)contexts,\n\n2024.URL https://arxiv.org/abs/2408.03350.Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu.Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs.In The Eleventh International Conference on Learning\nRepresentations, 2023.URL https://openreview.net/forum?id=SMa9EAovKMC.Kim Morrison. lean-training-data.https://github.com/kim-em/lean-training-data,\n\nGuillaume Lample, Timothee Lacroix, Marie anne Lachaux, Aurelien Rodriguez, Amaury\nHypertree proof search\nHayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.for neural\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=J4pX8Q8cxHH. theorem proving. 2024. 2024.Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie\n\nSi.A survey on deep learning for theorem proving, 2024.Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang.A survey of deep\nlearning for mathematical reasoning.In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp.14605–14631, Toronto, Canada, July 2023.Association for Computational Linguistics.10.18653/v1/2023.acl-long.817.URL\nhttps://aclanthology.org/2023.acl-long.817.doi:\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-reﬁne:\nIterative reﬁnement with self-feedback.In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.URL https://openreview.net/forum?id=S37hOerQLB.12\n\nleanprover-community. mathematics_in_lean.https://github.com/leanprover-community/mathematics_in_lea\n\n\fThe mathlib Community.The lean mathematical library.In Proceedings of the 9th ACM SIGPLAN\nInternational Conference on Certiﬁed Programs and Proofs, POPL ’20.ACM, January 2020. doi:\n10.1145/3372885.3373824.URL http://dx.doi.org/10.1145/3372885.3373824.Leonardo de Moura and Sebastian Ullrich.The lean 4 theorem prover and programming lan-\nIn Automated Deduction – CADE 28: 28th International Conference on Automated\nguage.Deduction, Virtual Event, July 12–15, 2021, Proceedings, pp.625–635, Berlin, Heidelberg,\n2021. Springer-Verlag.ISBN 978-3-030-79875-8. doi: 10.1007/978-3-030-79876-5_37.URL\nhttps://doi.org/10.1007/978-3-030-79876-5_37.OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red\nAvila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Moham-\nmad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher\nBerner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brit-\ntany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek\nChen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,\nHyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas\nDegry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,\nAdrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada\nFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel,\nTarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff\nHarris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jo-\nmoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,\nNitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik\nKim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, An-\ndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,\nStephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim\nMalfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew\nMayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil,\nDavid Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin,\nVinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeon-\nwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley\nPantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov,\nAndrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde\nde Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea\nPower, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick\nRyder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David\nSchnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah\nShoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama,\nIan Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie\nSummers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin\nTootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón\nUribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welin-\nder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,\nHannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah\nYoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,\nShengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.Gpt-4 technical\nreport, 2024. URL https://arxiv.org/abs/2303.08774.Stanislas Polu and Ilya Sutskever.Generative language modeling for automated theorem proving,\n\n2020.13\n\n\fStanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\n\nSutskever.Formal mathematics statement curriculum learning, 2022.Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, and Swarat Chaudhuri.An in-\n\ncontext learning agent for formal theorem-proving, 2024.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H.\nthought prompting elicits reasoning\nChi, Quoc V Le, and Denny Zhou.in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=_VjQlMeSB_J.Chain of\n\nFreek Wiedijk. Formal proof sketches.In Stefano Berardi, Mario Coppo, and Ferruccio Damiani\n(eds.), Types for Proofs and Programs, pp.378–393, Berlin, Heidelberg, 2004.Springer Berlin\nHeidelberg. ISBN 978-3-540-24849-1.Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil,\nRyan Prenger, and Anima Anandkumar.LeanDojo: Theorem proving with retrieval-augmented\nlanguage models.In Neural Information Processing Systems (NeurIPS), 2023.Docprompting: Gener-\nating code by retrieving the docs.In The Eleventh International Conference on Learning Repre-\nsentations, 2023.URL https://openreview.net/forum?id=ZTCxT2t2Ru.14\n\n\fA PROMPTS\n\nA.1 TEMPLATE\n\nIn this appendix, we note the prompts used by ImProver both for general LLM prompting, as well\nas the metric-speciﬁc prompts.For the main prompt sent to the LLM on each sample, we build a prompt string using a chat prompt\ntemplate that is then invoked at runtime to ﬁll in the variables.Namely, these variables include the set of metric prompts, previous results, input theorem, context,\na syntax documents, Mathlib documents, and examples.The prompt template is a conversation of the format:\n\nPlaceholder: All metric prompts with a ‘System’ role\nSystem: You will be given the proof context (i.e.the lean ﬁle contents/imports leading up\nto the theorem declaration) wrapped by <CONTEXT>...</CONTEXT>.You will be given the previous num_prev input/output pairs as well as their metric (met-\nric.name) score and correctness score, as well as any error messages, for your reference to\nimprove upon.Each of these previous results will be wrapped with <PREV I=0></PREV\nI=0>,...,<PREV I=num_prev-1></PREV I=num_prev-1>, with I=num_prev-1 being the\nmost recent result.Remember to use lean 4 syntax, which has signiﬁcant changes from the lean 3 syntax.To\nassist with the syntax relating to the current theorem and current error messages, you will\nbe given num_syntax_docs documents to refer to for ﬁxing these syntax issues.Each of\nthese documents will be wrapped with <SYNTAX_DOC>...</SYNTAX_DOC>.You will also receive num_mathlib_docs documents relevant to the current theorem to\nhelp with formulating your modiﬁed proof.Each of these will be wrapped with <CON-\nTENT_DOC>...</CONTENT_DOC>\nYou will also receive num_examples examples of input-output pairs of proofs that\nwere optimized for the metric metric.Each of these will be wrapped with <EXAM-\nPLE>...</EXAMPLE>\nYou will be given the tactic states as comments for reference.The current theorem will be\nwrapped in <CURRENT>...</CURRENT>\nSystem: Output format instructions\nPlaceholder: All retrieved syntax documentation\nPlaceholder: All retrieved mathlib documentation\nPlaceholder: All retrieved examples\nUser: <CONTEXT> context </CONTEXT>\nPlaceholder: Previous results and inputs/outputs\nPlaceholder: All metric prompts with a ‘User’ role\nUser: <CURRENT> theorem </CURRENT>\n\nThis prompt is then invoked and sent to the language model by ﬁlling in all the variables and place-\nholders.Notably, when we invoke the chain given by chain|llm|parser, we throttle the invoca-\ntion with a randomized exponential rate limit throttling to account for API rate limits, especially in\nhighly-parallelized requests like when benchmarking over a large number of theorems.A.2 METRIC PROMPTS\n\nLength Metric\n\nSystem: You are an AI assistant who shortens Lean 4 proofs while ensuring their correct-\nness.You will aim to reduce the number of lines of the tactic proof while ensuring that it\nproperly compiles in Lean 4.15\n\n\fUser: Shorten the current theorem (wrapped in <CURRENT>...</CURRENT>) to be as\nshort in length—measured in the number of lines of the proof—as possible, while also\nensuring that the output is still syntactically correct.\"Readability Metric\n\nCompletion Metric\n\nSystem: You are an AI assistant who rewrites Lean 4 proofs to be more readable while\nensuring their correctness.We measure readablity by considering the ratio of the number\nofexplicitly typed have tactics against the total number of tactics in the proof, as this is\nproportional to whether a proof is declarative in style, and thus, readable.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is\nmore readable and declarative and modular.System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.You will receive a Lean 4 proof you must\nmodify to eliminate any errors so that it compiles correctly and eliminate any “sorry”s with\nfull proofs.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a\nformal, complete, and correct Lean 4 proof by ﬁlling in its tactic proof.B ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this section, we provide more detailed information on the experimental setup and results used to\nevaluate ImProver.B.1 ABLATION DETAILS\n\nWe now proceed to show detailed results from our ablation testing.Table 8: Output and Chain-of-States Ablations\n\nOutput Format\n\nCoS\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.string\nTrue\nstring\nFalse\nstring list True\nstring list\nFalse\nstring tree\nTrue\nstring tree\nFalse\n\n7.53\n7.03\n8.04\n7.04\n7.62\n6.31\n\n16.12\n19.67\n12.38\n13.58\n15.34\n14.17\n\n46.72%\n35.77%\n64.96%\n51.82%\n49.64%\n44.53%\n\n16.79%\n15.33%\n21.17%\n18.98%\n18.25%\n16.06%\n\nBy Table 8, we see that the optimal combination in this testing group is a string list output\nformat with CoS enabled.Fix these values for all future tests.Examples\n\nTable 9: Example Retrieval Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.0\n3\n5\n7\n10\n\n5.67\n8.49\n8.38\n7.56\n9.34\n\n67.15%\n62.04%\n64.96%\n62.77%\n63.5%\n\n16.79%\n19.71%\n21.17%\n19.71%\n21.9%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the number of examples.By Table 9,\nwe see that the optimal combination in this testing group is 10 examples.Fix this value for all future\ntests.8.44\n13.68\n12.9\n12.04\n14.7\n\n16\n\n\fTable 10: Sampling Method Ablations\n\nMethod\n\nForward Keep Best\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.None\nreﬁnement\nreﬁnement\nreﬁnement\nreﬁnement\nbest-of-n\n\nN/A\n1\n5\n1\n5\nN/A\n\nN/A\nFalse\nFalse\nTrue\nTrue\nN/A\n\n9.34\n14.76\n12.5\n14.95\n13.15\n15.35\n\n14.7\n30.63\n20.88\n14.95\n13.15\n18.44\n\n63.5%\n48.18%\n59.85%\n100.0%\n100.0%\n83.21%\n\n21.9%\n30.66%\n30.66%\n30.66%\n29.93%\n36.5%\n\nNote that forward and keep-best values are parameters for reﬁnement of how many previous itera-\ntions to forward, and whether to keep the most recent or the best iteration in subsequent reﬁnement\nsteps.Now, with the previous optimal parameters ﬁxed, run the ablation on the sample method.By\nTable 10, we see that the optimal combination in this testing group is best-of-n.Fix this value\nfor all future tests.Table 11: Model and n Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.Model\n\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\n\nn\n\n3\n5\n7\n10\n15\n3\n5\n7\n10\n15\n20\n\n19.66\n20.12\n22.44\n21.73\n23.51\n3.65\n5.12\n3.65\n4.99\n4.35\n4.87\n\n24.36\n24.97\n27.21\n25.28\n26.28\n4.63\n6.21\n4.34\n5.69\n5.06\n5.56\n\n80.7%\n80.56%\n82.46%\n85.96%\n89.47%\n78.95%\n82.46%\n84.21%\n87.72%\n85.96%\n87.72%\n\n38.6%\n36.11%\n42.11%\n40.35%\n45.61%\n8.77%\n10.53%\n8.77%\n12.28%\n12.28%\n14.04%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the value of n and model.By\nTable 11, we see that the optimal combination in this testing group is GPT-4o with n = 15.Fix this\nvalue for all future tests.Table 12: RAG and Combination Sampling Method Ablations\n\nRAG Improvement Nonempty Improve. Accuracy\n\nImproved Acc.Combination\n\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nbest-of-n\nbest-of-n\n\nm\n\nm′\n\n5\n3\n5\n3\n3\n5\n3\n5\n5\n3\n5\n3\n3\n5\n5\n3\nN/A 15\nN/A 15\n\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n33.78\n31.23\n31.85\n31.35\n51.32\n50.1\n57.56\n49.75\n32.71\n33.48\n\n100.0%\n100.0%\n100.0%\n100.0%\n63.64%\n65.62%\n60.61%\n59.38%\n90.62%\n84.38%\n\n50.0%\n46.88%\n50.0%\n50.0%\n48.48%\n53.12%\n54.55%\n43.75%\n56.25%\n53.12%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the combination methods and if\nRAG is enabled.By Table 12, we see that the optimal combination in this testing group is a 5-step\nreﬁnement with each iteration being a best-of-3 call, with RAG enabled.B.2 ADDITIONAL QUALITATIVE EXAMPLES\n\nIn this section, we provide qualitative examples demonstrating the improvements ImProver achieves\nin proof optimization.33.78\n31.23\n31.85\n31.35\n32.66\n32.88\n34.88\n29.54\n29.64\n28.25\n\n17\n\n\fCompﬁles: Length Optimization See (§4.3)\n\nCompﬁles: Readability Optimization Consider Figure 5, in which a lemma from the 2019 IMO\nproblem 1 (from the Compﬁles dataset) is optimized for readability.This introduces multiple new\nhypotheses, which generalize a linear_property of the functions, and then reuses and instanti-\nates that (and others, too) hypothesis throughout the proof, creating a signiﬁcantly more declaritive,\nmodular, and therefore readable proof.Original (human-written)\n\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\nrefine hf 1, fun a => ?_i\nchange g a = g 1 * a\nrw [mul_comm, ← smul_eq_mul, ←\n\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nImProver (readability-optimized)\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\n\nhave linear_property : ∀ a, f a = g a := by\n\nintro a\nrfl\n\nhave g_smul : ∀ a, g a = g 1 * a := by\n\nintro a\nrw [mul_comm, ← smul_eq_mul, ←\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nrefine hf 1, fun a => ?_i\nhave f_eq_g : f a = g a := linear_property a\nhave g_a_eq : g a = g 1 * a := g_smul a\nrw [f_eq_g, linear_property 1, g_a_eq]\n\nFigure 5: Optimizing a lemma from IMO 2019 P1 for readability\n\nMIL: Length Optimization Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, eliminating simp calls and introducing proof terms\ninto the structure of the proof to shorten it from 9 tactic invocations to 7.Original (human-written)\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\n\n:= by\n\next x\nsimp only [mem_map]\nconstructor\n· rintro hy, y_in, hyi\n\nexact hϕ y, hy, y_in, rfli, hyi\n\n· rintro hy, hz, z_in, hzi, hyi\n\nuse z, z_in\ncalc ψ.comp ϕ z = ψ (ϕ z) := rfl\n_\n= ψ y := by congr\n\nImProver (length-optimized)\n\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\nby\next x\nsimp only [mem_map]\nconstructor\nrintro hy, y_in, hyi; exact hϕ y, hy, y_in,\n\n:=\n\nrfli, hyi\n\nrintro hy, hz, z_in, hzi, hyi; exact hz,\nz_in, (congr_arg ψ hz).trans hyi\n\nFigure 6: Optimizing a lemma from the solutions of MIL CH08 S01 for length\n\nMIL: Length Optimization 2 Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, converting a full tactic proof into a single proof term\nto shorten it from 28 tactic invocations to 1.Note that the model does not have access to the Lean\ncommands that symbolically generate proof terms, and therefore generates and estimates the proof\nterm entirely by itself.MIL: Readability Optimization See (§4.3)\n\nMathlib: Length Optimization Consider Figure 8, which optimizes a theorem in algebraic topol-\nogy from mathlib for length, eliminating simp calls and combining tactics to shorten it from 3 tactic\ninvocations to 1.Mathlib: Readability Optimization Consider Figure 9, a theorem from Mathlib that we optimize\nfor readability.This original proof carried a score of 0, as it does not contain any declarative statements.It is concise\nand efﬁcient, however, it is difﬁcult to understand and read.18\n\n\fOriginal (human-written)\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\next x; constructor\n· rintro (hxs, xnti | hxt, xnsi)\n\nImProver (length-optimized)\n\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\nexact Set.ext fun x => hfun h => h.elim\n\n(fun hxs, xnti => hOr.inl xs, fun h_, xti =\n> xnt xti) (fun hxt, xnsi => hOr.inr xt,\nfun hxs, _i => xns xsi),\n\nfun hh, nxsti => h.elim (fun xs => Or.inl h\nxs, fun xt => nxst hxs, xtii) (fun xt =>\nOr.inr hxt, fun xs => nxst hxs, xtii)i\n\n· constructor\n\nleft\nexact xs\nrintro h_, xti\ncontradiction\n\n.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.This intermediate tactic greatly helps in the understandability and clarity of the proof.Original (human-written)\n\nImProver (readability-optimized)\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n:=\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]; rfl\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\n:=\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nhave h1 : ∀ x y : Y, Subsingleton\n\n(Path.Homotopic.Quotient x y) ↔ ∀ {p1 p2\n: Path x y}, Path.Homotopic p1 p2 := by\nintro x y\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]\nrfl\n\nsimp only [h1]\n\nFigure 9: Optimizing a theorem from Mathlib/FundamentalGroupoid/SimplyConnected\nfor readability\n\n19",
          "time": 569.146281003952,
          "input_token_size": 16874,
          "output_token_size": 52,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_8e9232eb",
        "created_at": 1749627211.674018,
        "processing_time": 571.9478521347046,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "9cb168b2": {
      "question": "Who is the designed agent-based tool mentioned in the document?",
      "answer": "ImProver",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            20,
            21,
            22
          ],
          "provenance_ids": [
            21
          ],
          "provenance": "1\n\n\fWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more readable, and more declarative in style.1\n\nOriginal (human-written)\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\nImProver (length-optimized)\n\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\n(h1 : ∀ x, ∃!",
          "time": 4.65130352973938,
          "input_token_size": 16496,
          "output_token_size": 21,
          "original_provenance_id": 0
        },
        {
          "provenance_id": 1,
          "input_sentence_ids": [
            5,
            6,
            7
          ],
          "provenance_ids": [
            7
          ],
          "provenance": "We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.",
          "time": 7.777803421020508,
          "input_token_size": 17960,
          "output_token_size": 33,
          "original_provenance_id": 1
        },
        {
          "provenance_id": 2,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4
          ],
          "provenance_ids": [
            1,
            2,
            4
          ],
          "provenance": "0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.",
          "time": 9.64113187789917,
          "input_token_size": 18215,
          "output_token_size": 36,
          "original_provenance_id": 2
        },
        {
          "provenance_id": 3,
          "input_sentence_ids": [
            32,
            33,
            34
          ],
          "provenance_ids": [
            33
          ],
          "provenance": "Our model,\nImProver, builds on neural theorem proving techniques including full proof generation (Jiang et al.,\n2023; First et al., 2023), conditioning on example proofs (Jiang et al., 2023), retrieval (Yang et al.,\n2023; Thakur et al., 2024), and preceding ﬁle context (First et al., 2023; Hu et al., 2024), as well as\nerror correction (Madaan et al., 2023; Chen et al., 2023) and documentation retrieval (Zhou et al.,\n2023) from code generation.",
          "time": 13.445059061050415,
          "input_token_size": 20645,
          "output_token_size": 48,
          "original_provenance_id": 3
        },
        {
          "provenance_id": 4,
          "input_sentence_ids": [
            23,
            24,
            25
          ],
          "provenance_ids": [
            23
          ],
          "provenance": "y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nhave h2' : Classical.choose (h1 x).exists =\n\ny :=\nh1u _ (Classical.choose_spec (h1\nx).exists)\n\nrw [h2']\nobtain hw, h1e', h1u'i := h1 y\nhave h4 := Classical.choose_spec (h1\n\nhave hxw : x = w := by\n\ny).exists\n\napply h1u'\nrw [h2]\nexact h1e\n\nrw [hxw]\nexact h1u' _ h4\n\nFigure 1: ImProver automatically rewrites formal proofs to optimize a criterion such as length\nor readability while remaining correct.",
          "time": 15.88467264175415,
          "input_token_size": 21701,
          "output_token_size": 56,
          "original_provenance_id": 4
        }
      ],
      "metadata": {
        "question_id": "paper_9cb168b2",
        "created_at": 1750198575.555697,
        "processing_time": 18.5965576171875,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 17,
        "provenance_count": 5,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 17,
            "provenance_count": 5,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "9d3612f1": {
      "question": "What kind of tasks can be difficult due to formal proofs' concise nature?",
      "answer": "1. Understanding and interpreting formal proofs.\n2. Modifying or refactoring proofs for different styles or readability.\n3. Ensuring correctness while optimizing for length or readability.\n4. Learning from formal proofs due to their concise nature.\n5. Reusability of formal proofs in different contexts.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            132,
            134,
            135,
            136,
            137,
            138,
            140,
            141,
            143,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            171,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            191,
            192,
            193,
            194,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            213,
            215,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            252,
            253,
            254,
            255,
            256,
            257,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            293,
            294,
            295,
            296,
            297,
            298,
            301,
            302,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance": "4\n2\n0\n2\n \nt\nc\nO\n \n7\n \n \n]\nI\n\nA\n. s\nc\n[\n \n \n1\nv\n3\n5\n7\n4\n0\n.0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.To this end, we study a new problem of automated proof\noptimization: rewriting a proof so that it is correct and optimizes for an arbitrary\ncriterion, such as length or readability.As a ﬁrst method for automated proof opti-\nmization, we present ImProver, a large-language-model agent that rewrites proofs\nto optimize arbitrary user-deﬁned metrics in Lean.We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.In\npractice, however, informal, natural-language proofs are prone to imprecision, ambiguity, and error.Using a formal language such as Lean (Moura & Ullrich, 2021) removes ambiguity and precision\nand enables a proof assistant to verify correctness down to the primitives of a formal axiomatic\nsystem.Formal proofs, however, can be hard to read and often suffer from low reusability or excessive detail.For example, formal proofs in Lean’s extensive mathematical library, Mathlib (mathlib Community,\n2020), are generally designed to be concise and very general, often at the expense of readability.Formal proofs in an expository text, in contrast, may include detailed calculations steps, making\nthem readable but verbose.Machine learning systems trained on such proofs learn to mimic these\nvaried conventions (Hu et al., 2024), which may not be the optimal use of the limited supply of\nhuman-written proofs.As a result, we would like to be able to automatically refactor proofs to meet\na secondary objective such as length or readability.To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is\ncorrect and optimizes a criterion such as length or readability.We ﬁnd that naively applying LLMs\nto proof optimization falls short, often resulting in incorrect or poorly optimized proofs.We develop\nvarious improvements that can be applied on top of a black-box language model, including Chain-\nof-States prompting–an analogy to chain-of-thought prompting (Wei et al., 2022) that shows inter-\nmediate proof states–along with error-correction and retrieval.We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.1\n\n\fWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more readable, and more declarative in style.1\n\nOriginal (human-written)\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\nImProver (length-optimized)\n\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nrw [h1u _ (Classical.choose_spec _)]\nobtain hw, h1e', h1u'i := h1 y\nrw [h1u' _ ((h2 _ _).mpr h1e)]\nexact h1u' _ (Classical.choose_spec _)\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nhave h2' : Classical.choose (h1 x).exists =\n\ny :=\nh1u _ (Classical.choose_spec (h1\nx).exists)\n\nrw [h2']\nobtain hw, h1e', h1u'i := h1 y\nhave h4 := Classical.choose_spec (h1\n\nhave hxw : x = w := by\n\ny).exists\n\napply h1u'\nrw [h2]\nexact h1e\n\nrw [hxw]\nexact h1u' _ h4\n\nFigure 1: ImProver automatically rewrites formal proofs to optimize a criterion such as length\nor readability while remaining correct.In this example, ImProver optimizes a human-written\nlemma (right) from the 2022 International Math Olympiad (Question 2, solution from Comp-\nﬁles (David Renshaw, 2024)) for length.ImProver’s optimized proof is correct and more concise.2 RELATED WORK\n\nRecently there has been wide interest in automating theorem proving in interactive proof assistants;\nsee (Lu et al., 2023; Li et al., 2024) for surveys.A typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs\nsuch as Lean’s Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024).A model learns from the distribution of proofs in the cor-\npus, such as Mathlib-style proofs.Recently, the AlphaProof (AlphaProof & Teams, 2024) system\nwas shown to produce proofs with an arcane, non-human structure and syntax.We consider the\nnew problem of rewriting a proof to optimize a metric, such as rewriting a proof into a more read-\nable or more concise one.Proof optimization is more general than theorem proving, since we can\nalso rewrite an empty proof to optimize correctness.Finally, there is a rich literature on the var-\nied styles of (human) formal proofs (e.g., (Autexier & Dietrich, 2010; Wiedijk, 2004)).Our model,\nImProver, builds on neural theorem proving techniques including full proof generation (Jiang et al.,\n2023; First et al., 2023), conditioning on example proofs (Jiang et al., 2023), retrieval (Yang et al.,\n2023; Thakur et al., 2024), and preceding ﬁle context (First et al., 2023; Hu et al., 2024), as well as\nerror correction (Madaan et al., 2023; Chen et al., 2023) and documentation retrieval (Zhou et al.,\n2023) from code generation.ImProver brings these code generation techniques, along with new\nChain-of-States prompting and meta-programmed contextual information, into a uniﬁed proof opti-\nmization agent.3 AUTOMATED PROOF OPTIMIZATION WITH ImProver\n\nAutomated Proof Optimization.Given a theorem statement x, additional context c, and an initial\nproof y0, proof optimization consists of generating a new proof y that is correct and minimizes (or\nmaximizes) a metric µ(x, c, y0, y) → R.\n\n1Code is available at https://github.com/riyazahuja/ImProver.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.Note that shorter proofs often\nrepresent more efﬁcient proofs.Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.Concretely, we evaluate this using the ratio of number of explicitly typed\nhave tactics to total number of tactic invocations.Completion Metric: The completion of a proof simply describes its correctness.This is a trivial\nmetric which measures the number of errors present.The completion metric is used for concretely\nviewing proof optimization as a generalization of neural theorem proving.3.1\n\nIMPROVER\n\nWe develop several improvements that can be applied to a black-box LLM generator yout ∼\nG(·|xin), such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these pa-\nrameters.The explicit prompts and templates that are sent to the LLM can be found in (§A).3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.The intermediate states often contain valuable information (e.g., an expression\nafter it has been simpliﬁed) that is not present in the tactics.To allow the model to reason about\nthese intermediate goals and hypotheses, we use tools from Lean metaprogramming to automatically\nannotate each proof state as a comment prior to each tactic.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.The implementation of this extraction system is mod-\neled from the work (Kim Morrison, 2024).Speciﬁcally, in the compiler’s elaboration and eval-\nuation stages – where the parsed theorem code is ﬁrst converted into concrete syntax trees (in\npractice, Syntax objects) and abstract syntax trees (Expr objects) – we convert the CST and\nAST output objects into the relevant proof data and proof states in the form of proof trees\n(Lean.Elab.InfoTree).These proof trees contain detailed context and information on a tactic-\nby-tactic level relating to the modiﬁcation of the proof state, metavariable context, and proof cor-\nrectness.After state extraction is completed and cached for efﬁcient future access, we annotate the proof text\nitself to contain the intermediate states in the form as comments.Figure 2 shows an example. This explicit reasoning aims to help the generator model construct more optimized proofs via addi-\ntional symbolic data.3.1.2 OUTPUT FORMATTING. LLM outputs often contain ancillary and syntactically invalid content, especially before and after\nthe actual proof.Additionally, by applying additional structure to the LLM outputs, we may hope to\ngenerate more structured proofs.To analyze this hypothesis, we introduce two additional output for-\nmats to the standard str output: flat and structured.The former enforces a tactic sequence\noutput as a list of strings, and the latter enforces a proof tree output as a tree of strings.3.1.3 SAMPLING METHOD\n\nWe also introduce different methods of sampling between many (sequential or parallel) LLM infer-\nence calls, involving best-of-n and iterative reﬁnement implementations, as well as combinations\nthereof.3\n\n\fWithout Chain-of-States\n\nWith Chain-of-States\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u) := by\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u)\n\n:= by\n\nrintro x (hxs, xti | hxs, xui)\n· use xs; left; exact xt\n.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!-/\n. use xs; right; exact xu\n/-\nGoals Solved!-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.More speciﬁcally, our scoring function is given by the 2-ary\ncomparison function S, whose arguments are output objects y, y′.max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.This comparison function can be extended to evaluate the best output of any ﬁnite n via induction.This best-of-n technique is implemented as a curried function such that each of the n calls can be\nhandled by any arbitrary sampling method, or just a single standard prompt at user discretion.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.The reﬁnement process relies on user-deﬁned pa-\nrameters n and prev_num to specify the number of iterations and the number of previous iteration\ninfo to forward, respectively.Each iteration carries information on the last prev_num iterations,\nincluding input, output, metric score, correctness, and error messages.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.Similar to the best-of-n tech-\nnique, it relies on an argument n for the number of reﬁnement steps, and is curried such that each\nreﬁnement step can be handled by any other prompting function.However, unlike best-of-n, there is\nno opportunity for parallelism as each iteration is dependent on information from the previous call.Combination Sampling and Compound Prompt Functions Compound prompt functions utilize\nthe curried nature of the implementations of best-of-n and reﬁnement to nest these techniques within\none another.For example:\n\n4\n\n\fbest_of_n((refinement,m),n) is a compound sampling method that run a best-of-n,\nwhere each call is a m-step reﬁnement.refinement((best_of_n,m),n) is a compound sampling method that runs a n-step reﬁne-\nment, where each call is a best-of-m call to the LLM.Note that with each of these compound prompt functions, there are always a total of mn iterations.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.More speciﬁcally, example retrieval selects the most relevant user-generated examples of proof op-\ntimization on a speciﬁc metric.Namely, each metric is loaded with a cached (vector) database\npopulated with human-made examples of preoptimized and postoptimized pairs of Lean theorems.The number of examples that are retrieved is user-speciﬁed.Document retrieval extracts information using MMR from a pair of ﬁxed (vector) databases.The\ndatabases store semantically chunked data from the Theorem Proving in Lean (TPiL) handbook –\ncontaining syntax guides and tactic explanations – and the Mathlib mathematics libary – containing\nthousands of theorems and lemmas.The chunking is handled by a recursive character splitter, which\nsplits the TPiL markdown ﬁles at on its headers and Mathlib ﬁles at the start of theorems, examples,\nlemmas, and deﬁnitions – with chunk sizes of 1000 characters with a 200 character overlap.The Mathlib retriever ﬁnds the top k documents that score the highest MMR score against the current\ntheorem, the TPiL retriever ﬁnds the top k documents that score the highest MMR score against the\ncurrent theorem in context and all current error messages.This retrieval process helps in generating\nmore contextually accurate prompts that allow the language model to better correct its own errors as\nwell as ﬁnd useful lemmas to reference.4 EXPERIMENTS\n\nWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics and compare its results to those of the base GPT-4o and GPT-4o-mini\nmodels.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.4.1 SETUP\n\nOur experimentation is split into three distinct stages.We ﬁrst perform ablation testing on the\nImProver model parameters (§3.1) to ensure that ImProver’s parameter speciﬁcation is the optimal\none with respect to correctness and metric optimization score.We then evaluate this optimal param-\neter combination on datasets of varying complexity and analyze the performance and results thereof.Lastly, we note the performance of ImProver in NTP applications in comparison to the base GPT-4o\nand GPT-4o-mini models.Datasets. We evaluate ImProver on subsets of the following datasets.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.We use exercise solutions from set theory, elementary number theory, group\ntheory, topology, differential calculus, and integration & measure theory.This dataset contains the-\norems at an undergraduate-level of complexity.For our main results, we evaluated on 72 theorems\nfrom exercise solutions from MIL chapters 4, 5, 8, 9, and 10.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.This is a\ndataset of internationally-renowned competitive math problems, many of which are readable, yet\nquite verbose.This dataset contains theorems of a competitive format, and although they contain\n\n5\n\n\fconcepts only at a high-school level, the logical complexity of internationally-renowned competi-\ntion results is far above that.For our main results, we used all 26 theorems and lemmas from the\nCompﬁles database of complete solutions to the International Mathematics Olympiad (IMO) and\nthe American Mathematics Olympiad (USAMO) from 2016-2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.These proofs are extremely\nefﬁcient, concise, and generalized - which often comes at the cost of readability and understandabil-\nity.These results and theorems often are at the cutting edge of research.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.This is the most difﬁcult dataset. Models.Our base generator uses GPT-4o (OpenAI et al., 2024).Since no prior methods currently\nexist for automated proof optimization, we consider a prompted GPT-4o without the improvements\ndescribed in (§3.1) as our baseline.Additionally, for a given metric, we write a prompt that brieﬂy\ndescribes the metric and the proof optimization task.Speciﬁc\nprompt information is detailed in (§A)\n\nPerformance metrics.Since proof optimization is a new task, we deﬁne four performance metrics\nfor measuring aspects of correctness and improvement.First, we deﬁne improvement for length as percentage change in length, µlen(y0)−µlen(y)\n× 100.For\nreadability, we use the difference, µread(y)−µread(yo).If no correct output is generated by the model\nfor a speciﬁc theorem, improvement is deﬁned to be zero.We deﬁne nonempty improvement as\nthe improvement restricted to theorems for which some output has nonzero improvement.µlen(y0)\n\nIntuitively, improvement is the expected improvement in metric score from the input to output, ac-\ncounting for errors in the generation.The nonempty improvement score is the expected improvement\nin metric score, given that there are no errors in the generation.Similar improvement scores can be\ndeﬁned for other metrics using a binary function of the metric assigned to the original and optimized\nproofs.Additionally, the accuracy is the percentage of theorems in the dataset which the model was able to\ngenerate a correct output for.The improved accuracy is the percentage of theorems in the dataset\nwhich the model was able to generate a correct output for, as well as improve the metric to be\nnonzero.4.1.1 ABLATIONS\n\nWhen performing our ablation studies, we used a ﬁxed dataset (MIL) and metric (length) and varied\nthe parameters of all the features to ﬁnd the optimal combination.However, as there are over 8640\npossible combinations, it is inefﬁcient to test all combinations at once.Sampling Method: Here, we evaluate the effects of best-of-n and reﬁnement for a ﬁxed n = 5.Table 1: Average Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nMetric\n\nModel\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n3.7\n20.96\n\n2.21\n9.34\n\n6.25\n30.54\n\n4.18\n13.45\n\n15.15\n55.29\n\n8.02\n30.53\n\n18.58\n56.56\n\n14.48\n30.97\n\n26.36%\n100.0%\n\n18.75%\n100.0%\n\n8.31%\n35.44%\n\n6.13 %\n24.56%\n\n37.5%\n100.0%\n\n28.85%\n100.0%\n\n14.42%\n50.0%\n\n11.54%\n34.21%\n\nTable 2: MIL Proof optimization results.Improvement Nonempty Improvement Accuracy\n\nImproved Acc. recent.The model is ﬁxed as GPT-4o, CoS, output formatting, and examples are ﬁxed as the best\ncombination from the previous test, and no other features enabled.n and Model: Here, we evaluate the effects of larger n values and different models.CoS, output formatting, examples, and sampling method are ﬁxed as the best\ncombination from the previous test, and no other features enabled.Combos and RAG: We evaluate combination methods refinement(best_of_m',m) and\nbest_of_m'(refinement(m)), for m 6= m′ with mm′ equal to the optimal value m from\nthe previous test.Model, CoS, output for-\nmatting, examples, n, and sampling method are ﬁxed as the best combination from the previous\ntest.We proceed to analyze this data and its implications. Length optimization.First focusing on the length metric, we see that ImProver outperforms GPT-\n4o with respect to the improvement score by 566% (aggregated over all datasets).Additionally, we\nare guaranteed that ImProver produces a correct output, although that output may just be the same\nas the input.However, 35.44% of the time, it generates a correct output that is not the same length\nas the input, and in that case, we expect an average of a 55.29% reduction in length.Comparing\nthis with GPT-4o, we conclude that not only can ImProver optimize at a higher level on arbitrary\ntheorems, but its ability to generate nontrivial correct outputs is far greater in comparison to GPT-4o.7\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 3: Compﬁles Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n2.75\n18.86\n\n0.39\n5.74\n\n0.0\n6.19\n\n0.0\n4.63\n\n30.7\n54.48\n\n3.38\n24.89\n\n0.0\n53.65\n\n0.0\n33.19\n\n11.54%\n100.0%\n\n14.1%\n100.0%\n\n5.13%\n34.62%\n\n1.28%\n19.23%\n\n16.67%\n100.0%\n\n4.65%\n100.0%\n\n0.0%\n11.54%\n\n0.0%\n11.63%\n\nMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 4: Mathlib Proof optimization results. Readability optimization.Readability optimization is similar, with ImProver outperforming\nGPT-4o by 423%.Moreover, the accuracy, improved accuracy, and nonempty improvement dis-\nparities for readability parallel those of the length tests.However, it should be noted that for both\nGPT-4o and ImProver, the accuracy and improved accuracy scores were markedly smaller for read-\nability than length optimization.This suggests that for both models, it was generally more “difﬁcult”\nto generate a correct output, and moreover, generate a correct output with a better metric score\nthan the input, for readability optimization than length optimization.In other words, optimizing for\nreadability is more difﬁcult for the underlying generator than optimizing for length.However, we\nspeculate with higher-quality prompts, descriptions of the metric, and examples, this disparity can\nbe minimized.Regardless, we note that different metrics can be less likely to be correctly optimized,\nand that model performance is correlated with the metric it seeks to optimize – both for GPT-4o and\nImProver.Optimization varies based on dataset difﬁculty.However, it should be noted that for both metrics, the nonempty improvement of\nImProver stayed consistent, whereas for GPT-4o, it followed the aforementioned trend of decreas-\ning with difﬁculty.As such, we can speculate that the bottleneck in the improvement score is not the\nmodel’s ability to optimize the proof for a metric, but rather its ability to generate a new correct\nproof at all.As such, we conjecture that with more capable generator models, the accuracy – and\nthus, the improvement score – in optimization tasks will continue to increase, until the improvement\nscores match the nonempty improvement.Overall, we conclude that although the performance of both ImProver and GPT-4o decreases on\nlength and readability optimization on more difﬁcult datasets, ImProver signiﬁcantly outperforms\nGPT-4o on all datasets for length and readability optimization.4.2.1 ABLATION TESTING\n\nWe perform ablation studies using a subset of the MIL dataset as discussed in §4.1.1.The results\nof this factorial study are aggregated in Table 5.We measure the baseline results from the GPT-\n4o and GPT-4o-mini models, noting that GPT-4o is the better-scoring model (with respect to the\nimprovement score).Thus, ﬁxing this model, we vary the output formatting type and if CoS is\nenabled, and determine that outputting flat with CoS enabled maximizes the improvement score.8\n\n\fTable 5: Ablation results.Each cell in the ablation tests shows best / worst, which are the best\nand worst parameter combinations in the test group.The ImProver speciﬁcation outputs the input\ntheorem when no correct proof is generated, which results in an accuracy of 100% on MIL.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o-mini\nGPT-4o\n+ Output and CoS\n+ Example Retrieval\n+ Sampling Method\n+ n and Model\n+ Combos and RAG\nImProver\n\n0\n7.03\n8.04 / 6.31\n9.34 / 5.67\n15.35 / 9.34\n23.51 / 3.65\n34.88 / 28.25\n\n0\n19.67\n12.38 / 14.17\n14.7 / 8.44\n18.44 / 14.7\n26.28 / 4.63\n57.56 / 33.48\n\n3.62%\n35.77%\n\n0%\n15.33%\n\n64.96% / 44.53% 21.17% / 16.06%\n63.5% / 67.15%\n21.9% / 16.79%\n36.5% / 21.9%\n83.21% / 63.5%\n89.47% / 78.95% 45.61% / 8.77%\n60.61% / 84.38% 54.55% / 53.12%\n\n34.88\n\n57.56\n\n100%\n\n54.55%\n\nTable 6: CoS Readability Ablation results.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o\nImProver, CoS Disabled\nImProver\n\n4.97\n9.23\n16.69\n\n15.89\n24.61\n31.42\n\n37.5%\n100.0%\n100.0%\n\n12.5%\n28.12%\n46.88%\n\nFixing these parameters, we now vary the number of examples retrieved, noting that prompting\nwith 10 examples maximizes the improvement score.Fixing this parameter, we vary the sampling\nmethods (excluding compound methods and ﬁxing n = 5) and observe that best-of-n is the best\nparameter combination.Fixing these parameters, we consider all mixed compound sampling methods\nwith and without document retrieval enabled, concluding that a 5-step reﬁnement with best-of-3 on\neach iteration, with RAG enabled, is the optimal combination.Thus, as we can see from Table 5, the optimal parameter combination comes from gpt-4o outputting\nas a string list with CoS, RAG, 10 examples, 5-step reﬁnement with each iteration being a\nbest-of-3 evaluation.Changing any one of these parameters them leads to a reduction in performance.Additional ablation data can be found at (§B.1).4.2.2 NEURAL THEOREM PROVING EVALUATION\n\nWe evaluate ImProver’s neural theorem proving (NTP) performance using the completion metric.We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input\nproof.Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-4o.ImProver substantially outperforms GPT-4o across all topics, with an 80% increase in accuracy\ncompared to the base model, showing that proof optimization systems are easily extendable to NTP\nsystems.9\n\n\fTable 7: Proof generation results.Each cell shows percent accuracy.MIL\n\nSet Theory Group Theory\n\nOverall\n\nGPT-4o\nImProver\n\n18.18%\n45.45%\n\n25%\n33.33%\n\n21.73%\n39.13%\n\n4.3 QUALITATIVE RESULTS\n\nNext, we discuss qualitative examples showing the improvements from ImProver in proof optimiza-\ntion.These examples show the balance between correctness and the desired optimization metric,\nshowing how ImProver can produce more concise or readable proofs depending on the use case.Additional examples can be found in (§B.2)\n\nExample 1: Compﬁles Optimization.Consider Figure 1, a lemma from the 2022 IMO Question\n2 (Compﬁles) that we optimize for length.The original proof consisted of 12 tactic steps and multiple intermediate calculations.Original (human-written)\n\nImProver (readability-optimized)\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ncard G = card H * card K)\n\n: K ≃* G / H := by\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\n· rw [← ker_eq_bot_iff, (QuotientGroup.mk'\n\nH).ker_restrict K]\nsimp [h]\n\n· symm\n\nexact aux_card_eq h'\n\ncard G = card H * card K)\n:= by\n\n: K ≃* G / H\n\nhave injectivity : Function.Injective\n\n((QuotientGroup.mk' H).restrict K) := by\nrw [← ker_eq_bot_iff, (QuotientGroup.mk'\nH).ker_restrict K]\nsimp [h]\n\nhave card_eq : card (G / H) = card K := by\n\nexact aux_card_eq h'\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\nexact injectivity\nsymm\nexact card_eq\n\nFigure 3: Optimizing a group-theoretic result from MIL Chapter 8 Section 1 for readability.ImProver introduces two intermediate conjectures into the proof.Additionally observe how the\nmodel deﬁnes hypotheses for use in the latter half of the proof; these predeﬁned hypotheses could\neasily be converted into standalone lemmas for reuse.Example 3: Full Proof Generation.We analyze the application of ImProver to neural theorem\nproving in the MIL example from Figure 4.This theorem relating to group theory originally has no proof, however, ImProver generates one\nfrom scratch.This generated proof is veriﬁed to be correct by Lean, utilizing all the included hy-\npotheses as well as a retrieved mathlib theorem.10\n\n\fOriginal (human-written)\n\nImProver (completeness-optimized)\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n: S ≤ T) : comap ϕ S ≤ comap ϕ T := by\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n\n: S ≤ T) : comap ϕ S ≤ comap ϕ T\n\n:= by\n\nsorry\n\nintro g\nsimp only [mem_comap]\nintro hS\nexact hST hS\n\nFigure 4: Solving a group theorem exercise from MIL Chapter 8 Section 1 for readability.5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.By incorporating CoS, RAG, and other features, ImProver signiﬁcantly outperforms base\nlanguage models in proof optimization over undergraduate, competition, and research-level prob-\nlems.However, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance\non black-box LLM’s.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.As such, we believe that ImProver sets the stage for further\nwork on proof optimization to advance the study and use of AI in mathematics.ACKNOWLEDGEMENTS\n\nRiyaz Ahuja thanks the L3 Lab and Hoskinson Center for Formal Mathematics for their support.11\n\n\fREFERENCES\n\nand\n\nAlphaGeometry\n\nAlphaProof\nstandard\nhttps://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\n2024.\n\nsilver-medal\nproblems.achieves\nolympiad\n\nmathematical\n\ninternational\n\nsolving\n\nTeams.AI\n\nSerge Autexier and Dominik Dietrich.A tactic language for declarative proofs.In Matt Kaufmann\nand Lawrence C. Paulson (eds.), Interactive Theorem Proving, pp.99–114, Berlin, Heidelberg,\n2010. Springer Berlin Heidelberg.Jaime Carbonell and Jade Goldstein.The use of mmr, diversity-based reranking for reordering doc-\numents and producing summaries.In Proceedings of the 21st Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ’98, pp.Association for Computing Machinery. 10.1145/290941.291025.URL https://doi.org/10.1145/290941.291025.Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.Teaching large language models\n\nto self-debug, 2023.URL https://arxiv.org/abs/2304.05128. David Renshaw. compﬁles.https://github.com/dwrensha/compfiles, 2024.Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun.Proof artifact co-\nIn International Conference on Learning\n\ntraining for theorem proving with language models.Representations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.Jiewen Hu, Thomas Zhu, and Sean Welleck.minictx: Neural theorem proving with (long-)contexts,\n\n2024.URL https://arxiv.org/abs/2408.03350.Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu.URL https://openreview.net/forum?id=SMa9EAovKMC.Kim Morrison. lean-training-data.https://github.com/kim-em/lean-training-data,\n\nGuillaume Lample, Timothee Lacroix, Marie anne Lachaux, Aurelien Rodriguez, Amaury\nHypertree proof search\nHayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.for neural\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=J4pX8Q8cxHH. theorem proving. 2024. 2024.Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie\n\nSi.A survey on deep learning for theorem proving, 2024.Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang.A survey of deep\nlearning for mathematical reasoning.In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp.14605–14631, Toronto, Canada, July 2023.Association for Computational Linguistics.10.18653/v1/2023.acl-long.817.URL\nhttps://aclanthology.org/2023.acl-long.817.doi:\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-reﬁne:\nIterative reﬁnement with self-feedback.In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.URL https://openreview.net/forum?id=S37hOerQLB.12\n\nleanprover-community. mathematics_in_lean.https://github.com/leanprover-community/mathematics_in_lea\n\n\fThe mathlib Community.The lean mathematical library.In Proceedings of the 9th ACM SIGPLAN\nInternational Conference on Certiﬁed Programs and Proofs, POPL ’20.ACM, January 2020. doi:\n10.1145/3372885.3373824.URL http://dx.doi.org/10.1145/3372885.3373824.Leonardo de Moura and Sebastian Ullrich.The lean 4 theorem prover and programming lan-\nIn Automated Deduction – CADE 28: 28th International Conference on Automated\nguage.Deduction, Virtual Event, July 12–15, 2021, Proceedings, pp.625–635, Berlin, Heidelberg,\n2021. Springer-Verlag.ISBN 978-3-030-79875-8. doi: 10.1007/978-3-030-79876-5_37.Gpt-4 technical\nreport, 2024. URL https://arxiv.org/abs/2303.08774.Stanislas Polu and Ilya Sutskever.Generative language modeling for automated theorem proving,\n\n2020.13\n\n\fStanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\n\nSutskever.Formal mathematics statement curriculum learning, 2022.Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, and Swarat Chaudhuri.in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.Chain of\n\nFreek Wiedijk. Formal proof sketches.In Stefano Berardi, Mario Coppo, and Ferruccio Damiani\n(eds.), Types for Proofs and Programs, pp.378–393, Berlin, Heidelberg, 2004.Springer Berlin\nHeidelberg. ISBN 978-3-540-24849-1.Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil,\nRyan Prenger, and Anima Anandkumar.LeanDojo: Theorem proving with retrieval-augmented\nlanguage models.In Neural Information Processing Systems (NeurIPS), 2023.Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig.Docprompting: Gener-\nating code by retrieving the docs.In The Eleventh International Conference on Learning Repre-\nsentations, 2023.URL https://openreview.net/forum?id=ZTCxT2t2Ru.14\n\n\fA PROMPTS\n\nA.1 TEMPLATE\n\nIn this appendix, we note the prompts used by ImProver both for general LLM prompting, as well\nas the metric-speciﬁc prompts.For the main prompt sent to the LLM on each sample, we build a prompt string using a chat prompt\ntemplate that is then invoked at runtime to ﬁll in the variables.Namely, these variables include the set of metric prompts, previous results, input theorem, context,\na syntax documents, Mathlib documents, and examples.The prompt template is a conversation of the format:\n\nPlaceholder: All metric prompts with a ‘System’ role\nSystem: You will be given the proof context (i.e.the lean ﬁle contents/imports leading up\nto the theorem declaration) wrapped by <CONTEXT>...</CONTEXT>.You will be given the previous num_prev input/output pairs as well as their metric (met-\nric.name) score and correctness score, as well as any error messages, for your reference to\nimprove upon.Each of these previous results will be wrapped with <PREV I=0></PREV\nI=0>,...,<PREV I=num_prev-1></PREV I=num_prev-1>, with I=num_prev-1 being the\nmost recent result.Remember to use lean 4 syntax, which has signiﬁcant changes from the lean 3 syntax.To\nassist with the syntax relating to the current theorem and current error messages, you will\nbe given num_syntax_docs documents to refer to for ﬁxing these syntax issues.Each of\nthese documents will be wrapped with <SYNTAX_DOC>...</SYNTAX_DOC>.You will also receive num_mathlib_docs documents relevant to the current theorem to\nhelp with formulating your modiﬁed proof.Each of these will be wrapped with <CON-\nTENT_DOC>...</CONTENT_DOC>\nYou will also receive num_examples examples of input-output pairs of proofs that\nwere optimized for the metric metric.Each of these will be wrapped with <EXAM-\nPLE>...</EXAMPLE>\nYou will be given the tactic states as comments for reference.A.2 METRIC PROMPTS\n\nLength Metric\n\nSystem: You are an AI assistant who shortens Lean 4 proofs while ensuring their correct-\nness.You will aim to reduce the number of lines of the tactic proof while ensuring that it\nproperly compiles in Lean 4.15\n\n\fUser: Shorten the current theorem (wrapped in <CURRENT>...</CURRENT>) to be as\nshort in length—measured in the number of lines of the proof—as possible, while also\nensuring that the output is still syntactically correct.\"Readability Metric\n\nCompletion Metric\n\nSystem: You are an AI assistant who rewrites Lean 4 proofs to be more readable while\nensuring their correctness.We measure readablity by considering the ratio of the number\nofexplicitly typed have tactics against the total number of tactics in the proof, as this is\nproportional to whether a proof is declarative in style, and thus, readable.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is\nmore readable and declarative and modular.System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.You will receive a Lean 4 proof you must\nmodify to eliminate any errors so that it compiles correctly and eliminate any “sorry”s with\nfull proofs.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a\nformal, complete, and correct Lean 4 proof by ﬁlling in its tactic proof.B ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this section, we provide more detailed information on the experimental setup and results used to\nevaluate ImProver.Table 8: Output and Chain-of-States Ablations\n\nOutput Format\n\nCoS\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.string\nTrue\nstring\nFalse\nstring list True\nstring list\nFalse\nstring tree\nTrue\nstring tree\nFalse\n\n7.53\n7.03\n8.04\n7.04\n7.62\n6.31\n\n16.12\n19.67\n12.38\n13.58\n15.34\n14.17\n\n46.72%\n35.77%\n64.96%\n51.82%\n49.64%\n44.53%\n\n16.79%\n15.33%\n21.17%\n18.98%\n18.25%\n16.06%\n\nBy Table 8, we see that the optimal combination in this testing group is a string list output\nformat with CoS enabled.Fix these values for all future tests.Examples\n\nTable 9: Example Retrieval Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.0\n3\n5\n7\n10\n\n5.67\n8.49\n8.38\n7.56\n9.34\n\n67.15%\n62.04%\n64.96%\n62.77%\n63.5%\n\n16.79%\n19.71%\n21.17%\n19.71%\n21.9%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the number of examples.By Table 9,\nwe see that the optimal combination in this testing group is 10 examples.Fix this value for all future\ntests.8.44\n13.68\n12.9\n12.04\n14.7\n\n16\n\n\fTable 10: Sampling Method Ablations\n\nMethod\n\nForward Keep Best\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.None\nreﬁnement\nreﬁnement\nreﬁnement\nreﬁnement\nbest-of-n\n\nN/A\n1\n5\n1\n5\nN/A\n\nN/A\nFalse\nFalse\nTrue\nTrue\nN/A\n\n9.34\n14.76\n12.5\n14.95\n13.15\n15.35\n\n14.7\n30.63\n20.88\n14.95\n13.15\n18.44\n\n63.5%\n48.18%\n59.85%\n100.0%\n100.0%\n83.21%\n\n21.9%\n30.66%\n30.66%\n30.66%\n29.93%\n36.5%\n\nNote that forward and keep-best values are parameters for reﬁnement of how many previous itera-\ntions to forward, and whether to keep the most recent or the best iteration in subsequent reﬁnement\nsteps.Now, with the previous optimal parameters ﬁxed, run the ablation on the sample method.By\nTable 10, we see that the optimal combination in this testing group is best-of-n.Fix this value\nfor all future tests.Table 11: Model and n Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.Model\n\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\n\nn\n\n3\n5\n7\n10\n15\n3\n5\n7\n10\n15\n20\n\n19.66\n20.12\n22.44\n21.73\n23.51\n3.65\n5.12\n3.65\n4.99\n4.35\n4.87\n\n24.36\n24.97\n27.21\n25.28\n26.28\n4.63\n6.21\n4.34\n5.69\n5.06\n5.56\n\n80.7%\n80.56%\n82.46%\n85.96%\n89.47%\n78.95%\n82.46%\n84.21%\n87.72%\n85.96%\n87.72%\n\n38.6%\n36.11%\n42.11%\n40.35%\n45.61%\n8.77%\n10.53%\n8.77%\n12.28%\n12.28%\n14.04%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the value of n and model.By\nTable 11, we see that the optimal combination in this testing group is GPT-4o with n = 15.Fix this\nvalue for all future tests.Table 12: RAG and Combination Sampling Method Ablations\n\nRAG Improvement Nonempty Improve. Accuracy\n\nImproved Acc.Combination\n\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nbest-of-n\nbest-of-n\n\nm\n\nm′\n\n5\n3\n5\n3\n3\n5\n3\n5\n5\n3\n5\n3\n3\n5\n5\n3\nN/A 15\nN/A 15\n\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n33.78\n31.23\n31.85\n31.35\n51.32\n50.1\n57.56\n49.75\n32.71\n33.48\n\n100.0%\n100.0%\n100.0%\n100.0%\n63.64%\n65.62%\n60.61%\n59.38%\n90.62%\n84.38%\n\n50.0%\n46.88%\n50.0%\n50.0%\n48.48%\n53.12%\n54.55%\n43.75%\n56.25%\n53.12%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the combination methods and if\nRAG is enabled.By Table 12, we see that the optimal combination in this testing group is a 5-step\nreﬁnement with each iteration being a best-of-3 call, with RAG enabled.B.2 ADDITIONAL QUALITATIVE EXAMPLES\n\nIn this section, we provide qualitative examples demonstrating the improvements ImProver achieves\nin proof optimization.33.78\n31.23\n31.85\n31.35\n32.66\n32.88\n34.88\n29.54\n29.64\n28.25\n\n17\n\n\fCompﬁles: Length Optimization See (§4.3)\n\nCompﬁles: Readability Optimization Consider Figure 5, in which a lemma from the 2019 IMO\nproblem 1 (from the Compﬁles dataset) is optimized for readability.This introduces multiple new\nhypotheses, which generalize a linear_property of the functions, and then reuses and instanti-\nates that (and others, too) hypothesis throughout the proof, creating a signiﬁcantly more declaritive,\nmodular, and therefore readable proof.Original (human-written)\n\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\nrefine hf 1, fun a => ?_i\nchange g a = g 1 * a\nrw [mul_comm, ← smul_eq_mul, ←\n\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nImProver (readability-optimized)\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\n\nhave linear_property : ∀ a, f a = g a := by\n\nintro a\nrfl\n\nhave g_smul : ∀ a, g a = g 1 * a := by\n\nintro a\nrw [mul_comm, ← smul_eq_mul, ←\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nrefine hf 1, fun a => ?_i\nhave f_eq_g : f a = g a := linear_property a\nhave g_a_eq : g a = g 1 * a := g_smul a\nrw [f_eq_g, linear_property 1, g_a_eq]\n\nFigure 5: Optimizing a lemma from IMO 2019 P1 for readability\n\nMIL: Length Optimization Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, eliminating simp calls and introducing proof terms\ninto the structure of the proof to shorten it from 9 tactic invocations to 7.Original (human-written)\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\n\n:= by\n\next x\nsimp only [mem_map]\nconstructor\n· rintro hy, y_in, hyi\n\nexact hϕ y, hy, y_in, rfli, hyi\n\n· rintro hy, hz, z_in, hzi, hyi\n\nuse z, z_in\ncalc ψ.comp ϕ z = ψ (ϕ z) := rfl\n_\n= ψ y := by congr\n\nImProver (length-optimized)\n\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\nby\next x\nsimp only [mem_map]\nconstructor\nrintro hy, y_in, hyi; exact hϕ y, hy, y_in,\n\n:=\n\nrfli, hyi\n\nrintro hy, hz, z_in, hzi, hyi; exact hz,\nz_in, (congr_arg ψ hz).trans hyi\n\nFigure 6: Optimizing a lemma from the solutions of MIL CH08 S01 for length\n\nMIL: Length Optimization 2 Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, converting a full tactic proof into a single proof term\nto shorten it from 28 tactic invocations to 1.Note that the model does not have access to the Lean\ncommands that symbolically generate proof terms, and therefore generates and estimates the proof\nterm entirely by itself.MIL: Readability Optimization See (§4.3)\n\nMathlib: Length Optimization Consider Figure 8, which optimizes a theorem in algebraic topol-\nogy from mathlib for length, eliminating simp calls and combining tactics to shorten it from 3 tactic\ninvocations to 1.Mathlib: Readability Optimization Consider Figure 9, a theorem from Mathlib that we optimize\nfor readability.This original proof carried a score of 0, as it does not contain any declarative statements.It is concise\nand efﬁcient, however, it is difﬁcult to understand and read.18\n\n\fOriginal (human-written)\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\next x; constructor\n· rintro (hxs, xnti | hxt, xnsi)\n\nImProver (length-optimized)\n\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\nexact Set.ext fun x => hfun h => h.elim\n\n(fun hxs, xnti => hOr.inl xs, fun h_, xti =\n> xnt xti) (fun hxt, xnsi => hOr.inr xt,\nfun hxs, _i => xns xsi),\n\nfun hh, nxsti => h.elim (fun xs => Or.inl h\nxs, fun xt => nxst hxs, xtii) (fun xt =>\nOr.inr hxt, fun xs => nxst hxs, xtii)i\n\n· constructor\n\nleft\nexact xs\nrintro h_, xti\ncontradiction\n\n.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.This intermediate tactic greatly helps in the understandability and clarity of the proof.Original (human-written)\n\nImProver (readability-optimized)\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n:=\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]; rfl\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\n:=\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nhave h1 : ∀ x y : Y, Subsingleton\n\n(Path.Homotopic.Quotient x y) ↔ ∀ {p1 p2\n: Path x y}, Path.Homotopic p1 p2 := by\nintro x y\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]\nrfl\n\nsimp only [h1]\n\nFigure 9: Optimizing a theorem from Mathlib/FundamentalGroupoid/SimplyConnected\nfor readability\n\n19",
          "time": 621.849942445755,
          "input_token_size": 16880,
          "output_token_size": 91,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_9d3612f1",
        "created_at": 1750198699.6084573,
        "processing_time": 626.1005373001099,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "a5c1e001": {
      "question": "What future work does the document suggest for ImProver?",
      "answer": "1. Apply fine-tuning and reinforcement learning on a smaller model to match performance at a lower cost.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            0,
            92,
            102,
            121,
            129,
            130,
            131,
            132,
            133,
            134,
            137,
            139,
            140,
            141,
            142,
            144,
            152,
            160,
            161,
            163,
            165,
            166,
            167,
            168,
            172,
            174,
            175,
            176,
            177,
            179,
            180,
            181,
            182,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            196,
            197,
            198,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228
          ],
          "provenance": "4\n2\n0\n2\n \nt\nc\nO\n \n7\n \n \n]\nI\n\nA\n. s\nc\n[\n \n \n1\nv\n3\n5\n7\n4\n0\n.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.µlen(y0)\n\nIntuitively, improvement is the expected improvement in metric score from the input to output, ac-\ncounting for errors in the generation.We deﬁne the following testing groups with the speciﬁed parameter combina-\ntions:\n\nGPT-4o-mini/GPT-4o: This varies the GPT-4o model, outputting a string with no other features.Output and CoS: We evaluate the effects of different output formatting styles (string, string\nlist, string tree) and CoS (True, False), with the model ﬁxed as GPT-4o, with no other\nfeatures enabled.Example Retrieval: We evaluate the effects of increasing the number of examples provided (multi-\nshot prompting) in the range of 0, 3, 5, 7, and 10, with the model ﬁxed as GPT-4o, CoS and output\nformatting ﬁxed as the best combination from the previous test, and no other features enabled.Sampling Method: Here, we evaluate the effects of best-of-n and reﬁnement for a ﬁxed n = 5.Additionally we test on the reﬁnement cases if forwarding the most recent iteration result, or all\nprevious iteration results is the best, and if we should keep the best out of the iterations, or the most\n\n6\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 1: Average Proof optimization results.The model is ﬁxed as GPT-4o, CoS, output formatting, and examples are ﬁxed as the best\ncombination from the previous test, and no other features enabled.We test n =\n3, 5, 7, 10, 15 on GPT-4o and GPT-4o-mini, as well as n = 20 for GPT-4o-mini (as it is of a far\nlower token cost).CoS, output formatting, examples, and sampling method are ﬁxed as the best\ncombination from the previous test, and no other features enabled.Combos and RAG: We evaluate combination methods refinement(best_of_m',m) and\nbest_of_m'(refinement(m)), for m 6= m′ with mm′ equal to the optimal value m from\nthe previous test.We also test the effect of enabling document retrieval.Ablation data. We evaluate our ablations on a subset of MIL.We proceed to analyze this data and its implications. Length optimization.Table 4: Mathlib Proof optimization results. Readability optimization.Readability optimization is similar, with ImProver outperforming\nGPT-4o by 423%.However, it should be noted that for both\nGPT-4o and ImProver, the accuracy and improved accuracy scores were markedly smaller for read-\nability than length optimization.In other words, optimizing for\nreadability is more difﬁcult for the underlying generator than optimizing for length.However, we\nspeculate with higher-quality prompts, descriptions of the metric, and examples, this disparity can\nbe minimized.Regardless, we note that different metrics can be less likely to be correctly optimized,\nand that model performance is correlated with the metric it seeks to optimize – both for GPT-4o and\nImProver.Optimization varies based on dataset difﬁculty.Similarly, the accuracy and improved accuracy scores for both metrics and\nmodels decreased with higher difﬁculty datasets (disregarding ImProver’s accuracy scores, as they\nare ensured to be 100%).As such, we can speculate that the bottleneck in the improvement score is not the\nmodel’s ability to optimize the proof for a metric, but rather its ability to generate a new correct\nproof at all.As such, we conjecture that with more capable generator models, the accuracy – and\nthus, the improvement score – in optimization tasks will continue to increase, until the improvement\nscores match the nonempty improvement.Overall, we conclude that although the performance of both ImProver and GPT-4o decreases on\nlength and readability optimization on more difﬁcult datasets, ImProver signiﬁcantly outperforms\nGPT-4o on all datasets for length and readability optimization.4.2.1 ABLATION TESTING\n\nWe perform ablation studies using a subset of the MIL dataset as discussed in §4.1.1.We measure the baseline results from the GPT-\n4o and GPT-4o-mini models, noting that GPT-4o is the better-scoring model (with respect to the\nimprovement score).Thus, ﬁxing this model, we vary the output formatting type and if CoS is\nenabled, and determine that outputting flat with CoS enabled maximizes the improvement score.8\n\n\fTable 5: Ablation results.Each cell in the ablation tests shows best / worst, which are the best\nand worst parameter combinations in the test group.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o\nImProver, CoS Disabled\nImProver\n\n4.97\n9.23\n16.69\n\n15.89\n24.61\n31.42\n\n37.5%\n100.0%\n100.0%\n\n12.5%\n28.12%\n46.88%\n\nFixing these parameters, we now vary the number of examples retrieved, noting that prompting\nwith 10 examples maximizes the improvement score.Fixing this parameter, we vary the sampling\nmethods (excluding compound methods and ﬁxing n = 5) and observe that best-of-n is the best\nparameter combination.Now, as GPT-4o-mini is signiﬁcantly less computationally expensive than\nits GPT-4o counterpart, we test both models with the sample method ﬁxed to best-of-n, and vary\nn = 1, 3, 5, 7, 10, 15, and for GPT-4o-mini, also n = 20.We conclude that GPT-4o with n = 15\nis the most effective.Fixing these parameters, we consider all mixed compound sampling methods\nwith and without document retrieval enabled, concluding that a 5-step reﬁnement with best-of-3 on\neach iteration, with RAG enabled, is the optimal combination.Thus, as we can see from Table 5, the optimal parameter combination comes from gpt-4o outputting\nas a string list with CoS, RAG, 10 examples, 5-step reﬁnement with each iteration being a\nbest-of-3 evaluation.Changing any one of these parameters them leads to a reduction in performance.Additional ablation data can be found at (§B.1).We additionally examine the effects of dis-\nabling CoS on readability optimization tasks, as the previous study focused on length optimization\ntasks, and we speculate that CoS has a high impact on the performance of readability optimization\ntasks, as the proof states that are embedded due to CoS seem to be a critical aspect to generating the\nexplicit declarations that the readability metric measures.We conﬁrm this result by considering Table 6 and observe that simply enabling CoS nearly doubles\nthe improvement score, and signiﬁcantly improves the nonempty improvement score, suggesting that\nCoS has a high degree of impact on optimizing for the readability metric, as conjectured.However,\nwe also note a signiﬁcant increase in improved accuracy, which suggests that embedding the chain\nof states also improves the ability of the model to generate nontrivial correct outputs, implying that\nthe symbolic information contained in the states are critical to effectively modifying the structure\nand content of a proof.We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input\nproof.Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-4o.ImProver substantially outperforms GPT-4o across all topics, with an 80% increase in accuracy\ncompared to the base model, showing that proof optimization systems are easily extendable to NTP\nsystems.9\n\n\fTable 7: Proof generation results.Each cell shows percent accuracy.MIL\n\nSet Theory Group Theory\n\nOverall\n\nGPT-4o\nImProver\n\n18.18%\n45.45%\n\n25%\n33.33%\n\n21.73%\n39.13%\n\n4.3 QUALITATIVE RESULTS\n\nNext, we discuss qualitative examples showing the improvements from ImProver in proof optimiza-\ntion.These examples show the balance between correctness and the desired optimization metric,\nshowing how ImProver can produce more concise or readable proofs depending on the use case.Additional examples can be found in (§B.2)\n\nExample 1: Compﬁles Optimization.Consider Figure 1, a lemma from the 2022 IMO Question\n2 (Compﬁles) that we optimize for length.After applying\nImProver for length optimization, the proof was halved to only 6 tactics, eliminating redundant steps\nwhile maintaining correctness.In comparison, the shortened proof makes multiple nontrivial optimizations, such as eliminating the\nh2’ and h4 and hxw hypotheses, as well as fully generating proof terms for speciﬁc rewrites and\nother tactics.Example 2: MIL. Consider Figure 3, a result from MIL that we optimize for readability.Original (human-written)\n\nImProver (readability-optimized)\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ncard G = card H * card K)\n\n: K ≃* G / H := by\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\n· rw [← ker_eq_bot_iff, (QuotientGroup.mk'\n\nH).ker_restrict K]\nsimp [h]\n\n· symm\n\nexact aux_card_eq h'\n\ncard G = card H * card K)\n:= by\n\n: K ≃* G / H\n\nhave injectivity : Function.Injective\n\n((QuotientGroup.mk' H).restrict K) := by\nrw [← ker_eq_bot_iff, (QuotientGroup.mk'\nH).ker_restrict K]\nsimp [h]\n\nhave card_eq : card (G / H) = card K := by\n\nexact aux_card_eq h'\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\nexact injectivity\nsymm\nexact card_eq\n\nFigure 3: Optimizing a group-theoretic result from MIL Chapter 8 Section 1 for readability.We\ndeﬁne a proof to be readable if it is written in a declarative style, which consists of intermediate\nconjectures (have · · · statements).ImProver introduces two intermediate conjectures into the proof.This original proof carried a score of 0, as it does not contain any declarative statements.In compar-\nison, after applying ImProver, we transformed the proof to be more declarative, with many more\nintermediate steps with explicit have tactics for improved clarity.Additionally observe how the\nmodel deﬁnes hypotheses for use in the latter half of the proof; these predeﬁned hypotheses could\neasily be converted into standalone lemmas for reuse.Example 3: Full Proof Generation.We analyze the application of ImProver to neural theorem\nproving in the MIL example from Figure 4.This theorem relating to group theory originally has no proof, however, ImProver generates one\nfrom scratch.This generated proof is veriﬁed to be correct by Lean, utilizing all the included hy-\npotheses as well as a retrieved mathlib theorem.10\n\n\fOriginal (human-written)\n\nImProver (completeness-optimized)\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n: S ≤ T) : comap ϕ S ≤ comap ϕ T := by\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n\n: S ≤ T) : comap ϕ S ≤ comap ϕ T\n\n:= by\n\nsorry\n\nintro g\nsimp only [mem_comap]\nintro hS\nexact hST hS\n\nFigure 4: Solving a group theorem exercise from MIL Chapter 8 Section 1 for readability.5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.By incorporating CoS, RAG, and other features, ImProver signiﬁcantly outperforms base\nlanguage models in proof optimization over undergraduate, competition, and research-level prob-\nlems.However, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance\non black-box LLM’s.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.",
          "time": 200.65674209594727,
          "input_token_size": 16874,
          "output_token_size": 26,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_a5c1e001",
        "created_at": 1750198714.1702967,
        "processing_time": 203.21068263053894,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "a75422c4": {
      "question": "Who are the authors of the document and what are their contributions?",
      "answer": "Authors and their contributions:\n\n1. Riyaz Ahuja - Developed ImProver, a large-language-model agent for automated proof optimization.\n2. Jeremy Avigad - Contributed to the research and development of the proof optimization techniques.\n3. Prasad Tetali - Assisted in the formulation and testing of the proof optimization methods.\n4. Sean Welleck - Involved in the implementation and evaluation of the ImProver system.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance": "4\n2\n0\n2\n \nt\nc\nO\n \n7\n \n \n]\nI\n\nA\n. s\nc\n[\n \n \n1\nv\n3\n5\n7\n4\n0\n.0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.To this end, we study a new problem of automated proof\noptimization: rewriting a proof so that it is correct and optimizes for an arbitrary\ncriterion, such as length or readability.As a ﬁrst method for automated proof opti-\nmization, we present ImProver, a large-language-model agent that rewrites proofs\nto optimize arbitrary user-deﬁned metrics in Lean.We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.In\npractice, however, informal, natural-language proofs are prone to imprecision, ambiguity, and error.Using a formal language such as Lean (Moura & Ullrich, 2021) removes ambiguity and precision\nand enables a proof assistant to verify correctness down to the primitives of a formal axiomatic\nsystem.Formal proofs, however, can be hard to read and often suffer from low reusability or excessive detail.For example, formal proofs in Lean’s extensive mathematical library, Mathlib (mathlib Community,\n2020), are generally designed to be concise and very general, often at the expense of readability.Formal proofs in an expository text, in contrast, may include detailed calculations steps, making\nthem readable but verbose.Machine learning systems trained on such proofs learn to mimic these\nvaried conventions (Hu et al., 2024), which may not be the optimal use of the limited supply of\nhuman-written proofs.As a result, we would like to be able to automatically refactor proofs to meet\na secondary objective such as length or readability.To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is\ncorrect and optimizes a criterion such as length or readability.We ﬁnd that naively applying LLMs\nto proof optimization falls short, often resulting in incorrect or poorly optimized proofs.We develop\nvarious improvements that can be applied on top of a black-box language model, including Chain-\nof-States prompting–an analogy to chain-of-thought prompting (Wei et al., 2022) that shows inter-\nmediate proof states–along with error-correction and retrieval.We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.1\n\n\fWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more readable, and more declarative in style.1\n\nOriginal (human-written)\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\nImProver (length-optimized)\n\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nrw [h1u _ (Classical.choose_spec _)]\nobtain hw, h1e', h1u'i := h1 y\nrw [h1u' _ ((h2 _ _).mpr h1e)]\nexact h1u' _ (Classical.choose_spec _)\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nhave h2' : Classical.choose (h1 x).exists =\n\ny :=\nh1u _ (Classical.choose_spec (h1\nx).exists)\n\nrw [h2']\nobtain hw, h1e', h1u'i := h1 y\nhave h4 := Classical.choose_spec (h1\n\nhave hxw : x = w := by\n\ny).exists\n\napply h1u'\nrw [h2]\nexact h1e\n\nrw [hxw]\nexact h1u' _ h4\n\nFigure 1: ImProver automatically rewrites formal proofs to optimize a criterion such as length\nor readability while remaining correct.In this example, ImProver optimizes a human-written\nlemma (right) from the 2022 International Math Olympiad (Question 2, solution from Comp-\nﬁles (David Renshaw, 2024)) for length.ImProver’s optimized proof is correct and more concise.2 RELATED WORK\n\nRecently there has been wide interest in automating theorem proving in interactive proof assistants;\nsee (Lu et al., 2023; Li et al., 2024) for surveys.A typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs\nsuch as Lean’s Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024).A model learns from the distribution of proofs in the cor-\npus, such as Mathlib-style proofs.Recently, the AlphaProof (AlphaProof & Teams, 2024) system\nwas shown to produce proofs with an arcane, non-human structure and syntax.We consider the\nnew problem of rewriting a proof to optimize a metric, such as rewriting a proof into a more read-\nable or more concise one.Proof optimization is more general than theorem proving, since we can\nalso rewrite an empty proof to optimize correctness.Finally, there is a rich literature on the var-\nied styles of (human) formal proofs (e.g., (Autexier & Dietrich, 2010; Wiedijk, 2004)).Our model,\nImProver, builds on neural theorem proving techniques including full proof generation (Jiang et al.,\n2023; First et al., 2023), conditioning on example proofs (Jiang et al., 2023), retrieval (Yang et al.,\n2023; Thakur et al., 2024), and preceding ﬁle context (First et al., 2023; Hu et al., 2024), as well as\nerror correction (Madaan et al., 2023; Chen et al., 2023) and documentation retrieval (Zhou et al.,\n2023) from code generation.ImProver brings these code generation techniques, along with new\nChain-of-States prompting and meta-programmed contextual information, into a uniﬁed proof opti-\nmization agent.3 AUTOMATED PROOF OPTIMIZATION WITH ImProver\n\nAutomated Proof Optimization.Given a theorem statement x, additional context c, and an initial\nproof y0, proof optimization consists of generating a new proof y that is correct and minimizes (or\nmaximizes) a metric µ(x, c, y0, y) → R.\n\n1Code is available at https://github.com/riyazahuja/ImProver.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.Note that shorter proofs often\nrepresent more efﬁcient proofs.Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.Concretely, we evaluate this using the ratio of number of explicitly typed\nhave tactics to total number of tactic invocations.Completion Metric: The completion of a proof simply describes its correctness.This is a trivial\nmetric which measures the number of errors present.The completion metric is used for concretely\nviewing proof optimization as a generalization of neural theorem proving.3.1\n\nIMPROVER\n\nWe develop several improvements that can be applied to a black-box LLM generator yout ∼\nG(·|xin), such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these pa-\nrameters.The explicit prompts and templates that are sent to the LLM can be found in (§A).3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.The intermediate states often contain valuable information (e.g., an expression\nafter it has been simpliﬁed) that is not present in the tactics.To allow the model to reason about\nthese intermediate goals and hypotheses, we use tools from Lean metaprogramming to automatically\nannotate each proof state as a comment prior to each tactic.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.The implementation of this extraction system is mod-\neled from the work (Kim Morrison, 2024).Speciﬁcally, in the compiler’s elaboration and eval-\nuation stages – where the parsed theorem code is ﬁrst converted into concrete syntax trees (in\npractice, Syntax objects) and abstract syntax trees (Expr objects) – we convert the CST and\nAST output objects into the relevant proof data and proof states in the form of proof trees\n(Lean.Elab.InfoTree).These proof trees contain detailed context and information on a tactic-\nby-tactic level relating to the modiﬁcation of the proof state, metavariable context, and proof cor-\nrectness.After state extraction is completed and cached for efﬁcient future access, we annotate the proof text\nitself to contain the intermediate states in the form as comments.Figure 2 shows an example. This explicit reasoning aims to help the generator model construct more optimized proofs via addi-\ntional symbolic data.3.1.2 OUTPUT FORMATTING. LLM outputs often contain ancillary and syntactically invalid content, especially before and after\nthe actual proof.Additionally, by applying additional structure to the LLM outputs, we may hope to\ngenerate more structured proofs.To analyze this hypothesis, we introduce two additional output for-\nmats to the standard str output: flat and structured.The former enforces a tactic sequence\noutput as a list of strings, and the latter enforces a proof tree output as a tree of strings.3.1.3 SAMPLING METHOD\n\nWe also introduce different methods of sampling between many (sequential or parallel) LLM infer-\nence calls, involving best-of-n and iterative reﬁnement implementations, as well as combinations\nthereof.3\n\n\fWithout Chain-of-States\n\nWith Chain-of-States\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u) := by\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u)\n\n:= by\n\nrintro x (hxs, xti | hxs, xui)\n· use xs; left; exact xt\n.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!-/\n. use xs; right; exact xu\n/-\nGoals Solved!-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.More speciﬁcally, our scoring function is given by the 2-ary\ncomparison function S, whose arguments are output objects y, y′.max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.This comparison function can be extended to evaluate the best output of any ﬁnite n via induction.This best-of-n technique is implemented as a curried function such that each of the n calls can be\nhandled by any arbitrary sampling method, or just a single standard prompt at user discretion.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.The reﬁnement process relies on user-deﬁned pa-\nrameters n and prev_num to specify the number of iterations and the number of previous iteration\ninfo to forward, respectively.Each iteration carries information on the last prev_num iterations,\nincluding input, output, metric score, correctness, and error messages.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.Similar to the best-of-n tech-\nnique, it relies on an argument n for the number of reﬁnement steps, and is curried such that each\nreﬁnement step can be handled by any other prompting function.However, unlike best-of-n, there is\nno opportunity for parallelism as each iteration is dependent on information from the previous call.Combination Sampling and Compound Prompt Functions Compound prompt functions utilize\nthe curried nature of the implementations of best-of-n and reﬁnement to nest these techniques within\none another.For example:\n\n4\n\n\fbest_of_n((refinement,m),n) is a compound sampling method that run a best-of-n,\nwhere each call is a m-step reﬁnement.refinement((best_of_n,m),n) is a compound sampling method that runs a n-step reﬁne-\nment, where each call is a best-of-m call to the LLM.Note that with each of these compound prompt functions, there are always a total of mn iterations.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.More speciﬁcally, example retrieval selects the most relevant user-generated examples of proof op-\ntimization on a speciﬁc metric.Namely, each metric is loaded with a cached (vector) database\npopulated with human-made examples of preoptimized and postoptimized pairs of Lean theorems.The number of examples that are retrieved is user-speciﬁed.Document retrieval extracts information using MMR from a pair of ﬁxed (vector) databases.The\ndatabases store semantically chunked data from the Theorem Proving in Lean (TPiL) handbook –\ncontaining syntax guides and tactic explanations – and the Mathlib mathematics libary – containing\nthousands of theorems and lemmas.The chunking is handled by a recursive character splitter, which\nsplits the TPiL markdown ﬁles at on its headers and Mathlib ﬁles at the start of theorems, examples,\nlemmas, and deﬁnitions – with chunk sizes of 1000 characters with a 200 character overlap.The Mathlib retriever ﬁnds the top k documents that score the highest MMR score against the current\ntheorem, the TPiL retriever ﬁnds the top k documents that score the highest MMR score against the\ncurrent theorem in context and all current error messages.This retrieval process helps in generating\nmore contextually accurate prompts that allow the language model to better correct its own errors as\nwell as ﬁnd useful lemmas to reference.4 EXPERIMENTS\n\nWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics and compare its results to those of the base GPT-4o and GPT-4o-mini\nmodels.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.4.1 SETUP\n\nOur experimentation is split into three distinct stages.We ﬁrst perform ablation testing on the\nImProver model parameters (§3.1) to ensure that ImProver’s parameter speciﬁcation is the optimal\none with respect to correctness and metric optimization score.We then evaluate this optimal param-\neter combination on datasets of varying complexity and analyze the performance and results thereof.Lastly, we note the performance of ImProver in NTP applications in comparison to the base GPT-4o\nand GPT-4o-mini models.Datasets. We evaluate ImProver on subsets of the following datasets.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.We use exercise solutions from set theory, elementary number theory, group\ntheory, topology, differential calculus, and integration & measure theory.This dataset contains the-\norems at an undergraduate-level of complexity.For our main results, we evaluated on 72 theorems\nfrom exercise solutions from MIL chapters 4, 5, 8, 9, and 10.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.This is a\ndataset of internationally-renowned competitive math problems, many of which are readable, yet\nquite verbose.This dataset contains theorems of a competitive format, and although they contain\n\n5\n\n\fconcepts only at a high-school level, the logical complexity of internationally-renowned competi-\ntion results is far above that.For our main results, we used all 26 theorems and lemmas from the\nCompﬁles database of complete solutions to the International Mathematics Olympiad (IMO) and\nthe American Mathematics Olympiad (USAMO) from 2016-2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.These proofs are extremely\nefﬁcient, concise, and generalized - which often comes at the cost of readability and understandabil-\nity.These results and theorems often are at the cutting edge of research.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.This is the most difﬁcult dataset. Models.Our base generator uses GPT-4o (OpenAI et al., 2024).Since no prior methods currently\nexist for automated proof optimization, we consider a prompted GPT-4o without the improvements\ndescribed in (§3.1) as our baseline.Additionally, for a given metric, we write a prompt that brieﬂy\ndescribes the metric and the proof optimization task.We also provide instructions, context, and in-\nformation depending on the features selected, and add the theorem and proof to the prompt.Speciﬁc\nprompt information is detailed in (§A)\n\nPerformance metrics.Since proof optimization is a new task, we deﬁne four performance metrics\nfor measuring aspects of correctness and improvement.First, we deﬁne improvement for length as percentage change in length, µlen(y0)−µlen(y)\n× 100.For\nreadability, we use the difference, µread(y)−µread(yo).If no correct output is generated by the model\nfor a speciﬁc theorem, improvement is deﬁned to be zero.We deﬁne nonempty improvement as\nthe improvement restricted to theorems for which some output has nonzero improvement.µlen(y0)\n\nIntuitively, improvement is the expected improvement in metric score from the input to output, ac-\ncounting for errors in the generation.The nonempty improvement score is the expected improvement\nin metric score, given that there are no errors in the generation.Similar improvement scores can be\ndeﬁned for other metrics using a binary function of the metric assigned to the original and optimized\nproofs.Additionally, the accuracy is the percentage of theorems in the dataset which the model was able to\ngenerate a correct output for.The improved accuracy is the percentage of theorems in the dataset\nwhich the model was able to generate a correct output for, as well as improve the metric to be\nnonzero.4.1.1 ABLATIONS\n\nWhen performing our ablation studies, we used a ﬁxed dataset (MIL) and metric (length) and varied\nthe parameters of all the features to ﬁnd the optimal combination.However, as there are over 8640\npossible combinations, it is inefﬁcient to test all combinations at once.As such, we evaluate using a\nfactorial testing method. Testing Groups.We deﬁne the following testing groups with the speciﬁed parameter combina-\ntions:\n\nGPT-4o-mini/GPT-4o: This varies the GPT-4o model, outputting a string with no other features.Output and CoS: We evaluate the effects of different output formatting styles (string, string\nlist, string tree) and CoS (True, False), with the model ﬁxed as GPT-4o, with no other\nfeatures enabled.Example Retrieval: We evaluate the effects of increasing the number of examples provided (multi-\nshot prompting) in the range of 0, 3, 5, 7, and 10, with the model ﬁxed as GPT-4o, CoS and output\nformatting ﬁxed as the best combination from the previous test, and no other features enabled.Sampling Method: Here, we evaluate the effects of best-of-n and reﬁnement for a ﬁxed n = 5.Additionally we test on the reﬁnement cases if forwarding the most recent iteration result, or all\nprevious iteration results is the best, and if we should keep the best out of the iterations, or the most\n\n6\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 1: Average Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nMetric\n\nModel\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n3.7\n20.96\n\n2.21\n9.34\n\n6.25\n30.54\n\n4.18\n13.45\n\n15.15\n55.29\n\n8.02\n30.53\n\n18.58\n56.56\n\n14.48\n30.97\n\n26.36%\n100.0%\n\n18.75%\n100.0%\n\n8.31%\n35.44%\n\n6.13 %\n24.56%\n\n37.5%\n100.0%\n\n28.85%\n100.0%\n\n14.42%\n50.0%\n\n11.54%\n34.21%\n\nTable 2: MIL Proof optimization results.Improvement Nonempty Improvement Accuracy\n\nImproved Acc. recent.The model is ﬁxed as GPT-4o, CoS, output formatting, and examples are ﬁxed as the best\ncombination from the previous test, and no other features enabled.n and Model: Here, we evaluate the effects of larger n values and different models.We test n =\n3, 5, 7, 10, 15 on GPT-4o and GPT-4o-mini, as well as n = 20 for GPT-4o-mini (as it is of a far\nlower token cost).CoS, output formatting, examples, and sampling method are ﬁxed as the best\ncombination from the previous test, and no other features enabled.Combos and RAG: We evaluate combination methods refinement(best_of_m',m) and\nbest_of_m'(refinement(m)), for m 6= m′ with mm′ equal to the optimal value m from\nthe previous test.We also test the effect of enabling document retrieval.Model, CoS, output for-\nmatting, examples, n, and sampling method are ﬁxed as the best combination from the previous\ntest.Ablation data. We evaluate our ablations on a subset of MIL.However, due to the increase in\nmodel calls for larger n values, we switch a representative sample of this subset for some test groups.Namely,\n\nGPT-4o-mini, GPT-4o, Output and Cos, Example Retrieval, and Sampling Method are tested\non the 133 theorems in the solutions of C03_Logic, C04_Sets_and_Functions, and\nC05_Elementary_Number_Theory.n and Model are tested on 55 theorems from a representative sample of the aforementioned, and\nCombos and RAG are tested on a representative sample of 32 theorems from the aforementioned.4.2 RESULTS\n\nImProver is capable of optimizing proofs in all settings.From Table 2, Table 3, and Table 4,\nwe can see that ImProver is capable of optimizing proofs on all datasets for both the length and\nreadability metrics.Furthermore, Table 1 shows that across all metrics, ImProver signiﬁcantly\noutperforms GPT-4o on proof optimization tasks on every experimental measure – aggregated from\nall datasets.Additionally, from Table 2, Table 3, and Table 4, we can see that ImProver outperforms\nGPT-4o on each dataset as well.We proceed to analyze this data and its implications. Length optimization.First focusing on the length metric, we see that ImProver outperforms GPT-\n4o with respect to the improvement score by 566% (aggregated over all datasets).Additionally, we\nare guaranteed that ImProver produces a correct output, although that output may just be the same\nas the input.However, 35.44% of the time, it generates a correct output that is not the same length\nas the input, and in that case, we expect an average of a 55.29% reduction in length.Comparing\nthis with GPT-4o, we conclude that not only can ImProver optimize at a higher level on arbitrary\ntheorems, but its ability to generate nontrivial correct outputs is far greater in comparison to GPT-4o.7\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 3: Compﬁles Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n2.75\n18.86\n\n0.39\n5.74\n\n0.0\n6.19\n\n0.0\n4.63\n\n30.7\n54.48\n\n3.38\n24.89\n\n0.0\n53.65\n\n0.0\n33.19\n\n11.54%\n100.0%\n\n14.1%\n100.0%\n\n5.13%\n34.62%\n\n1.28%\n19.23%\n\n16.67%\n100.0%\n\n4.65%\n100.0%\n\n0.0%\n11.54%\n\n0.0%\n11.63%\n\nMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 4: Mathlib Proof optimization results. Readability optimization.Readability optimization is similar, with ImProver outperforming\nGPT-4o by 423%.Moreover, the accuracy, improved accuracy, and nonempty improvement dis-\nparities for readability parallel those of the length tests.However, it should be noted that for both\nGPT-4o and ImProver, the accuracy and improved accuracy scores were markedly smaller for read-\nability than length optimization.This suggests that for both models, it was generally more “difﬁcult”\nto generate a correct output, and moreover, generate a correct output with a better metric score\nthan the input, for readability optimization than length optimization.In other words, optimizing for\nreadability is more difﬁcult for the underlying generator than optimizing for length.However, we\nspeculate with higher-quality prompts, descriptions of the metric, and examples, this disparity can\nbe minimized.Regardless, we note that different metrics can be less likely to be correctly optimized,\nand that model performance is correlated with the metric it seeks to optimize – both for GPT-4o and\nImProver.Optimization varies based on dataset difﬁculty.Additionally noting Table 2, Table 3, and\nTable 4, we observe that the improvement score for both metrics for both GPT-4o and ImProver\nis highest for the MIL dataset, lower for Compﬁles, and the lowest on the Mathlib theorems.This\nsuggests that the expected improvement in metric score decreases with higher difﬁcultly – with\nundergraduate-level theorems having a signiﬁcantly higher expected improvement than research-\nlevel theorems.However, it should be noted that for both metrics, the nonempty improvement of\nImProver stayed consistent, whereas for GPT-4o, it followed the aforementioned trend of decreas-\ning with difﬁculty.Similarly, the accuracy and improved accuracy scores for both metrics and\nmodels decreased with higher difﬁculty datasets (disregarding ImProver’s accuracy scores, as they\nare ensured to be 100%).This suggests that although the base GPT-4o generator is less likely to\ngenerate a correct output for higher difﬁculty datasets, the improvements that ImProver makes to\nthe base generator allows it to maintain its improvement in the metric score whenever a correct out-\nput is generated.As such, we can speculate that the bottleneck in the improvement score is not the\nmodel’s ability to optimize the proof for a metric, but rather its ability to generate a new correct\nproof at all.As such, we conjecture that with more capable generator models, the accuracy – and\nthus, the improvement score – in optimization tasks will continue to increase, until the improvement\nscores match the nonempty improvement.Overall, we conclude that although the performance of both ImProver and GPT-4o decreases on\nlength and readability optimization on more difﬁcult datasets, ImProver signiﬁcantly outperforms\nGPT-4o on all datasets for length and readability optimization.4.2.1 ABLATION TESTING\n\nWe perform ablation studies using a subset of the MIL dataset as discussed in §4.1.1.The results\nof this factorial study are aggregated in Table 5.We measure the baseline results from the GPT-\n4o and GPT-4o-mini models, noting that GPT-4o is the better-scoring model (with respect to the\nimprovement score).Thus, ﬁxing this model, we vary the output formatting type and if CoS is\nenabled, and determine that outputting flat with CoS enabled maximizes the improvement score.8\n\n\fTable 5: Ablation results.Each cell in the ablation tests shows best / worst, which are the best\nand worst parameter combinations in the test group.The ImProver speciﬁcation outputs the input\ntheorem when no correct proof is generated, which results in an accuracy of 100% on MIL.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o-mini\nGPT-4o\n+ Output and CoS\n+ Example Retrieval\n+ Sampling Method\n+ n and Model\n+ Combos and RAG\nImProver\n\n0\n7.03\n8.04 / 6.31\n9.34 / 5.67\n15.35 / 9.34\n23.51 / 3.65\n34.88 / 28.25\n\n0\n19.67\n12.38 / 14.17\n14.7 / 8.44\n18.44 / 14.7\n26.28 / 4.63\n57.56 / 33.48\n\n3.62%\n35.77%\n\n0%\n15.33%\n\n64.96% / 44.53% 21.17% / 16.06%\n63.5% / 67.15%\n21.9% / 16.79%\n36.5% / 21.9%\n83.21% / 63.5%\n89.47% / 78.95% 45.61% / 8.77%\n60.61% / 84.38% 54.55% / 53.12%\n\n34.88\n\n57.56\n\n100%\n\n54.55%\n\nTable 6: CoS Readability Ablation results.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o\nImProver, CoS Disabled\nImProver\n\n4.97\n9.23\n16.69\n\n15.89\n24.61\n31.42\n\n37.5%\n100.0%\n100.0%\n\n12.5%\n28.12%\n46.88%\n\nFixing these parameters, we now vary the number of examples retrieved, noting that prompting\nwith 10 examples maximizes the improvement score.Fixing this parameter, we vary the sampling\nmethods (excluding compound methods and ﬁxing n = 5) and observe that best-of-n is the best\nparameter combination.Now, as GPT-4o-mini is signiﬁcantly less computationally expensive than\nits GPT-4o counterpart, we test both models with the sample method ﬁxed to best-of-n, and vary\nn = 1, 3, 5, 7, 10, 15, and for GPT-4o-mini, also n = 20.We conclude that GPT-4o with n = 15\nis the most effective.Fixing these parameters, we consider all mixed compound sampling methods\nwith and without document retrieval enabled, concluding that a 5-step reﬁnement with best-of-3 on\neach iteration, with RAG enabled, is the optimal combination.Thus, as we can see from Table 5, the optimal parameter combination comes from gpt-4o outputting\nas a string list with CoS, RAG, 10 examples, 5-step reﬁnement with each iteration being a\nbest-of-3 evaluation.Changing any one of these parameters them leads to a reduction in performance.Additional ablation data can be found at (§B.1).Readability and Chain-of-States (CoS) Ablation.We additionally examine the effects of dis-\nabling CoS on readability optimization tasks, as the previous study focused on length optimization\ntasks, and we speculate that CoS has a high impact on the performance of readability optimization\ntasks, as the proof states that are embedded due to CoS seem to be a critical aspect to generating the\nexplicit declarations that the readability metric measures.We conﬁrm this result by considering Table 6 and observe that simply enabling CoS nearly doubles\nthe improvement score, and signiﬁcantly improves the nonempty improvement score, suggesting that\nCoS has a high degree of impact on optimizing for the readability metric, as conjectured.However,\nwe also note a signiﬁcant increase in improved accuracy, which suggests that embedding the chain\nof states also improves the ability of the model to generate nontrivial correct outputs, implying that\nthe symbolic information contained in the states are critical to effectively modifying the structure\nand content of a proof.4.2.2 NEURAL THEOREM PROVING EVALUATION\n\nWe evaluate ImProver’s neural theorem proving (NTP) performance using the completion metric.We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input\nproof.Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-4o.ImProver substantially outperforms GPT-4o across all topics, with an 80% increase in accuracy\ncompared to the base model, showing that proof optimization systems are easily extendable to NTP\nsystems.9\n\n\fTable 7: Proof generation results.Each cell shows percent accuracy.MIL\n\nSet Theory Group Theory\n\nOverall\n\nGPT-4o\nImProver\n\n18.18%\n45.45%\n\n25%\n33.33%\n\n21.73%\n39.13%\n\n4.3 QUALITATIVE RESULTS\n\nNext, we discuss qualitative examples showing the improvements from ImProver in proof optimiza-\ntion.These examples show the balance between correctness and the desired optimization metric,\nshowing how ImProver can produce more concise or readable proofs depending on the use case.Additional examples can be found in (§B.2)\n\nExample 1: Compﬁles Optimization.Consider Figure 1, a lemma from the 2022 IMO Question\n2 (Compﬁles) that we optimize for length.The original proof consisted of 12 tactic steps and multiple intermediate calculations.After applying\nImProver for length optimization, the proof was halved to only 6 tactics, eliminating redundant steps\nwhile maintaining correctness.In comparison, the shortened proof makes multiple nontrivial optimizations, such as eliminating the\nh2’ and h4 and hxw hypotheses, as well as fully generating proof terms for speciﬁc rewrites and\nother tactics.Example 2: MIL. Consider Figure 3, a result from MIL that we optimize for readability.Original (human-written)\n\nImProver (readability-optimized)\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ncard G = card H * card K)\n\n: K ≃* G / H := by\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\n· rw [← ker_eq_bot_iff, (QuotientGroup.mk'\n\nH).ker_restrict K]\nsimp [h]\n\n· symm\n\nexact aux_card_eq h'\n\ncard G = card H * card K)\n:= by\n\n: K ≃* G / H\n\nhave injectivity : Function.Injective\n\n((QuotientGroup.mk' H).restrict K) := by\nrw [← ker_eq_bot_iff, (QuotientGroup.mk'\nH).ker_restrict K]\nsimp [h]\n\nhave card_eq : card (G / H) = card K := by\n\nexact aux_card_eq h'\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\nexact injectivity\nsymm\nexact card_eq\n\nFigure 3: Optimizing a group-theoretic result from MIL Chapter 8 Section 1 for readability.We\ndeﬁne a proof to be readable if it is written in a declarative style, which consists of intermediate\nconjectures (have · · · statements).ImProver introduces two intermediate conjectures into the proof.This original proof carried a score of 0, as it does not contain any declarative statements.In compar-\nison, after applying ImProver, we transformed the proof to be more declarative, with many more\nintermediate steps with explicit have tactics for improved clarity.Additionally observe how the\nmodel deﬁnes hypotheses for use in the latter half of the proof; these predeﬁned hypotheses could\neasily be converted into standalone lemmas for reuse.Example 3: Full Proof Generation.We analyze the application of ImProver to neural theorem\nproving in the MIL example from Figure 4.This theorem relating to group theory originally has no proof, however, ImProver generates one\nfrom scratch.This generated proof is veriﬁed to be correct by Lean, utilizing all the included hy-\npotheses as well as a retrieved mathlib theorem.10\n\n\fOriginal (human-written)\n\nImProver (completeness-optimized)\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n: S ≤ T) : comap ϕ S ≤ comap ϕ T := by\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n\n: S ≤ T) : comap ϕ S ≤ comap ϕ T\n\n:= by\n\nsorry\n\nintro g\nsimp only [mem_comap]\nintro hS\nexact hST hS\n\nFigure 4: Solving a group theorem exercise from MIL Chapter 8 Section 1 for readability.5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.By incorporating CoS, RAG, and other features, ImProver signiﬁcantly outperforms base\nlanguage models in proof optimization over undergraduate, competition, and research-level prob-\nlems.However, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance\non black-box LLM’s.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.As such, we believe that ImProver sets the stage for further\nwork on proof optimization to advance the study and use of AI in mathematics.ACKNOWLEDGEMENTS\n\nRiyaz Ahuja thanks the L3 Lab and Hoskinson Center for Formal Mathematics for their support.Sean Welleck thanks Convergent Research, the Lean FRO, and the OpenAI Researcher Access Pro-\ngram for their support.11\n\n\fREFERENCES\n\nand\n\nAlphaGeometry\n\nAlphaProof\nstandard\nhttps://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\n2024.\n\nsilver-medal\nproblems.achieves\nolympiad\n\nmathematical\n\ninternational\n\nsolving\n\nTeams.AI\n\nSerge Autexier and Dominik Dietrich.A tactic language for declarative proofs.In Matt Kaufmann\nand Lawrence C. Paulson (eds.), Interactive Theorem Proving, pp.99–114, Berlin, Heidelberg,\n2010. Springer Berlin Heidelberg.Jaime Carbonell and Jade Goldstein.The use of mmr, diversity-based reranking for reordering doc-\numents and producing summaries.In Proceedings of the 21st Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ’98, pp.335–336,\nISBN 1581130155. doi:\nNew York, NY, USA, 1998.Association for Computing Machinery. 10.1145/290941.291025.URL https://doi.org/10.1145/290941.291025.Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.Teaching large language models\n\nto self-debug, 2023.URL https://arxiv.org/abs/2304.05128. David Renshaw. compﬁles.https://github.com/dwrensha/compfiles, 2024.Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun.Baldur: Whole-proof generation and\n\nrepair with large language models, 2023.Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu.Proof artifact co-\nIn International Conference on Learning\n\ntraining for theorem proving with language models.Representations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.Jiewen Hu, Thomas Zhu, and Sean Welleck.minictx: Neural theorem proving with (long-)contexts,\n\n2024.URL https://arxiv.org/abs/2408.03350.Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu.Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs.In The Eleventh International Conference on Learning\nRepresentations, 2023.URL https://openreview.net/forum?id=SMa9EAovKMC.Kim Morrison. lean-training-data.https://github.com/kim-em/lean-training-data,\n\nGuillaume Lample, Timothee Lacroix, Marie anne Lachaux, Aurelien Rodriguez, Amaury\nHypertree proof search\nHayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.for neural\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=J4pX8Q8cxHH. theorem proving. 2024. 2024.Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie\n\nSi.A survey on deep learning for theorem proving, 2024.A survey of deep\nlearning for mathematical reasoning.In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp.14605–14631, Toronto, Canada, July 2023.Association for Computational Linguistics.10.18653/v1/2023.acl-long.817.URL\nhttps://aclanthology.org/2023.acl-long.817.doi:\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-reﬁne:\nIterative reﬁnement with self-feedback.In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.URL https://openreview.net/forum?id=S37hOerQLB.12\n\nleanprover-community. mathematics_in_lean.https://github.com/leanprover-community/mathematics_in_lea\n\n\fThe mathlib Community.The lean mathematical library.In Proceedings of the 9th ACM SIGPLAN\nInternational Conference on Certiﬁed Programs and Proofs, POPL ’20.ACM, January 2020. doi:\n10.1145/3372885.3373824.URL http://dx.doi.org/10.1145/3372885.3373824.Leonardo de Moura and Sebastian Ullrich.The lean 4 theorem prover and programming lan-\nIn Automated Deduction – CADE 28: 28th International Conference on Automated\nguage.Deduction, Virtual Event, July 12–15, 2021, Proceedings, pp.625–635, Berlin, Heidelberg,\n2021. Springer-Verlag.ISBN 978-3-030-79875-8. doi: 10.1007/978-3-030-79876-5_37.URL\nhttps://doi.org/10.1007/978-3-030-79876-5_37.OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red\nAvila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Moham-\nmad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher\nBerner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brit-\ntany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek\nChen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,\nHyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas\nDegry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,\nAdrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada\nFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel,\nTarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff\nHarris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jo-\nmoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,\nNitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik\nKim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, An-\ndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,\nStephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim\nMalfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew\nMayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil,\nDavid Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin,\nVinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeon-\nwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley\nPantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov,\nAndrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde\nde Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea\nPower, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick\nRyder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David\nSchnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah\nShoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama,\nIan Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie\nSummers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin\nTootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón\nUribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welin-\nder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,\nHannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah\nYoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,\nShengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.Gpt-4 technical\nreport, 2024. URL https://arxiv.org/abs/2303.08774.Stanislas Polu and Ilya Sutskever.Generative language modeling for automated theorem proving,\n\n2020.13\n\n\fStanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\n\nSutskever.Formal mathematics statement curriculum learning, 2022.Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, and Swarat Chaudhuri.An in-\n\ncontext learning agent for formal theorem-proving, 2024.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H.\nthought prompting elicits reasoning\nChi, Quoc V Le, and Denny Zhou.in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=_VjQlMeSB_J.Chain of\n\nFreek Wiedijk. Formal proof sketches.In Stefano Berardi, Mario Coppo, and Ferruccio Damiani\n(eds.), Types for Proofs and Programs, pp.378–393, Berlin, Heidelberg, 2004.Springer Berlin\nHeidelberg. ISBN 978-3-540-24849-1.Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil,\nRyan Prenger, and Anima Anandkumar.LeanDojo: Theorem proving with retrieval-augmented\nlanguage models.In Neural Information Processing Systems (NeurIPS), 2023.Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig.Docprompting: Gener-\nating code by retrieving the docs.In The Eleventh International Conference on Learning Repre-\nsentations, 2023.URL https://openreview.net/forum?id=ZTCxT2t2Ru.14\n\n\fA PROMPTS\n\nA.1 TEMPLATE\n\nIn this appendix, we note the prompts used by ImProver both for general LLM prompting, as well\nas the metric-speciﬁc prompts.For the main prompt sent to the LLM on each sample, we build a prompt string using a chat prompt\ntemplate that is then invoked at runtime to ﬁll in the variables.Namely, these variables include the set of metric prompts, previous results, input theorem, context,\na syntax documents, Mathlib documents, and examples.The prompt template is a conversation of the format:\n\nPlaceholder: All metric prompts with a ‘System’ role\nSystem: You will be given the proof context (i.e.the lean ﬁle contents/imports leading up\nto the theorem declaration) wrapped by <CONTEXT>...</CONTEXT>.You will be given the previous num_prev input/output pairs as well as their metric (met-\nric.name) score and correctness score, as well as any error messages, for your reference to\nimprove upon.Each of these previous results will be wrapped with <PREV I=0></PREV\nI=0>,...,<PREV I=num_prev-1></PREV I=num_prev-1>, with I=num_prev-1 being the\nmost recent result.Remember to use lean 4 syntax, which has signiﬁcant changes from the lean 3 syntax.To\nassist with the syntax relating to the current theorem and current error messages, you will\nbe given num_syntax_docs documents to refer to for ﬁxing these syntax issues.Each of\nthese documents will be wrapped with <SYNTAX_DOC>...</SYNTAX_DOC>.You will also receive num_mathlib_docs documents relevant to the current theorem to\nhelp with formulating your modiﬁed proof.Each of these will be wrapped with <CON-\nTENT_DOC>...</CONTENT_DOC>\nYou will also receive num_examples examples of input-output pairs of proofs that\nwere optimized for the metric metric.Each of these will be wrapped with <EXAM-\nPLE>...</EXAMPLE>\nYou will be given the tactic states as comments for reference.The current theorem will be\nwrapped in <CURRENT>...</CURRENT>\nSystem: Output format instructions\nPlaceholder: All retrieved syntax documentation\nPlaceholder: All retrieved mathlib documentation\nPlaceholder: All retrieved examples\nUser: <CONTEXT> context </CONTEXT>\nPlaceholder: Previous results and inputs/outputs\nPlaceholder: All metric prompts with a ‘User’ role\nUser: <CURRENT> theorem </CURRENT>\n\nThis prompt is then invoked and sent to the language model by ﬁlling in all the variables and place-\nholders.Notably, when we invoke the chain given by chain|llm|parser, we throttle the invoca-\ntion with a randomized exponential rate limit throttling to account for API rate limits, especially in\nhighly-parallelized requests like when benchmarking over a large number of theorems.A.2 METRIC PROMPTS\n\nLength Metric\n\nSystem: You are an AI assistant who shortens Lean 4 proofs while ensuring their correct-\nness.You will aim to reduce the number of lines of the tactic proof while ensuring that it\nproperly compiles in Lean 4.15\n\n\fUser: Shorten the current theorem (wrapped in <CURRENT>...</CURRENT>) to be as\nshort in length—measured in the number of lines of the proof—as possible, while also\nensuring that the output is still syntactically correct.\"Readability Metric\n\nCompletion Metric\n\nSystem: You are an AI assistant who rewrites Lean 4 proofs to be more readable while\nensuring their correctness.We measure readablity by considering the ratio of the number\nofexplicitly typed have tactics against the total number of tactics in the proof, as this is\nproportional to whether a proof is declarative in style, and thus, readable.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is\nmore readable and declarative and modular.System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.You will receive a Lean 4 proof you must\nmodify to eliminate any errors so that it compiles correctly and eliminate any “sorry”s with\nfull proofs.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a\nformal, complete, and correct Lean 4 proof by ﬁlling in its tactic proof.B ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this section, we provide more detailed information on the experimental setup and results used to\nevaluate ImProver.B.1 ABLATION DETAILS\n\nWe now proceed to show detailed results from our ablation testing.Table 8: Output and Chain-of-States Ablations\n\nOutput Format\n\nCoS\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.string\nTrue\nstring\nFalse\nstring list True\nstring list\nFalse\nstring tree\nTrue\nstring tree\nFalse\n\n7.53\n7.03\n8.04\n7.04\n7.62\n6.31\n\n16.12\n19.67\n12.38\n13.58\n15.34\n14.17\n\n46.72%\n35.77%\n64.96%\n51.82%\n49.64%\n44.53%\n\n16.79%\n15.33%\n21.17%\n18.98%\n18.25%\n16.06%\n\nBy Table 8, we see that the optimal combination in this testing group is a string list output\nformat with CoS enabled.Fix these values for all future tests.Examples\n\nTable 9: Example Retrieval Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.0\n3\n5\n7\n10\n\n5.67\n8.49\n8.38\n7.56\n9.34\n\n67.15%\n62.04%\n64.96%\n62.77%\n63.5%\n\n16.79%\n19.71%\n21.17%\n19.71%\n21.9%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the number of examples.By Table 9,\nwe see that the optimal combination in this testing group is 10 examples.Fix this value for all future\ntests.8.44\n13.68\n12.9\n12.04\n14.7\n\n16\n\n\fTable 10: Sampling Method Ablations\n\nMethod\n\nForward Keep Best\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.None\nreﬁnement\nreﬁnement\nreﬁnement\nreﬁnement\nbest-of-n\n\nN/A\n1\n5\n1\n5\nN/A\n\nN/A\nFalse\nFalse\nTrue\nTrue\nN/A\n\n9.34\n14.76\n12.5\n14.95\n13.15\n15.35\n\n14.7\n30.63\n20.88\n14.95\n13.15\n18.44\n\n63.5%\n48.18%\n59.85%\n100.0%\n100.0%\n83.21%\n\n21.9%\n30.66%\n30.66%\n30.66%\n29.93%\n36.5%\n\nNote that forward and keep-best values are parameters for reﬁnement of how many previous itera-\ntions to forward, and whether to keep the most recent or the best iteration in subsequent reﬁnement\nsteps.Now, with the previous optimal parameters ﬁxed, run the ablation on the sample method.By\nTable 10, we see that the optimal combination in this testing group is best-of-n.Fix this value\nfor all future tests.Table 11: Model and n Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.Model\n\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\n\nn\n\n3\n5\n7\n10\n15\n3\n5\n7\n10\n15\n20\n\n19.66\n20.12\n22.44\n21.73\n23.51\n3.65\n5.12\n3.65\n4.99\n4.35\n4.87\n\n24.36\n24.97\n27.21\n25.28\n26.28\n4.63\n6.21\n4.34\n5.69\n5.06\n5.56\n\n80.7%\n80.56%\n82.46%\n85.96%\n89.47%\n78.95%\n82.46%\n84.21%\n87.72%\n85.96%\n87.72%\n\n38.6%\n36.11%\n42.11%\n40.35%\n45.61%\n8.77%\n10.53%\n8.77%\n12.28%\n12.28%\n14.04%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the value of n and model.By\nTable 11, we see that the optimal combination in this testing group is GPT-4o with n = 15.Fix this\nvalue for all future tests.Table 12: RAG and Combination Sampling Method Ablations\n\nRAG Improvement Nonempty Improve. Accuracy\n\nImproved Acc.Combination\n\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nbest-of-n\nbest-of-n\n\nm\n\nm′\n\n5\n3\n5\n3\n3\n5\n3\n5\n5\n3\n5\n3\n3\n5\n5\n3\nN/A 15\nN/A 15\n\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n33.78\n31.23\n31.85\n31.35\n51.32\n50.1\n57.56\n49.75\n32.71\n33.48\n\n100.0%\n100.0%\n100.0%\n100.0%\n63.64%\n65.62%\n60.61%\n59.38%\n90.62%\n84.38%\n\n50.0%\n46.88%\n50.0%\n50.0%\n48.48%\n53.12%\n54.55%\n43.75%\n56.25%\n53.12%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the combination methods and if\nRAG is enabled.By Table 12, we see that the optimal combination in this testing group is a 5-step\nreﬁnement with each iteration being a best-of-3 call, with RAG enabled.B.2 ADDITIONAL QUALITATIVE EXAMPLES\n\nIn this section, we provide qualitative examples demonstrating the improvements ImProver achieves\nin proof optimization.33.78\n31.23\n31.85\n31.35\n32.66\n32.88\n34.88\n29.54\n29.64\n28.25\n\n17\n\n\fCompﬁles: Length Optimization See (§4.3)\n\nCompﬁles: Readability Optimization Consider Figure 5, in which a lemma from the 2019 IMO\nproblem 1 (from the Compﬁles dataset) is optimized for readability.This introduces multiple new\nhypotheses, which generalize a linear_property of the functions, and then reuses and instanti-\nates that (and others, too) hypothesis throughout the proof, creating a signiﬁcantly more declaritive,\nmodular, and therefore readable proof.Original (human-written)\n\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\nrefine hf 1, fun a => ?_i\nchange g a = g 1 * a\nrw [mul_comm, ← smul_eq_mul, ←\n\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nImProver (readability-optimized)\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\n\nhave linear_property : ∀ a, f a = g a := by\n\nintro a\nrfl\n\nhave g_smul : ∀ a, g a = g 1 * a := by\n\nintro a\nrw [mul_comm, ← smul_eq_mul, ←\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nrefine hf 1, fun a => ?_i\nhave f_eq_g : f a = g a := linear_property a\nhave g_a_eq : g a = g 1 * a := g_smul a\nrw [f_eq_g, linear_property 1, g_a_eq]\n\nFigure 5: Optimizing a lemma from IMO 2019 P1 for readability\n\nMIL: Length Optimization Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, eliminating simp calls and introducing proof terms\ninto the structure of the proof to shorten it from 9 tactic invocations to 7.Original (human-written)\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\n\n:= by\n\next x\nsimp only [mem_map]\nconstructor\n· rintro hy, y_in, hyi\n\nexact hϕ y, hy, y_in, rfli, hyi\n\n· rintro hy, hz, z_in, hzi, hyi\n\nuse z, z_in\ncalc ψ.comp ϕ z = ψ (ϕ z) := rfl\n_\n= ψ y := by congr\n\nImProver (length-optimized)\n\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\nby\next x\nsimp only [mem_map]\nconstructor\nrintro hy, y_in, hyi; exact hϕ y, hy, y_in,\n\n:=\n\nrfli, hyi\n\nrintro hy, hz, z_in, hzi, hyi; exact hz,\nz_in, (congr_arg ψ hz).trans hyi\n\nFigure 6: Optimizing a lemma from the solutions of MIL CH08 S01 for length\n\nMIL: Length Optimization 2 Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, converting a full tactic proof into a single proof term\nto shorten it from 28 tactic invocations to 1.Note that the model does not have access to the Lean\ncommands that symbolically generate proof terms, and therefore generates and estimates the proof\nterm entirely by itself.MIL: Readability Optimization See (§4.3)\n\nMathlib: Length Optimization Consider Figure 8, which optimizes a theorem in algebraic topol-\nogy from mathlib for length, eliminating simp calls and combining tactics to shorten it from 3 tactic\ninvocations to 1.Mathlib: Readability Optimization Consider Figure 9, a theorem from Mathlib that we optimize\nfor readability.This original proof carried a score of 0, as it does not contain any declarative statements.It is concise\nand efﬁcient, however, it is difﬁcult to understand and read.18\n\n\fOriginal (human-written)\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\next x; constructor\n· rintro (hxs, xnti | hxt, xnsi)\n\nImProver (length-optimized)\n\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\nexact Set.ext fun x => hfun h => h.elim\n\n(fun hxs, xnti => hOr.inl xs, fun h_, xti =\n> xnt xti) (fun hxt, xnsi => hOr.inr xt,\nfun hxs, _i => xns xsi),\n\nfun hh, nxsti => h.elim (fun xs => Or.inl h\nxs, fun xt => nxst hxs, xtii) (fun xt =>\nOr.inr hxt, fun xs => nxst hxs, xtii)i\n\n· constructor\n\nleft\nexact xs\nrintro h_, xti\ncontradiction\n\n.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.This intermediate tactic greatly helps in the understandability and clarity of the proof.Original (human-written)\n\nImProver (readability-optimized)\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n:=\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]; rfl\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\n:=\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nhave h1 : ∀ x y : Y, Subsingleton\n\n(Path.Homotopic.Quotient x y) ↔ ∀ {p1 p2\n: Path x y}, Path.Homotopic p1 p2 := by\nintro x y\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]\nrfl\n\nsimp only [h1]\n\nFigure 9: Optimizing a theorem from Mathlib/FundamentalGroupoid/SimplyConnected\nfor readability\n\n19",
          "time": 927.7011940479279,
          "input_token_size": 16876,
          "output_token_size": 80,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_a75422c4",
        "created_at": 1749627219.1490438,
        "processing_time": 931.9036457538605,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "a8b40704": {
      "question": "What are the limitations of ImProver mentioned in the document?",
      "answer": "1. High cost and slow runtime.\n2. Reliance on black-box LLMs.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234
          ],
          "provenance_ids": [
            217,
            218,
            219,
            220,
            221,
            222,
            224,
            226,
            227,
            228
          ],
          "provenance": "In compar-\nison, after applying ImProver, we transformed the proof to be more declarative, with many more\nintermediate steps with explicit have tactics for improved clarity.Additionally observe how the\nmodel deﬁnes hypotheses for use in the latter half of the proof; these predeﬁned hypotheses could\neasily be converted into standalone lemmas for reuse.Example 3: Full Proof Generation.We analyze the application of ImProver to neural theorem\nproving in the MIL example from Figure 4.This theorem relating to group theory originally has no proof, however, ImProver generates one\nfrom scratch.This generated proof is veriﬁed to be correct by Lean, utilizing all the included hy-\npotheses as well as a retrieved mathlib theorem.5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.However, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance\non black-box LLM’s.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.",
          "time": 25.366076707839966,
          "input_token_size": 16187,
          "output_token_size": 85,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_a8b40704",
        "created_at": 1749769303.801609,
        "processing_time": 36.81668448448181,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "shorter_provenance",
        "sentence_count": 24,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 24,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "b0c88cd9": {
      "question": "What language is used for mathematical proofs in the document?",
      "answer": "Lean",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            26,
            27,
            28
          ],
          "provenance_ids": [
            27
          ],
          "provenance": "A typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs\nsuch as Lean’s Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024).",
          "time": 4.138163805007935,
          "input_token_size": 16811,
          "output_token_size": 8,
          "original_provenance_id": 0
        },
        {
          "provenance_id": 1,
          "input_sentence_ids": [
            23,
            24,
            25
          ],
          "provenance_ids": [
            23
          ],
          "provenance": "y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nhave h2' : Classical.choose (h1 x).exists =\n\ny :=\nh1u _ (Classical.choose_spec (h1\nx).exists)\n\nrw [h2']\nobtain hw, h1e', h1u'i := h1 y\nhave h4 := Classical.choose_spec (h1\n\nhave hxw : x = w := by\n\ny).exists\n\napply h1u'\nrw [h2]\nexact h1e\n\nrw [hxw]\nexact h1u' _ h4\n\nFigure 1: ImProver automatically rewrites formal proofs to optimize a criterion such as length\nor readability while remaining correct.",
          "time": 5.956475257873535,
          "input_token_size": 17115,
          "output_token_size": 9,
          "original_provenance_id": 1
        },
        {
          "provenance_id": 2,
          "input_sentence_ids": [
            20,
            21,
            22
          ],
          "provenance_ids": [
            20
          ],
          "provenance": "We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.",
          "time": 9.124810934066772,
          "input_token_size": 20016,
          "output_token_size": 14,
          "original_provenance_id": 2
        },
        {
          "provenance_id": 3,
          "input_sentence_ids": [
            11,
            12,
            13
          ],
          "provenance_ids": [
            13
          ],
          "provenance": "For example, formal proofs in Lean’s extensive mathematical library, Mathlib (mathlib Community,\n2020), are generally designed to be concise and very general, often at the expense of readability.",
          "time": 11.489641904830933,
          "input_token_size": 20662,
          "output_token_size": 18,
          "original_provenance_id": 3
        },
        {
          "provenance_id": 4,
          "input_sentence_ids": [
            5,
            6,
            7
          ],
          "provenance_ids": [
            7
          ],
          "provenance": "We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.",
          "time": 13.524448871612549,
          "input_token_size": 21738,
          "output_token_size": 22,
          "original_provenance_id": 4
        }
      ],
      "metadata": {
        "question_id": "paper_b0c88cd9",
        "created_at": 1750198582.990026,
        "processing_time": 16.509676218032837,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 15,
        "provenance_count": 5,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 15,
            "provenance_count": 5,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "b0d0b34b": {
      "question": "How does ImProver's performance in length optimization compare to GPT-4o?",
      "answer": "ImProver outperforms GPT-4o in length optimization by 566% in improvement score.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            38,
            50,
            134,
            136,
            137,
            138,
            139,
            141,
            142,
            153,
            159,
            160
          ],
          "provenance": "We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.Table 1: Average Proof optimization results.Improvement Nonempty Improvement Accuracy\n\nImproved Acc. recent.The model is ﬁxed as GPT-4o, CoS, output formatting, and examples are ﬁxed as the best\ncombination from the previous test, and no other features enabled.n and Model: Here, we evaluate the effects of larger n values and different models.We test n =\n3, 5, 7, 10, 15 on GPT-4o and GPT-4o-mini, as well as n = 20 for GPT-4o-mini (as it is of a far\nlower token cost).Combos and RAG: We evaluate combination methods refinement(best_of_m',m) and\nbest_of_m'(refinement(m)), for m 6= m′ with mm′ equal to the optimal value m from\nthe previous test.We also test the effect of enabling document retrieval.First focusing on the length metric, we see that ImProver outperforms GPT-\n4o with respect to the improvement score by 566% (aggregated over all datasets).Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n2.75\n18.86\n\n0.39\n5.74\n\n0.0\n6.19\n\n0.0\n4.63\n\n30.7\n54.48\n\n3.38\n24.89\n\n0.0\n53.65\n\n0.0\n33.19\n\n11.54%\n100.0%\n\n14.1%\n100.0%\n\n5.13%\n34.62%\n\n1.28%\n19.23%\n\n16.67%\n100.0%\n\n4.65%\n100.0%\n\n0.0%\n11.54%\n\n0.0%\n11.63%\n\nMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 4: Mathlib Proof optimization results. Readability optimization.",
          "time": 106.81437802314758,
          "input_token_size": 16884,
          "output_token_size": 18,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_b0d0b34b",
        "created_at": 1749769242.530634,
        "processing_time": 113.25210404396057,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "b2a67368": {
      "question": "What potential application does the document suggest for ImProver in theorem proving?",
      "answer": "ImProver is suggested to be used for automated proof optimization in theorem proving, specifically for rewriting proofs to optimize for criteria such as length, readability, and correctness.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            20,
            21,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            40,
            41,
            42,
            55,
            59,
            61,
            65,
            66,
            68,
            75,
            78,
            98,
            103,
            107,
            109,
            113,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            143,
            190,
            191,
            192,
            196,
            198,
            199,
            200,
            201,
            202,
            203,
            205,
            206,
            207,
            208,
            214,
            215,
            216,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            241,
            242,
            244,
            245,
            246,
            247,
            248,
            249,
            251,
            252,
            253,
            254,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            268,
            269,
            270,
            271,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            317,
            318,
            322,
            323,
            324,
            325,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            348,
            350,
            351,
            352,
            353,
            354,
            355,
            360,
            362,
            363,
            364,
            365,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            376
          ],
          "provenance": "4\n2\n0\n2\n \nt\nc\nO\n \n7\n \n \n]\nI\n\nA\n. s\nc\n[\n \n \n1\nv\n3\n5\n7\n4\n0\n.0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.To this end, we study a new problem of automated proof\noptimization: rewriting a proof so that it is correct and optimizes for an arbitrary\ncriterion, such as length or readability.As a ﬁrst method for automated proof opti-\nmization, we present ImProver, a large-language-model agent that rewrites proofs\nto optimize arbitrary user-deﬁned metrics in Lean.We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.In\npractice, however, informal, natural-language proofs are prone to imprecision, ambiguity, and error.Formal proofs, however, can be hard to read and often suffer from low reusability or excessive detail.For example, formal proofs in Lean’s extensive mathematical library, Mathlib (mathlib Community,\n2020), are generally designed to be concise and very general, often at the expense of readability.Formal proofs in an expository text, in contrast, may include detailed calculations steps, making\nthem readable but verbose.Machine learning systems trained on such proofs learn to mimic these\nvaried conventions (Hu et al., 2024), which may not be the optimal use of the limited supply of\nhuman-written proofs.As a result, we would like to be able to automatically refactor proofs to meet\na secondary objective such as length or readability.To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is\ncorrect and optimizes a criterion such as length or readability.We ﬁnd that naively applying LLMs\nto proof optimization falls short, often resulting in incorrect or poorly optimized proofs.We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.1\n\n\fWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more readable, and more declarative in style.1\n\nOriginal (human-written)\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\nImProver (length-optimized)\n\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\n(h1 : ∀ x, ∃!2 RELATED WORK\n\nRecently there has been wide interest in automating theorem proving in interactive proof assistants;\nsee (Lu et al., 2023; Li et al., 2024) for surveys.A typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs\nsuch as Lean’s Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024).A model learns from the distribution of proofs in the cor-\npus, such as Mathlib-style proofs.Recently, the AlphaProof (AlphaProof & Teams, 2024) system\nwas shown to produce proofs with an arcane, non-human structure and syntax.We consider the\nnew problem of rewriting a proof to optimize a metric, such as rewriting a proof into a more read-\nable or more concise one.Proof optimization is more general than theorem proving, since we can\nalso rewrite an empty proof to optimize correctness.Finally, there is a rich literature on the var-\nied styles of (human) formal proofs (e.g., (Autexier & Dietrich, 2010; Wiedijk, 2004)).Our model,\nImProver, builds on neural theorem proving techniques including full proof generation (Jiang et al.,\n2023; First et al., 2023), conditioning on example proofs (Jiang et al., 2023), retrieval (Yang et al.,\n2023; Thakur et al., 2024), and preceding ﬁle context (First et al., 2023; Hu et al., 2024), as well as\nerror correction (Madaan et al., 2023; Chen et al., 2023) and documentation retrieval (Zhou et al.,\n2023) from code generation.ImProver brings these code generation techniques, along with new\nChain-of-States prompting and meta-programmed contextual information, into a uniﬁed proof opti-\nmization agent.3 AUTOMATED PROOF OPTIMIZATION WITH ImProver\n\nAutomated Proof Optimization.Given a theorem statement x, additional context c, and an initial\nproof y0, proof optimization consists of generating a new proof y that is correct and minimizes (or\nmaximizes) a metric µ(x, c, y0, y) → R.\n\n1Code is available at https://github.com/riyazahuja/ImProver.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.Concretely, we evaluate this using the ratio of number of explicitly typed\nhave tactics to total number of tactic invocations.Completion Metric: The completion of a proof simply describes its correctness.After state extraction is completed and cached for efﬁcient future access, we annotate the proof text\nitself to contain the intermediate states in the form as comments.To analyze this hypothesis, we introduce two additional output for-\nmats to the standard str output: flat and structured.3.1.3 SAMPLING METHOD\n\nWe also introduce different methods of sampling between many (sequential or parallel) LLM infer-\nence calls, involving best-of-n and iterative reﬁnement implementations, as well as combinations\nthereof.-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.Combination Sampling and Compound Prompt Functions Compound prompt functions utilize\nthe curried nature of the implementations of best-of-n and reﬁnement to nest these techniques within\none another.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.This is a\ndataset of internationally-renowned competitive math problems, many of which are readable, yet\nquite verbose.These proofs are extremely\nefﬁcient, concise, and generalized - which often comes at the cost of readability and understandabil-\nity.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.Additionally, for a given metric, we write a prompt that brieﬂy\ndescribes the metric and the proof optimization task.First, we deﬁne improvement for length as percentage change in length, µlen(y0)−µlen(y)\n× 100.For\nreadability, we use the difference, µread(y)−µread(yo).If no correct output is generated by the model\nfor a speciﬁc theorem, improvement is deﬁned to be zero.We deﬁne nonempty improvement as\nthe improvement restricted to theorems for which some output has nonzero improvement.µlen(y0)\n\nIntuitively, improvement is the expected improvement in metric score from the input to output, ac-\ncounting for errors in the generation.The nonempty improvement score is the expected improvement\nin metric score, given that there are no errors in the generation.Similar improvement scores can be\ndeﬁned for other metrics using a binary function of the metric assigned to the original and optimized\nproofs.Model, CoS, output for-\nmatting, examples, n, and sampling method are ﬁxed as the best combination from the previous\ntest.We conclude that GPT-4o with n = 15\nis the most effective.Fixing these parameters, we consider all mixed compound sampling methods\nwith and without document retrieval enabled, concluding that a 5-step reﬁnement with best-of-3 on\neach iteration, with RAG enabled, is the optimal combination.Thus, as we can see from Table 5, the optimal parameter combination comes from gpt-4o outputting\nas a string list with CoS, RAG, 10 examples, 5-step reﬁnement with each iteration being a\nbest-of-3 evaluation.We additionally examine the effects of dis-\nabling CoS on readability optimization tasks, as the previous study focused on length optimization\ntasks, and we speculate that CoS has a high impact on the performance of readability optimization\ntasks, as the proof states that are embedded due to CoS seem to be a critical aspect to generating the\nexplicit declarations that the readability metric measures.However,\nwe also note a signiﬁcant increase in improved accuracy, which suggests that embedding the chain\nof states also improves the ability of the model to generate nontrivial correct outputs, implying that\nthe symbolic information contained in the states are critical to effectively modifying the structure\nand content of a proof.4.2.2 NEURAL THEOREM PROVING EVALUATION\n\nWe evaluate ImProver’s neural theorem proving (NTP) performance using the completion metric.We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input\nproof.Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-4o.ImProver substantially outperforms GPT-4o across all topics, with an 80% increase in accuracy\ncompared to the base model, showing that proof optimization systems are easily extendable to NTP\nsystems.9\n\n\fTable 7: Proof generation results.MIL\n\nSet Theory Group Theory\n\nOverall\n\nGPT-4o\nImProver\n\n18.18%\n45.45%\n\n25%\n33.33%\n\n21.73%\n39.13%\n\n4.3 QUALITATIVE RESULTS\n\nNext, we discuss qualitative examples showing the improvements from ImProver in proof optimiza-\ntion.These examples show the balance between correctness and the desired optimization metric,\nshowing how ImProver can produce more concise or readable proofs depending on the use case.Additional examples can be found in (§B.2)\n\nExample 1: Compﬁles Optimization.Consider Figure 1, a lemma from the 2022 IMO Question\n2 (Compﬁles) that we optimize for length.We\ndeﬁne a proof to be readable if it is written in a declarative style, which consists of intermediate\nconjectures (have · · · statements).ImProver introduces two intermediate conjectures into the proof.This original proof carried a score of 0, as it does not contain any declarative statements.Example 3: Full Proof Generation.We analyze the application of ImProver to neural theorem\nproving in the MIL example from Figure 4.This theorem relating to group theory originally has no proof, however, ImProver generates one\nfrom scratch.This generated proof is veriﬁed to be correct by Lean, utilizing all the included hy-\npotheses as well as a retrieved mathlib theorem.10\n\n\fOriginal (human-written)\n\nImProver (completeness-optimized)\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n: S ≤ T) : comap ϕ S ≤ comap ϕ T := by\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n\n: S ≤ T) : comap ϕ S ≤ comap ϕ T\n\n:= by\n\nsorry\n\nintro g\nsimp only [mem_comap]\nintro hS\nexact hST hS\n\nFigure 4: Solving a group theorem exercise from MIL Chapter 8 Section 1 for readability.5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.By incorporating CoS, RAG, and other features, ImProver signiﬁcantly outperforms base\nlanguage models in proof optimization over undergraduate, competition, and research-level prob-\nlems.However, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance\non black-box LLM’s.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.As such, we believe that ImProver sets the stage for further\nwork on proof optimization to advance the study and use of AI in mathematics.ACKNOWLEDGEMENTS\n\nRiyaz Ahuja thanks the L3 Lab and Hoskinson Center for Formal Mathematics for their support.Sean Welleck thanks Convergent Research, the Lean FRO, and the OpenAI Researcher Access Pro-\ngram for their support.11\n\n\fREFERENCES\n\nand\n\nAlphaGeometry\n\nAlphaProof\nstandard\nhttps://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\n2024.\n\nsilver-medal\nproblems.achieves\nolympiad\n\nmathematical\n\ninternational\n\nsolving\n\nTeams.AI\n\nSerge Autexier and Dominik Dietrich.A tactic language for declarative proofs.In Matt Kaufmann\nand Lawrence C. Paulson (eds.), Interactive Theorem Proving, pp.In Proceedings of the 21st Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ’98, pp.335–336,\nISBN 1581130155. doi:\nNew York, NY, USA, 1998.URL https://doi.org/10.1145/290941.291025.Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.Teaching large language models\n\nto self-debug, 2023.URL https://arxiv.org/abs/2304.05128. David Renshaw. compﬁles.https://github.com/dwrensha/compfiles, 2024.Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun.Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu.Proof artifact co-\nIn International Conference on Learning\n\ntraining for theorem proving with language models.Representations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.Jiewen Hu, Thomas Zhu, and Sean Welleck.Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu.Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs.In The Eleventh International Conference on Learning\nRepresentations, 2023.URL https://openreview.net/forum?id=SMa9EAovKMC.Kim Morrison. lean-training-data.https://github.com/kim-em/lean-training-data,\n\nGuillaume Lample, Timothee Lacroix, Marie anne Lachaux, Aurelien Rodriguez, Amaury\nHypertree proof search\nHayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.for neural\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=J4pX8Q8cxHH. theorem proving. 2024. 2024.Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie\n\nSi.Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang.A survey of deep\nlearning for mathematical reasoning.In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp.Association for Computational Linguistics.10.18653/v1/2023.acl-long.817.URL\nhttps://aclanthology.org/2023.acl-long.817.doi:\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-reﬁne:\nIterative reﬁnement with self-feedback.In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.URL https://openreview.net/forum?id=S37hOerQLB.12\n\nleanprover-community. mathematics_in_lean.https://github.com/leanprover-community/mathematics_in_lea\n\n\fThe mathlib Community.The lean mathematical library.Leonardo de Moura and Sebastian Ullrich.The lean 4 theorem prover and programming lan-\nIn Automated Deduction – CADE 28: 28th International Conference on Automated\nguage.Deduction, Virtual Event, July 12–15, 2021, Proceedings, pp.625–635, Berlin, Heidelberg,\n2021. Springer-Verlag.ISBN 978-3-030-79875-8. doi: 10.1007/978-3-030-79876-5_37.URL\nhttps://doi.org/10.1007/978-3-030-79876-5_37.OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red\nAvila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Moham-\nmad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher\nBerner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brit-\ntany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek\nChen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,\nHyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas\nDegry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,\nAdrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada\nFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel,\nTarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff\nHarris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jo-\nmoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,\nNitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik\nKim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, An-\ndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,\nStephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim\nMalfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew\nMayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil,\nDavid Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin,\nVinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeon-\nwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley\nPantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov,\nAndrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde\nde Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea\nPower, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick\nRyder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David\nSchnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah\nShoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama,\nIan Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie\nSummers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin\nTootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón\nUribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welin-\nder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,\nHannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah\nYoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,\nShengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.Gpt-4 technical\nreport, 2024. URL https://arxiv.org/abs/2303.08774.Stanislas Polu and Ilya Sutskever.Generative language modeling for automated theorem proving,\n\n2020.13\n\n\fStanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\n\nSutskever.Formal mathematics statement curriculum learning, 2022.Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, and Swarat Chaudhuri.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H.\nthought prompting elicits reasoning\nChi, Quoc V Le, and Denny Zhou.in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=_VjQlMeSB_J.Chain of\n\nFreek Wiedijk. Formal proof sketches.In Stefano Berardi, Mario Coppo, and Ferruccio Damiani\n(eds.), Types for Proofs and Programs, pp.378–393, Berlin, Heidelberg, 2004.Springer Berlin\nHeidelberg. ISBN 978-3-540-24849-1.Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil,\nRyan Prenger, and Anima Anandkumar.LeanDojo: Theorem proving with retrieval-augmented\nlanguage models.In Neural Information Processing Systems (NeurIPS), 2023.Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig.Docprompting: Gener-\nating code by retrieving the docs.For the main prompt sent to the LLM on each sample, we build a prompt string using a chat prompt\ntemplate that is then invoked at runtime to ﬁll in the variables.Namely, these variables include the set of metric prompts, previous results, input theorem, context,\na syntax documents, Mathlib documents, and examples.Each of these previous results will be wrapped with <PREV I=0></PREV\nI=0>,...,<PREV I=num_prev-1></PREV I=num_prev-1>, with I=num_prev-1 being the\nmost recent result.Remember to use lean 4 syntax, which has signiﬁcant changes from the lean 3 syntax.To\nassist with the syntax relating to the current theorem and current error messages, you will\nbe given num_syntax_docs documents to refer to for ﬁxing these syntax issues.Each of\nthese documents will be wrapped with <SYNTAX_DOC>...</SYNTAX_DOC>.Each of these will be wrapped with <CON-\nTENT_DOC>...</CONTENT_DOC>\nYou will also receive num_examples examples of input-output pairs of proofs that\nwere optimized for the metric metric.Each of these will be wrapped with <EXAM-\nPLE>...</EXAMPLE>\nYou will be given the tactic states as comments for reference.The current theorem will be\nwrapped in <CURRENT>...</CURRENT>\nSystem: Output format instructions\nPlaceholder: All retrieved syntax documentation\nPlaceholder: All retrieved mathlib documentation\nPlaceholder: All retrieved examples\nUser: <CONTEXT> context </CONTEXT>\nPlaceholder: Previous results and inputs/outputs\nPlaceholder: All metric prompts with a ‘User’ role\nUser: <CURRENT> theorem </CURRENT>\n\nThis prompt is then invoked and sent to the language model by ﬁlling in all the variables and place-\nholders.Notably, when we invoke the chain given by chain|llm|parser, we throttle the invoca-\ntion with a randomized exponential rate limit throttling to account for API rate limits, especially in\nhighly-parallelized requests like when benchmarking over a large number of theorems.A.2 METRIC PROMPTS\n\nLength Metric\n\nSystem: You are an AI assistant who shortens Lean 4 proofs while ensuring their correct-\nness.You will aim to reduce the number of lines of the tactic proof while ensuring that it\nproperly compiles in Lean 4.15\n\n\fUser: Shorten the current theorem (wrapped in <CURRENT>...</CURRENT>) to be as\nshort in length—measured in the number of lines of the proof—as possible, while also\nensuring that the output is still syntactically correct.\"Readability Metric\n\nCompletion Metric\n\nSystem: You are an AI assistant who rewrites Lean 4 proofs to be more readable while\nensuring their correctness.We measure readablity by considering the ratio of the number\nofexplicitly typed have tactics against the total number of tactics in the proof, as this is\nproportional to whether a proof is declarative in style, and thus, readable.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is\nmore readable and declarative and modular.System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.You will receive a Lean 4 proof you must\nmodify to eliminate any errors so that it compiles correctly and eliminate any “sorry”s with\nfull proofs.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a\nformal, complete, and correct Lean 4 proof by ﬁlling in its tactic proof.B ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this section, we provide more detailed information on the experimental setup and results used to\nevaluate ImProver.B.1 ABLATION DETAILS\n\nWe now proceed to show detailed results from our ablation testing.Table 8: Output and Chain-of-States Ablations\n\nOutput Format\n\nCoS\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.string\nTrue\nstring\nFalse\nstring list True\nstring list\nFalse\nstring tree\nTrue\nstring tree\nFalse\n\n7.53\n7.03\n8.04\n7.04\n7.62\n6.31\n\n16.12\n19.67\n12.38\n13.58\n15.34\n14.17\n\n46.72%\n35.77%\n64.96%\n51.82%\n49.64%\n44.53%\n\n16.79%\n15.33%\n21.17%\n18.98%\n18.25%\n16.06%\n\nBy Table 8, we see that the optimal combination in this testing group is a string list output\nformat with CoS enabled.Fix these values for all future tests.Examples\n\nTable 9: Example Retrieval Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.0\n3\n5\n7\n10\n\n5.67\n8.49\n8.38\n7.56\n9.34\n\n67.15%\n62.04%\n64.96%\n62.77%\n63.5%\n\n16.79%\n19.71%\n21.17%\n19.71%\n21.9%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the number of examples.Fix this value for all future\ntests.None\nreﬁnement\nreﬁnement\nreﬁnement\nreﬁnement\nbest-of-n\n\nN/A\n1\n5\n1\n5\nN/A\n\nN/A\nFalse\nFalse\nTrue\nTrue\nN/A\n\n9.34\n14.76\n12.5\n14.95\n13.15\n15.35\n\n14.7\n30.63\n20.88\n14.95\n13.15\n18.44\n\n63.5%\n48.18%\n59.85%\n100.0%\n100.0%\n83.21%\n\n21.9%\n30.66%\n30.66%\n30.66%\n29.93%\n36.5%\n\nNote that forward and keep-best values are parameters for reﬁnement of how many previous itera-\ntions to forward, and whether to keep the most recent or the best iteration in subsequent reﬁnement\nsteps.Now, with the previous optimal parameters ﬁxed, run the ablation on the sample method.By\nTable 10, we see that the optimal combination in this testing group is best-of-n.Fix this value\nfor all future tests.Table 11: Model and n Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.Model\n\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\n\nn\n\n3\n5\n7\n10\n15\n3\n5\n7\n10\n15\n20\n\n19.66\n20.12\n22.44\n21.73\n23.51\n3.65\n5.12\n3.65\n4.99\n4.35\n4.87\n\n24.36\n24.97\n27.21\n25.28\n26.28\n4.63\n6.21\n4.34\n5.69\n5.06\n5.56\n\n80.7%\n80.56%\n82.46%\n85.96%\n89.47%\n78.95%\n82.46%\n84.21%\n87.72%\n85.96%\n87.72%\n\n38.6%\n36.11%\n42.11%\n40.35%\n45.61%\n8.77%\n10.53%\n8.77%\n12.28%\n12.28%\n14.04%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the value of n and model.By Table 12, we see that the optimal combination in this testing group is a 5-step\nreﬁnement with each iteration being a best-of-3 call, with RAG enabled.33.78\n31.23\n31.85\n31.35\n32.66\n32.88\n34.88\n29.54\n29.64\n28.25\n\n17\n\n\fCompﬁles: Length Optimization See (§4.3)\n\nCompﬁles: Readability Optimization Consider Figure 5, in which a lemma from the 2019 IMO\nproblem 1 (from the Compﬁles dataset) is optimized for readability.This introduces multiple new\nhypotheses, which generalize a linear_property of the functions, and then reuses and instanti-\nates that (and others, too) hypothesis throughout the proof, creating a signiﬁcantly more declaritive,\nmodular, and therefore readable proof.Original (human-written)\n\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\nrefine hf 1, fun a => ?_i\nchange g a = g 1 * a\nrw [mul_comm, ← smul_eq_mul, ←\n\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nImProver (readability-optimized)\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\n\nhave linear_property : ∀ a, f a = g a := by\n\nintro a\nrfl\n\nhave g_smul : ∀ a, g a = g 1 * a := by\n\nintro a\nrw [mul_comm, ← smul_eq_mul, ←\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nrefine hf 1, fun a => ?_i\nhave f_eq_g : f a = g a := linear_property a\nhave g_a_eq : g a = g 1 * a := g_smul a\nrw [f_eq_g, linear_property 1, g_a_eq]\n\nFigure 5: Optimizing a lemma from IMO 2019 P1 for readability\n\nMIL: Length Optimization Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, eliminating simp calls and introducing proof terms\ninto the structure of the proof to shorten it from 9 tactic invocations to 7.Original (human-written)\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\n\n:= by\n\next x\nsimp only [mem_map]\nconstructor\n· rintro hy, y_in, hyi\n\nexact hϕ y, hy, y_in, rfli, hyi\n\n· rintro hy, hz, z_in, hzi, hyi\n\nuse z, z_in\ncalc ψ.comp ϕ z = ψ (ϕ z) := rfl\n_\n= ψ y := by congr\n\nImProver (length-optimized)\n\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\nby\next x\nsimp only [mem_map]\nconstructor\nrintro hy, y_in, hyi; exact hϕ y, hy, y_in,\n\n:=\n\nrfli, hyi\n\nrintro hy, hz, z_in, hzi, hyi; exact hz,\nz_in, (congr_arg ψ hz).trans hyi\n\nFigure 6: Optimizing a lemma from the solutions of MIL CH08 S01 for length\n\nMIL: Length Optimization 2 Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, converting a full tactic proof into a single proof term\nto shorten it from 28 tactic invocations to 1.MIL: Readability Optimization See (§4.3)\n\nMathlib: Length Optimization Consider Figure 8, which optimizes a theorem in algebraic topol-\nogy from mathlib for length, eliminating simp calls and combining tactics to shorten it from 3 tactic\ninvocations to 1.Mathlib: Readability Optimization Consider Figure 9, a theorem from Mathlib that we optimize\nfor readability.This original proof carried a score of 0, as it does not contain any declarative statements.It is concise\nand efﬁcient, however, it is difﬁcult to understand and read.18\n\n\fOriginal (human-written)\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\next x; constructor\n· rintro (hxs, xnti | hxt, xnsi)\n\nImProver (length-optimized)\n\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\nexact Set.ext fun x => hfun h => h.elim\n\n(fun hxs, xnti => hOr.inl xs, fun h_, xti =\n> xnt xti) (fun hxt, xnsi => hOr.inr xt,\nfun hxs, _i => xns xsi),\n\nfun hh, nxsti => h.elim (fun xs => Or.inl h\nxs, fun xt => nxst hxs, xtii) (fun xt =>\nOr.inr hxt, fun xs => nxst hxs, xtii)i\n\n· constructor\n\nleft\nexact xs\nrintro h_, xti\ncontradiction\n\n.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.Original (human-written)\n\nImProver (readability-optimized)\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n:=\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]; rfl\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\n:=\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nhave h1 : ∀ x y : Y, Subsingleton\n\n(Path.Homotopic.Quotient x y) ↔ ∀ {p1 p2\n: Path x y}, Path.Homotopic p1 p2 := by\nintro x y\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]\nrfl\n\nsimp only [h1]\n\nFigure 9: Optimizing a theorem from Mathlib/FundamentalGroupoid/SimplyConnected\nfor readability\n\n19",
          "time": 352.7621581554413,
          "input_token_size": 16880,
          "output_token_size": 54,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_b2a67368",
        "created_at": 1750198728.7673655,
        "processing_time": 356.6955690383911,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "bbcf8aaf": {
      "question": "Who are the authors of the document?",
      "answer": "Riyaz Ahuja, Jeremy Avigad, Prasad Tetali, Sean Welleck",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4
          ],
          "provenance_ids": [
            1
          ],
          "provenance": "0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.",
          "time": 8.3588285446167,
          "input_token_size": 16089,
          "output_token_size": 127,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_bbcf8aaf",
        "created_at": 1749869872.7468464,
        "processing_time": 14.587307214736938,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "shorter_provenance",
        "sentence_count": 5,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 5,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "c30336a1": {
      "question": "What technique does ImProver use to annotate each proof state?",
      "answer": "Chain-of-States technique.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            5,
            6,
            7
          ],
          "provenance_ids": [
            7
          ],
          "provenance": "We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.",
          "time": 5.37372612953186,
          "input_token_size": 17936,
          "output_token_size": 44,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_c30336a1",
        "created_at": 1750198590.5348346,
        "processing_time": 10.763736248016357,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 3,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 3,
            "provenance_count": 1,
            "has_answer": true
          },
          {
            "source": "test_outputs_prev1",
            "sentence_count": 3,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "d501b04b": {
      "question": "What method does ImProver use for selecting relevant examples?",
      "answer": "MMR (Maximum Marginal Relevance)-based retrieval.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            82,
            83,
            84
          ],
          "provenance_ids": [
            82,
            83
          ],
          "provenance": "3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.More speciﬁcally, example retrieval selects the most relevant user-generated examples of proof op-\ntimization on a speciﬁc metric.",
          "time": 6.715828895568848,
          "input_token_size": 19662,
          "output_token_size": 106,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_d501b04b",
        "created_at": 1750198627.1507993,
        "processing_time": 12.62693166732788,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 3,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 3,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "dae7b203": {
      "question": "What is one key component of ImProver's methodology?",
      "answer": "Chain-of-States prompting",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93
          ],
          "provenance_ids": [
            42,
            44,
            45,
            47,
            48,
            53,
            55,
            56,
            57,
            58,
            62,
            63,
            64,
            65
          ],
          "provenance": "Completion Metric: The completion of a proof simply describes its correctness.The completion metric is used for concretely\nviewing proof optimization as a generalization of neural theorem proving.3.1\n\nIMPROVER\n\nWe develop several improvements that can be applied to a black-box LLM generator yout ∼\nG(·|xin), such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these pa-\nrameters.3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.The intermediate states often contain valuable information (e.g., an expression\nafter it has been simpliﬁed) that is not present in the tactics.Speciﬁcally, in the compiler’s elaboration and eval-\nuation stages – where the parsed theorem code is ﬁrst converted into concrete syntax trees (in\npractice, Syntax objects) and abstract syntax trees (Expr objects) – we convert the CST and\nAST output objects into the relevant proof data and proof states in the form of proof trees\n(Lean.Elab.InfoTree).After state extraction is completed and cached for efﬁcient future access, we annotate the proof text\nitself to contain the intermediate states in the form as comments.Figure 2 shows an example. This explicit reasoning aims to help the generator model construct more optimized proofs via addi-\ntional symbolic data.3.1.2 OUTPUT FORMATTING. LLM outputs often contain ancillary and syntactically invalid content, especially before and after\nthe actual proof.Additionally, by applying additional structure to the LLM outputs, we may hope to\ngenerate more structured proofs.3\n\n\fWithout Chain-of-States\n\nWith Chain-of-States\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u) := by\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u)\n\n:= by\n\nrintro x (hxs, xti | hxs, xui)\n· use xs; left; exact xt\n.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!-/\n. use xs; right; exact xu\n/-\nGoals Solved!-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).",
          "time": 24.58993434906006,
          "input_token_size": 15990,
          "output_token_size": 24,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_dae7b203",
        "created_at": 1750198684.9771862,
        "processing_time": 29.25039052963257,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 94,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 94,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "de13457b": {
      "question": "What datasets were used to evaluate ImProver, and why are they significant?",
      "answer": "1. Mathematics in Lean (MIL)\n2. Compfiles\n3. Mathlib",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116
          ],
          "provenance_ids": [
            98,
            99,
            102,
            106
          ],
          "provenance": "Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.We use exercise solutions from set theory, elementary number theory, group\ntheory, topology, differential calculus, and integration & measure theory.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.",
          "time": 29.64428973197937,
          "input_token_size": 17419,
          "output_token_size": 230,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_de13457b",
        "created_at": 1749869944.7185235,
        "processing_time": 34.3648521900177,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "shorter_provenance",
        "sentence_count": 23,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 23,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "e4775d20": {
      "question": "Which retrieval method is used by ImProver?",
      "answer": "MMR (Maximum Marginal Relevance)-based retrieval.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187
          ],
          "provenance_ids": [
            8,
            9,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            40,
            41,
            42,
            43,
            44,
            45,
            47,
            51,
            52,
            54,
            58,
            59,
            60,
            63,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            77,
            80,
            81,
            82,
            84,
            103,
            106,
            107,
            109,
            110,
            111,
            112,
            113,
            114,
            117,
            125,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            142,
            143,
            144,
            145,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            165,
            166,
            167,
            168,
            169,
            170,
            172,
            173,
            174,
            175,
            176,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187
          ],
          "provenance": "We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.ImProver’s optimized proof is correct and more concise.2 RELATED WORK\n\nRecently there has been wide interest in automating theorem proving in interactive proof assistants;\nsee (Lu et al., 2023; Li et al., 2024) for surveys.A typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs\nsuch as Lean’s Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024).A model learns from the distribution of proofs in the cor-\npus, such as Mathlib-style proofs.Recently, the AlphaProof (AlphaProof & Teams, 2024) system\nwas shown to produce proofs with an arcane, non-human structure and syntax.We consider the\nnew problem of rewriting a proof to optimize a metric, such as rewriting a proof into a more read-\nable or more concise one.Proof optimization is more general than theorem proving, since we can\nalso rewrite an empty proof to optimize correctness.Finally, there is a rich literature on the var-\nied styles of (human) formal proofs (e.g., (Autexier & Dietrich, 2010; Wiedijk, 2004)).Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.Concretely, we evaluate this using the ratio of number of explicitly typed\nhave tactics to total number of tactic invocations.Completion Metric: The completion of a proof simply describes its correctness.This is a trivial\nmetric which measures the number of errors present.The completion metric is used for concretely\nviewing proof optimization as a generalization of neural theorem proving.3.1\n\nIMPROVER\n\nWe develop several improvements that can be applied to a black-box LLM generator yout ∼\nG(·|xin), such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these pa-\nrameters.3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.The implementation of this extraction system is mod-\neled from the work (Kim Morrison, 2024).These proof trees contain detailed context and information on a tactic-\nby-tactic level relating to the modiﬁcation of the proof state, metavariable context, and proof cor-\nrectness.Additionally, by applying additional structure to the LLM outputs, we may hope to\ngenerate more structured proofs.To analyze this hypothesis, we introduce two additional output for-\nmats to the standard str output: flat and structured.The former enforces a tactic sequence\noutput as a list of strings, and the latter enforces a proof tree output as a tree of strings.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.More speciﬁcally, our scoring function is given by the 2-ary\ncomparison function S, whose arguments are output objects y, y′.max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.This comparison function can be extended to evaluate the best output of any ﬁnite n via induction.This best-of-n technique is implemented as a curried function such that each of the n calls can be\nhandled by any arbitrary sampling method, or just a single standard prompt at user discretion.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.However, unlike best-of-n, there is\nno opportunity for parallelism as each iteration is dependent on information from the previous call.refinement((best_of_n,m),n) is a compound sampling method that runs a n-step reﬁne-\nment, where each call is a best-of-m call to the LLM.Note that with each of these compound prompt functions, there are always a total of mn iterations.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.Namely, each metric is loaded with a cached (vector) database\npopulated with human-made examples of preoptimized and postoptimized pairs of Lean theorems.This is a\ndataset of internationally-renowned competitive math problems, many of which are readable, yet\nquite verbose.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.These proofs are extremely\nefﬁcient, concise, and generalized - which often comes at the cost of readability and understandabil-\nity.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.This is the most difﬁcult dataset. Models.Our base generator uses GPT-4o (OpenAI et al., 2024).Since no prior methods currently\nexist for automated proof optimization, we consider a prompted GPT-4o without the improvements\ndescribed in (§3.1) as our baseline.Additionally, for a given metric, we write a prompt that brieﬂy\ndescribes the metric and the proof optimization task.We also provide instructions, context, and in-\nformation depending on the features selected, and add the theorem and proof to the prompt.First, we deﬁne improvement for length as percentage change in length, µlen(y0)−µlen(y)\n× 100.The improved accuracy is the percentage of theorems in the dataset\nwhich the model was able to generate a correct output for, as well as improve the metric to be\nnonzero.However, as there are over 8640\npossible combinations, it is inefﬁcient to test all combinations at once.As such, we evaluate using a\nfactorial testing method. Testing Groups.We deﬁne the following testing groups with the speciﬁed parameter combina-\ntions:\n\nGPT-4o-mini/GPT-4o: This varies the GPT-4o model, outputting a string with no other features.Output and CoS: We evaluate the effects of different output formatting styles (string, string\nlist, string tree) and CoS (True, False), with the model ﬁxed as GPT-4o, with no other\nfeatures enabled.Example Retrieval: We evaluate the effects of increasing the number of examples provided (multi-\nshot prompting) in the range of 0, 3, 5, 7, and 10, with the model ﬁxed as GPT-4o, CoS and output\nformatting ﬁxed as the best combination from the previous test, and no other features enabled.Sampling Method: Here, we evaluate the effects of best-of-n and reﬁnement for a ﬁxed n = 5.Additionally we test on the reﬁnement cases if forwarding the most recent iteration result, or all\nprevious iteration results is the best, and if we should keep the best out of the iterations, or the most\n\n6\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 1: Average Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nMetric\n\nModel\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n3.7\n20.96\n\n2.21\n9.34\n\n6.25\n30.54\n\n4.18\n13.45\n\n15.15\n55.29\n\n8.02\n30.53\n\n18.58\n56.56\n\n14.48\n30.97\n\n26.36%\n100.0%\n\n18.75%\n100.0%\n\n8.31%\n35.44%\n\n6.13 %\n24.56%\n\n37.5%\n100.0%\n\n28.85%\n100.0%\n\n14.42%\n50.0%\n\n11.54%\n34.21%\n\nTable 2: MIL Proof optimization results.Improvement Nonempty Improvement Accuracy\n\nImproved Acc. recent.The model is ﬁxed as GPT-4o, CoS, output formatting, and examples are ﬁxed as the best\ncombination from the previous test, and no other features enabled.n and Model: Here, we evaluate the effects of larger n values and different models.We test n =\n3, 5, 7, 10, 15 on GPT-4o and GPT-4o-mini, as well as n = 20 for GPT-4o-mini (as it is of a far\nlower token cost).CoS, output formatting, examples, and sampling method are ﬁxed as the best\ncombination from the previous test, and no other features enabled.We also test the effect of enabling document retrieval.Model, CoS, output for-\nmatting, examples, n, and sampling method are ﬁxed as the best combination from the previous\ntest.Ablation data. We evaluate our ablations on a subset of MIL.However, due to the increase in\nmodel calls for larger n values, we switch a representative sample of this subset for some test groups.From Table 2, Table 3, and Table 4,\nwe can see that ImProver is capable of optimizing proofs on all datasets for both the length and\nreadability metrics.Furthermore, Table 1 shows that across all metrics, ImProver signiﬁcantly\noutperforms GPT-4o on proof optimization tasks on every experimental measure – aggregated from\nall datasets.Additionally, from Table 2, Table 3, and Table 4, we can see that ImProver outperforms\nGPT-4o on each dataset as well.We proceed to analyze this data and its implications. Length optimization.First focusing on the length metric, we see that ImProver outperforms GPT-\n4o with respect to the improvement score by 566% (aggregated over all datasets).Additionally, we\nare guaranteed that ImProver produces a correct output, although that output may just be the same\nas the input.However, 35.44% of the time, it generates a correct output that is not the same length\nas the input, and in that case, we expect an average of a 55.29% reduction in length.Comparing\nthis with GPT-4o, we conclude that not only can ImProver optimize at a higher level on arbitrary\ntheorems, but its ability to generate nontrivial correct outputs is far greater in comparison to GPT-4o.7\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 3: Compﬁles Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n2.75\n18.86\n\n0.39\n5.74\n\n0.0\n6.19\n\n0.0\n4.63\n\n30.7\n54.48\n\n3.38\n24.89\n\n0.0\n53.65\n\n0.0\n33.19\n\n11.54%\n100.0%\n\n14.1%\n100.0%\n\n5.13%\n34.62%\n\n1.28%\n19.23%\n\n16.67%\n100.0%\n\n4.65%\n100.0%\n\n0.0%\n11.54%\n\n0.0%\n11.63%\n\nMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 4: Mathlib Proof optimization results. Readability optimization.Readability optimization is similar, with ImProver outperforming\nGPT-4o by 423%.Moreover, the accuracy, improved accuracy, and nonempty improvement dis-\nparities for readability parallel those of the length tests.However, it should be noted that for both\nGPT-4o and ImProver, the accuracy and improved accuracy scores were markedly smaller for read-\nability than length optimization.In other words, optimizing for\nreadability is more difﬁcult for the underlying generator than optimizing for length.However, we\nspeculate with higher-quality prompts, descriptions of the metric, and examples, this disparity can\nbe minimized.Regardless, we note that different metrics can be less likely to be correctly optimized,\nand that model performance is correlated with the metric it seeks to optimize – both for GPT-4o and\nImProver.Optimization varies based on dataset difﬁculty.Additionally noting Table 2, Table 3, and\nTable 4, we observe that the improvement score for both metrics for both GPT-4o and ImProver\nis highest for the MIL dataset, lower for Compﬁles, and the lowest on the Mathlib theorems.This\nsuggests that the expected improvement in metric score decreases with higher difﬁcultly – with\nundergraduate-level theorems having a signiﬁcantly higher expected improvement than research-\nlevel theorems.Similarly, the accuracy and improved accuracy scores for both metrics and\nmodels decreased with higher difﬁculty datasets (disregarding ImProver’s accuracy scores, as they\nare ensured to be 100%).This suggests that although the base GPT-4o generator is less likely to\ngenerate a correct output for higher difﬁculty datasets, the improvements that ImProver makes to\nthe base generator allows it to maintain its improvement in the metric score whenever a correct out-\nput is generated.As such, we can speculate that the bottleneck in the improvement score is not the\nmodel’s ability to optimize the proof for a metric, but rather its ability to generate a new correct\nproof at all.As such, we conjecture that with more capable generator models, the accuracy – and\nthus, the improvement score – in optimization tasks will continue to increase, until the improvement\nscores match the nonempty improvement.Overall, we conclude that although the performance of both ImProver and GPT-4o decreases on\nlength and readability optimization on more difﬁcult datasets, ImProver signiﬁcantly outperforms\nGPT-4o on all datasets for length and readability optimization.We measure the baseline results from the GPT-\n4o and GPT-4o-mini models, noting that GPT-4o is the better-scoring model (with respect to the\nimprovement score).Thus, ﬁxing this model, we vary the output formatting type and if CoS is\nenabled, and determine that outputting flat with CoS enabled maximizes the improvement score.8\n\n\fTable 5: Ablation results.Each cell in the ablation tests shows best / worst, which are the best\nand worst parameter combinations in the test group.The ImProver speciﬁcation outputs the input\ntheorem when no correct proof is generated, which results in an accuracy of 100% on MIL.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o-mini\nGPT-4o\n+ Output and CoS\n+ Example Retrieval\n+ Sampling Method\n+ n and Model\n+ Combos and RAG\nImProver\n\n0\n7.03\n8.04 / 6.31\n9.34 / 5.67\n15.35 / 9.34\n23.51 / 3.65\n34.88 / 28.25\n\n0\n19.67\n12.38 / 14.17\n14.7 / 8.44\n18.44 / 14.7\n26.28 / 4.63\n57.56 / 33.48\n\n3.62%\n35.77%\n\n0%\n15.33%\n\n64.96% / 44.53% 21.17% / 16.06%\n63.5% / 67.15%\n21.9% / 16.79%\n36.5% / 21.9%\n83.21% / 63.5%\n89.47% / 78.95% 45.61% / 8.77%\n60.61% / 84.38% 54.55% / 53.12%\n\n34.88\n\n57.56\n\n100%\n\n54.55%\n\nTable 6: CoS Readability Ablation results.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o\nImProver, CoS Disabled\nImProver\n\n4.97\n9.23\n16.69\n\n15.89\n24.61\n31.42\n\n37.5%\n100.0%\n100.0%\n\n12.5%\n28.12%\n46.88%\n\nFixing these parameters, we now vary the number of examples retrieved, noting that prompting\nwith 10 examples maximizes the improvement score.",
          "time": 98.63606142997742,
          "input_token_size": 15441,
          "output_token_size": 23,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_e4775d20",
        "created_at": 1750198663.491287,
        "processing_time": 105.1656858921051,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 188,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 188,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "e487be80": {
      "question": "How does ImProver address the issue of incorrect proofs generated by naive LLMs?",
      "answer": "ImProver addresses the issue of incorrect proofs generated by naive LLMs by incorporating various improvements such as the use of symbolic Lean context in a novel Chain-of-States technique, as well as error-correction and retrieval.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            3,
            4,
            6,
            7,
            8,
            9,
            345,
            361,
            366,
            367,
            368,
            369,
            370,
            371,
            372
          ],
          "provenance": "For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.As a ﬁrst method for automated proof opti-\nmization, we present ImProver, a large-language-model agent that rewrites proofs\nto optimize arbitrary user-deﬁned metrics in Lean.We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.Examples\n\nTable 9: Example Retrieval Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.B.2 ADDITIONAL QUALITATIVE EXAMPLES\n\nIn this section, we provide qualitative examples demonstrating the improvements ImProver achieves\nin proof optimization.Note that the model does not have access to the Lean\ncommands that symbolically generate proof terms, and therefore generates and estimates the proof\nterm entirely by itself.MIL: Readability Optimization See (§4.3)\n\nMathlib: Length Optimization Consider Figure 8, which optimizes a theorem in algebraic topol-\nogy from mathlib for length, eliminating simp calls and combining tactics to shorten it from 3 tactic\ninvocations to 1.Mathlib: Readability Optimization Consider Figure 9, a theorem from Mathlib that we optimize\nfor readability.This original proof carried a score of 0, as it does not contain any declarative statements.It is concise\nand efﬁcient, however, it is difﬁcult to understand and read.18\n\n\fOriginal (human-written)\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\next x; constructor\n· rintro (hxs, xnti | hxt, xnsi)\n\nImProver (length-optimized)\n\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\nexact Set.ext fun x => hfun h => h.elim\n\n(fun hxs, xnti => hOr.inl xs, fun h_, xti =\n> xnt xti) (fun hxt, xnsi => hOr.inr xt,\nfun hxs, _i => xns xsi),\n\nfun hh, nxsti => h.elim (fun xs => Or.inl h\nxs, fun xt => nxst hxs, xtii) (fun xt =>\nOr.inr hxt, fun xs => nxst hxs, xtii)i\n\n· constructor\n\nleft\nexact xs\nrintro h_, xti\ncontradiction\n\n.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.",
          "time": 84.0038697719574,
          "input_token_size": 16886,
          "output_token_size": 99,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_e487be80",
        "created_at": 1749869937.579627,
        "processing_time": 87.41699504852295,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.835491"
      }
    }
  },
  {
    "e6228b89": {
      "question": "What are the key components of the Chain-of-States technique used by ImProver?",
      "answer": "1. Chain-of-States prompting\n2. Symbolic Lean context extraction\n3. Intermediate proof states annotation\n4. Error-correction\n5. Retrieval-augmented generation (RAG)",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance": "4\n2\n0\n2\n \nt\nc\nO\n \n7\n \n \n]\nI\n\nA\n. s\nc\n[\n \n \n1\nv\n3\n5\n7\n4\n0\n.0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.To this end, we study a new problem of automated proof\noptimization: rewriting a proof so that it is correct and optimizes for an arbitrary\ncriterion, such as length or readability.As a ﬁrst method for automated proof opti-\nmization, we present ImProver, a large-language-model agent that rewrites proofs\nto optimize arbitrary user-deﬁned metrics in Lean.We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.In\npractice, however, informal, natural-language proofs are prone to imprecision, ambiguity, and error.Using a formal language such as Lean (Moura & Ullrich, 2021) removes ambiguity and precision\nand enables a proof assistant to verify correctness down to the primitives of a formal axiomatic\nsystem.Formal proofs, however, can be hard to read and often suffer from low reusability or excessive detail.For example, formal proofs in Lean’s extensive mathematical library, Mathlib (mathlib Community,\n2020), are generally designed to be concise and very general, often at the expense of readability.Formal proofs in an expository text, in contrast, may include detailed calculations steps, making\nthem readable but verbose.Machine learning systems trained on such proofs learn to mimic these\nvaried conventions (Hu et al., 2024), which may not be the optimal use of the limited supply of\nhuman-written proofs.As a result, we would like to be able to automatically refactor proofs to meet\na secondary objective such as length or readability.To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is\ncorrect and optimizes a criterion such as length or readability.We ﬁnd that naively applying LLMs\nto proof optimization falls short, often resulting in incorrect or poorly optimized proofs.We develop\nvarious improvements that can be applied on top of a black-box language model, including Chain-\nof-States prompting–an analogy to chain-of-thought prompting (Wei et al., 2022) that shows inter-\nmediate proof states–along with error-correction and retrieval.We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.1\n\n\fWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more readable, and more declarative in style.1\n\nOriginal (human-written)\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\nImProver (length-optimized)\n\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nrw [h1u _ (Classical.choose_spec _)]\nobtain hw, h1e', h1u'i := h1 y\nrw [h1u' _ ((h2 _ _).mpr h1e)]\nexact h1u' _ (Classical.choose_spec _)\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nhave h2' : Classical.choose (h1 x).exists =\n\ny :=\nh1u _ (Classical.choose_spec (h1\nx).exists)\n\nrw [h2']\nobtain hw, h1e', h1u'i := h1 y\nhave h4 := Classical.choose_spec (h1\n\nhave hxw : x = w := by\n\ny).exists\n\napply h1u'\nrw [h2]\nexact h1e\n\nrw [hxw]\nexact h1u' _ h4\n\nFigure 1: ImProver automatically rewrites formal proofs to optimize a criterion such as length\nor readability while remaining correct.In this example, ImProver optimizes a human-written\nlemma (right) from the 2022 International Math Olympiad (Question 2, solution from Comp-\nﬁles (David Renshaw, 2024)) for length.ImProver’s optimized proof is correct and more concise.2 RELATED WORK\n\nRecently there has been wide interest in automating theorem proving in interactive proof assistants;\nsee (Lu et al., 2023; Li et al., 2024) for surveys.A typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs\nsuch as Lean’s Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024).A model learns from the distribution of proofs in the cor-\npus, such as Mathlib-style proofs.Recently, the AlphaProof (AlphaProof & Teams, 2024) system\nwas shown to produce proofs with an arcane, non-human structure and syntax.We consider the\nnew problem of rewriting a proof to optimize a metric, such as rewriting a proof into a more read-\nable or more concise one.Proof optimization is more general than theorem proving, since we can\nalso rewrite an empty proof to optimize correctness.Finally, there is a rich literature on the var-\nied styles of (human) formal proofs (e.g., (Autexier & Dietrich, 2010; Wiedijk, 2004)).Our model,\nImProver, builds on neural theorem proving techniques including full proof generation (Jiang et al.,\n2023; First et al., 2023), conditioning on example proofs (Jiang et al., 2023), retrieval (Yang et al.,\n2023; Thakur et al., 2024), and preceding ﬁle context (First et al., 2023; Hu et al., 2024), as well as\nerror correction (Madaan et al., 2023; Chen et al., 2023) and documentation retrieval (Zhou et al.,\n2023) from code generation.ImProver brings these code generation techniques, along with new\nChain-of-States prompting and meta-programmed contextual information, into a uniﬁed proof opti-\nmization agent.3 AUTOMATED PROOF OPTIMIZATION WITH ImProver\n\nAutomated Proof Optimization.Given a theorem statement x, additional context c, and an initial\nproof y0, proof optimization consists of generating a new proof y that is correct and minimizes (or\nmaximizes) a metric µ(x, c, y0, y) → R.\n\n1Code is available at https://github.com/riyazahuja/ImProver.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.We consider 3 metrics:\n\nLength Metric: The length metric measures the number of tactic invocations in the tactic proof,\naiming to reduce the proof’s length while ensuring its correctness.Note that shorter proofs often\nrepresent more efﬁcient proofs.Readability Metric: We consider a proof to be readable if it\nis written in a declarative\nstyle (Autexier & Dietrich, 2010; Wiedijk, 2004), which is related to the number of independent\nsubproofs in a proof.Concretely, we evaluate this using the ratio of number of explicitly typed\nhave tactics to total number of tactic invocations.Completion Metric: The completion of a proof simply describes its correctness.This is a trivial\nmetric which measures the number of errors present.The completion metric is used for concretely\nviewing proof optimization as a generalization of neural theorem proving.3.1\n\nIMPROVER\n\nWe develop several improvements that can be applied to a black-box LLM generator yout ∼\nG(·|xin), such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these pa-\nrameters.The explicit prompts and templates that are sent to the LLM can be found in (§A).3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.The intermediate states often contain valuable information (e.g., an expression\nafter it has been simpliﬁed) that is not present in the tactics.To allow the model to reason about\nthese intermediate goals and hypotheses, we use tools from Lean metaprogramming to automatically\nannotate each proof state as a comment prior to each tactic.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.The implementation of this extraction system is mod-\neled from the work (Kim Morrison, 2024).Speciﬁcally, in the compiler’s elaboration and eval-\nuation stages – where the parsed theorem code is ﬁrst converted into concrete syntax trees (in\npractice, Syntax objects) and abstract syntax trees (Expr objects) – we convert the CST and\nAST output objects into the relevant proof data and proof states in the form of proof trees\n(Lean.Elab.InfoTree).These proof trees contain detailed context and information on a tactic-\nby-tactic level relating to the modiﬁcation of the proof state, metavariable context, and proof cor-\nrectness.After state extraction is completed and cached for efﬁcient future access, we annotate the proof text\nitself to contain the intermediate states in the form as comments.Figure 2 shows an example. This explicit reasoning aims to help the generator model construct more optimized proofs via addi-\ntional symbolic data.3.1.2 OUTPUT FORMATTING. LLM outputs often contain ancillary and syntactically invalid content, especially before and after\nthe actual proof.Additionally, by applying additional structure to the LLM outputs, we may hope to\ngenerate more structured proofs.To analyze this hypothesis, we introduce two additional output for-\nmats to the standard str output: flat and structured.The former enforces a tactic sequence\noutput as a list of strings, and the latter enforces a proof tree output as a tree of strings.3.1.3 SAMPLING METHOD\n\nWe also introduce different methods of sampling between many (sequential or parallel) LLM infer-\nence calls, involving best-of-n and iterative reﬁnement implementations, as well as combinations\nthereof.3\n\n\fWithout Chain-of-States\n\nWith Chain-of-States\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u) := by\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u)\n\n:= by\n\nrintro x (hxs, xti | hxs, xui)\n· use xs; left; exact xt\n.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!-/\n. use xs; right; exact xu\n/-\nGoals Solved!-/\n\nFigure 2: A Lean proof (left) with Chain-of-States prompting annotations (right).Best-of-n The best-of-n technique generates multiple (n) calls to the language model and selects\nthe “best” via a simple selection policy that ﬁrst prioritizes output correctness, and secondly priori-\ntizes the evaluated metric delta score.More speciﬁcally, our scoring function is given by the 2-ary\ncomparison function S, whose arguments are output objects y, y′.max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.This comparison function can be extended to evaluate the best output of any ﬁnite n via induction.This best-of-n technique is implemented as a curried function such that each of the n calls can be\nhandled by any arbitrary sampling method, or just a single standard prompt at user discretion.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.Error correction and Reﬁnement\nInspired by self-debugging techniques in code genera-\ntion (Madaan et al., 2023; Chen et al., 2023), ImProver identiﬁes and corrects errors in the gen-\nerated proofs by iteratively reﬁning its outputs.The reﬁnement process relies on user-deﬁned pa-\nrameters n and prev_num to specify the number of iterations and the number of previous iteration\ninfo to forward, respectively.Each iteration carries information on the last prev_num iterations,\nincluding input, output, metric score, correctness, and error messages.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.Similar to the best-of-n tech-\nnique, it relies on an argument n for the number of reﬁnement steps, and is curried such that each\nreﬁnement step can be handled by any other prompting function.However, unlike best-of-n, there is\nno opportunity for parallelism as each iteration is dependent on information from the previous call.Combination Sampling and Compound Prompt Functions Compound prompt functions utilize\nthe curried nature of the implementations of best-of-n and reﬁnement to nest these techniques within\none another.For example:\n\n4\n\n\fbest_of_n((refinement,m),n) is a compound sampling method that run a best-of-n,\nwhere each call is a m-step reﬁnement.refinement((best_of_n,m),n) is a compound sampling method that runs a n-step reﬁne-\nment, where each call is a best-of-m call to the LLM.Note that with each of these compound prompt functions, there are always a total of mn iterations.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.More speciﬁcally, example retrieval selects the most relevant user-generated examples of proof op-\ntimization on a speciﬁc metric.Namely, each metric is loaded with a cached (vector) database\npopulated with human-made examples of preoptimized and postoptimized pairs of Lean theorems.The number of examples that are retrieved is user-speciﬁed.Document retrieval extracts information using MMR from a pair of ﬁxed (vector) databases.The\ndatabases store semantically chunked data from the Theorem Proving in Lean (TPiL) handbook –\ncontaining syntax guides and tactic explanations – and the Mathlib mathematics libary – containing\nthousands of theorems and lemmas.The chunking is handled by a recursive character splitter, which\nsplits the TPiL markdown ﬁles at on its headers and Mathlib ﬁles at the start of theorems, examples,\nlemmas, and deﬁnitions – with chunk sizes of 1000 characters with a 200 character overlap.The Mathlib retriever ﬁnds the top k documents that score the highest MMR score against the current\ntheorem, the TPiL retriever ﬁnds the top k documents that score the highest MMR score against the\ncurrent theorem in context and all current error messages.This retrieval process helps in generating\nmore contextually accurate prompts that allow the language model to better correct its own errors as\nwell as ﬁnd useful lemmas to reference.4 EXPERIMENTS\n\nWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics and compare its results to those of the base GPT-4o and GPT-4o-mini\nmodels.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.4.1 SETUP\n\nOur experimentation is split into three distinct stages.We ﬁrst perform ablation testing on the\nImProver model parameters (§3.1) to ensure that ImProver’s parameter speciﬁcation is the optimal\none with respect to correctness and metric optimization score.We then evaluate this optimal param-\neter combination on datasets of varying complexity and analyze the performance and results thereof.Lastly, we note the performance of ImProver in NTP applications in comparison to the base GPT-4o\nand GPT-4o-mini models.Datasets. We evaluate ImProver on subsets of the following datasets.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.We use exercise solutions from set theory, elementary number theory, group\ntheory, topology, differential calculus, and integration & measure theory.This dataset contains the-\norems at an undergraduate-level of complexity.For our main results, we evaluated on 72 theorems\nfrom exercise solutions from MIL chapters 4, 5, 8, 9, and 10.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.This is a\ndataset of internationally-renowned competitive math problems, many of which are readable, yet\nquite verbose.This dataset contains theorems of a competitive format, and although they contain\n\n5\n\n\fconcepts only at a high-school level, the logical complexity of internationally-renowned competi-\ntion results is far above that.For our main results, we used all 26 theorems and lemmas from the\nCompﬁles database of complete solutions to the International Mathematics Olympiad (IMO) and\nthe American Mathematics Olympiad (USAMO) from 2016-2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.These proofs are extremely\nefﬁcient, concise, and generalized - which often comes at the cost of readability and understandabil-\nity.These results and theorems often are at the cutting edge of research.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.This is the most difﬁcult dataset. Models.Our base generator uses GPT-4o (OpenAI et al., 2024).Since no prior methods currently\nexist for automated proof optimization, we consider a prompted GPT-4o without the improvements\ndescribed in (§3.1) as our baseline.Additionally, for a given metric, we write a prompt that brieﬂy\ndescribes the metric and the proof optimization task.We also provide instructions, context, and in-\nformation depending on the features selected, and add the theorem and proof to the prompt.Speciﬁc\nprompt information is detailed in (§A)\n\nPerformance metrics.Since proof optimization is a new task, we deﬁne four performance metrics\nfor measuring aspects of correctness and improvement.First, we deﬁne improvement for length as percentage change in length, µlen(y0)−µlen(y)\n× 100.For\nreadability, we use the difference, µread(y)−µread(yo).If no correct output is generated by the model\nfor a speciﬁc theorem, improvement is deﬁned to be zero.We deﬁne nonempty improvement as\nthe improvement restricted to theorems for which some output has nonzero improvement.µlen(y0)\n\nIntuitively, improvement is the expected improvement in metric score from the input to output, ac-\ncounting for errors in the generation.The nonempty improvement score is the expected improvement\nin metric score, given that there are no errors in the generation.Similar improvement scores can be\ndeﬁned for other metrics using a binary function of the metric assigned to the original and optimized\nproofs.Additionally, the accuracy is the percentage of theorems in the dataset which the model was able to\ngenerate a correct output for.The improved accuracy is the percentage of theorems in the dataset\nwhich the model was able to generate a correct output for, as well as improve the metric to be\nnonzero.4.1.1 ABLATIONS\n\nWhen performing our ablation studies, we used a ﬁxed dataset (MIL) and metric (length) and varied\nthe parameters of all the features to ﬁnd the optimal combination.However, as there are over 8640\npossible combinations, it is inefﬁcient to test all combinations at once.As such, we evaluate using a\nfactorial testing method. Testing Groups.We deﬁne the following testing groups with the speciﬁed parameter combina-\ntions:\n\nGPT-4o-mini/GPT-4o: This varies the GPT-4o model, outputting a string with no other features.Output and CoS: We evaluate the effects of different output formatting styles (string, string\nlist, string tree) and CoS (True, False), with the model ﬁxed as GPT-4o, with no other\nfeatures enabled.Example Retrieval: We evaluate the effects of increasing the number of examples provided (multi-\nshot prompting) in the range of 0, 3, 5, 7, and 10, with the model ﬁxed as GPT-4o, CoS and output\nformatting ﬁxed as the best combination from the previous test, and no other features enabled.Sampling Method: Here, we evaluate the effects of best-of-n and reﬁnement for a ﬁxed n = 5.Additionally we test on the reﬁnement cases if forwarding the most recent iteration result, or all\nprevious iteration results is the best, and if we should keep the best out of the iterations, or the most\n\n6\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 1: Average Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nMetric\n\nModel\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n3.7\n20.96\n\n2.21\n9.34\n\n6.25\n30.54\n\n4.18\n13.45\n\n15.15\n55.29\n\n8.02\n30.53\n\n18.58\n56.56\n\n14.48\n30.97\n\n26.36%\n100.0%\n\n18.75%\n100.0%\n\n8.31%\n35.44%\n\n6.13 %\n24.56%\n\n37.5%\n100.0%\n\n28.85%\n100.0%\n\n14.42%\n50.0%\n\n11.54%\n34.21%\n\nTable 2: MIL Proof optimization results.Improvement Nonempty Improvement Accuracy\n\nImproved Acc. recent.The model is ﬁxed as GPT-4o, CoS, output formatting, and examples are ﬁxed as the best\ncombination from the previous test, and no other features enabled.n and Model: Here, we evaluate the effects of larger n values and different models.We test n =\n3, 5, 7, 10, 15 on GPT-4o and GPT-4o-mini, as well as n = 20 for GPT-4o-mini (as it is of a far\nlower token cost).CoS, output formatting, examples, and sampling method are ﬁxed as the best\ncombination from the previous test, and no other features enabled.Combos and RAG: We evaluate combination methods refinement(best_of_m',m) and\nbest_of_m'(refinement(m)), for m 6= m′ with mm′ equal to the optimal value m from\nthe previous test.We also test the effect of enabling document retrieval.Model, CoS, output for-\nmatting, examples, n, and sampling method are ﬁxed as the best combination from the previous\ntest.Ablation data. We evaluate our ablations on a subset of MIL.However, due to the increase in\nmodel calls for larger n values, we switch a representative sample of this subset for some test groups.Namely,\n\nGPT-4o-mini, GPT-4o, Output and Cos, Example Retrieval, and Sampling Method are tested\non the 133 theorems in the solutions of C03_Logic, C04_Sets_and_Functions, and\nC05_Elementary_Number_Theory.n and Model are tested on 55 theorems from a representative sample of the aforementioned, and\nCombos and RAG are tested on a representative sample of 32 theorems from the aforementioned.4.2 RESULTS\n\nImProver is capable of optimizing proofs in all settings.From Table 2, Table 3, and Table 4,\nwe can see that ImProver is capable of optimizing proofs on all datasets for both the length and\nreadability metrics.Furthermore, Table 1 shows that across all metrics, ImProver signiﬁcantly\noutperforms GPT-4o on proof optimization tasks on every experimental measure – aggregated from\nall datasets.Additionally, from Table 2, Table 3, and Table 4, we can see that ImProver outperforms\nGPT-4o on each dataset as well.We proceed to analyze this data and its implications. Length optimization.First focusing on the length metric, we see that ImProver outperforms GPT-\n4o with respect to the improvement score by 566% (aggregated over all datasets).Additionally, we\nare guaranteed that ImProver produces a correct output, although that output may just be the same\nas the input.However, 35.44% of the time, it generates a correct output that is not the same length\nas the input, and in that case, we expect an average of a 55.29% reduction in length.Comparing\nthis with GPT-4o, we conclude that not only can ImProver optimize at a higher level on arbitrary\ntheorems, but its ability to generate nontrivial correct outputs is far greater in comparison to GPT-4o.7\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 3: Compﬁles Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n2.75\n18.86\n\n0.39\n5.74\n\n0.0\n6.19\n\n0.0\n4.63\n\n30.7\n54.48\n\n3.38\n24.89\n\n0.0\n53.65\n\n0.0\n33.19\n\n11.54%\n100.0%\n\n14.1%\n100.0%\n\n5.13%\n34.62%\n\n1.28%\n19.23%\n\n16.67%\n100.0%\n\n4.65%\n100.0%\n\n0.0%\n11.54%\n\n0.0%\n11.63%\n\nMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 4: Mathlib Proof optimization results. Readability optimization.Readability optimization is similar, with ImProver outperforming\nGPT-4o by 423%.Moreover, the accuracy, improved accuracy, and nonempty improvement dis-\nparities for readability parallel those of the length tests.However, it should be noted that for both\nGPT-4o and ImProver, the accuracy and improved accuracy scores were markedly smaller for read-\nability than length optimization.This suggests that for both models, it was generally more “difﬁcult”\nto generate a correct output, and moreover, generate a correct output with a better metric score\nthan the input, for readability optimization than length optimization.In other words, optimizing for\nreadability is more difﬁcult for the underlying generator than optimizing for length.However, we\nspeculate with higher-quality prompts, descriptions of the metric, and examples, this disparity can\nbe minimized.Regardless, we note that different metrics can be less likely to be correctly optimized,\nand that model performance is correlated with the metric it seeks to optimize – both for GPT-4o and\nImProver.Optimization varies based on dataset difﬁculty.Additionally noting Table 2, Table 3, and\nTable 4, we observe that the improvement score for both metrics for both GPT-4o and ImProver\nis highest for the MIL dataset, lower for Compﬁles, and the lowest on the Mathlib theorems.This\nsuggests that the expected improvement in metric score decreases with higher difﬁcultly – with\nundergraduate-level theorems having a signiﬁcantly higher expected improvement than research-\nlevel theorems.However, it should be noted that for both metrics, the nonempty improvement of\nImProver stayed consistent, whereas for GPT-4o, it followed the aforementioned trend of decreas-\ning with difﬁculty.Similarly, the accuracy and improved accuracy scores for both metrics and\nmodels decreased with higher difﬁculty datasets (disregarding ImProver’s accuracy scores, as they\nare ensured to be 100%).This suggests that although the base GPT-4o generator is less likely to\ngenerate a correct output for higher difﬁculty datasets, the improvements that ImProver makes to\nthe base generator allows it to maintain its improvement in the metric score whenever a correct out-\nput is generated.As such, we can speculate that the bottleneck in the improvement score is not the\nmodel’s ability to optimize the proof for a metric, but rather its ability to generate a new correct\nproof at all.As such, we conjecture that with more capable generator models, the accuracy – and\nthus, the improvement score – in optimization tasks will continue to increase, until the improvement\nscores match the nonempty improvement.Overall, we conclude that although the performance of both ImProver and GPT-4o decreases on\nlength and readability optimization on more difﬁcult datasets, ImProver signiﬁcantly outperforms\nGPT-4o on all datasets for length and readability optimization.4.2.1 ABLATION TESTING\n\nWe perform ablation studies using a subset of the MIL dataset as discussed in §4.1.1.The results\nof this factorial study are aggregated in Table 5.We measure the baseline results from the GPT-\n4o and GPT-4o-mini models, noting that GPT-4o is the better-scoring model (with respect to the\nimprovement score).Thus, ﬁxing this model, we vary the output formatting type and if CoS is\nenabled, and determine that outputting flat with CoS enabled maximizes the improvement score.8\n\n\fTable 5: Ablation results.Each cell in the ablation tests shows best / worst, which are the best\nand worst parameter combinations in the test group.The ImProver speciﬁcation outputs the input\ntheorem when no correct proof is generated, which results in an accuracy of 100% on MIL.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o-mini\nGPT-4o\n+ Output and CoS\n+ Example Retrieval\n+ Sampling Method\n+ n and Model\n+ Combos and RAG\nImProver\n\n0\n7.03\n8.04 / 6.31\n9.34 / 5.67\n15.35 / 9.34\n23.51 / 3.65\n34.88 / 28.25\n\n0\n19.67\n12.38 / 14.17\n14.7 / 8.44\n18.44 / 14.7\n26.28 / 4.63\n57.56 / 33.48\n\n3.62%\n35.77%\n\n0%\n15.33%\n\n64.96% / 44.53% 21.17% / 16.06%\n63.5% / 67.15%\n21.9% / 16.79%\n36.5% / 21.9%\n83.21% / 63.5%\n89.47% / 78.95% 45.61% / 8.77%\n60.61% / 84.38% 54.55% / 53.12%\n\n34.88\n\n57.56\n\n100%\n\n54.55%\n\nTable 6: CoS Readability Ablation results.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o\nImProver, CoS Disabled\nImProver\n\n4.97\n9.23\n16.69\n\n15.89\n24.61\n31.42\n\n37.5%\n100.0%\n100.0%\n\n12.5%\n28.12%\n46.88%\n\nFixing these parameters, we now vary the number of examples retrieved, noting that prompting\nwith 10 examples maximizes the improvement score.Fixing this parameter, we vary the sampling\nmethods (excluding compound methods and ﬁxing n = 5) and observe that best-of-n is the best\nparameter combination.Now, as GPT-4o-mini is signiﬁcantly less computationally expensive than\nits GPT-4o counterpart, we test both models with the sample method ﬁxed to best-of-n, and vary\nn = 1, 3, 5, 7, 10, 15, and for GPT-4o-mini, also n = 20.We conclude that GPT-4o with n = 15\nis the most effective.Fixing these parameters, we consider all mixed compound sampling methods\nwith and without document retrieval enabled, concluding that a 5-step reﬁnement with best-of-3 on\neach iteration, with RAG enabled, is the optimal combination.Thus, as we can see from Table 5, the optimal parameter combination comes from gpt-4o outputting\nas a string list with CoS, RAG, 10 examples, 5-step reﬁnement with each iteration being a\nbest-of-3 evaluation.Changing any one of these parameters them leads to a reduction in performance.Additional ablation data can be found at (§B.1).Readability and Chain-of-States (CoS) Ablation.We additionally examine the effects of dis-\nabling CoS on readability optimization tasks, as the previous study focused on length optimization\ntasks, and we speculate that CoS has a high impact on the performance of readability optimization\ntasks, as the proof states that are embedded due to CoS seem to be a critical aspect to generating the\nexplicit declarations that the readability metric measures.We conﬁrm this result by considering Table 6 and observe that simply enabling CoS nearly doubles\nthe improvement score, and signiﬁcantly improves the nonempty improvement score, suggesting that\nCoS has a high degree of impact on optimizing for the readability metric, as conjectured.However,\nwe also note a signiﬁcant increase in improved accuracy, which suggests that embedding the chain\nof states also improves the ability of the model to generate nontrivial correct outputs, implying that\nthe symbolic information contained in the states are critical to effectively modifying the structure\nand content of a proof.4.2.2 NEURAL THEOREM PROVING EVALUATION\n\nWe evaluate ImProver’s neural theorem proving (NTP) performance using the completion metric.We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input\nproof.Table 7 shows the accuracy on the dataset split by topic for both ImProver and GPT-4o.ImProver substantially outperforms GPT-4o across all topics, with an 80% increase in accuracy\ncompared to the base model, showing that proof optimization systems are easily extendable to NTP\nsystems.9\n\n\fTable 7: Proof generation results.Each cell shows percent accuracy.MIL\n\nSet Theory Group Theory\n\nOverall\n\nGPT-4o\nImProver\n\n18.18%\n45.45%\n\n25%\n33.33%\n\n21.73%\n39.13%\n\n4.3 QUALITATIVE RESULTS\n\nNext, we discuss qualitative examples showing the improvements from ImProver in proof optimiza-\ntion.These examples show the balance between correctness and the desired optimization metric,\nshowing how ImProver can produce more concise or readable proofs depending on the use case.Additional examples can be found in (§B.2)\n\nExample 1: Compﬁles Optimization.Consider Figure 1, a lemma from the 2022 IMO Question\n2 (Compﬁles) that we optimize for length.The original proof consisted of 12 tactic steps and multiple intermediate calculations.After applying\nImProver for length optimization, the proof was halved to only 6 tactics, eliminating redundant steps\nwhile maintaining correctness.In comparison, the shortened proof makes multiple nontrivial optimizations, such as eliminating the\nh2’ and h4 and hxw hypotheses, as well as fully generating proof terms for speciﬁc rewrites and\nother tactics.Example 2: MIL. Consider Figure 3, a result from MIL that we optimize for readability.Original (human-written)\n\nImProver (readability-optimized)\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ncard G = card H * card K)\n\n: K ≃* G / H := by\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\n· rw [← ker_eq_bot_iff, (QuotientGroup.mk'\n\nH).ker_restrict K]\nsimp [h]\n\n· symm\n\nexact aux_card_eq h'\n\ncard G = card H * card K)\n:= by\n\n: K ≃* G / H\n\nhave injectivity : Function.Injective\n\n((QuotientGroup.mk' H).restrict K) := by\nrw [← ker_eq_bot_iff, (QuotientGroup.mk'\nH).ker_restrict K]\nsimp [h]\n\nhave card_eq : card (G / H) = card K := by\n\nexact aux_card_eq h'\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\nexact injectivity\nsymm\nexact card_eq\n\nFigure 3: Optimizing a group-theoretic result from MIL Chapter 8 Section 1 for readability.We\ndeﬁne a proof to be readable if it is written in a declarative style, which consists of intermediate\nconjectures (have · · · statements).ImProver introduces two intermediate conjectures into the proof.This original proof carried a score of 0, as it does not contain any declarative statements.In compar-\nison, after applying ImProver, we transformed the proof to be more declarative, with many more\nintermediate steps with explicit have tactics for improved clarity.Additionally observe how the\nmodel deﬁnes hypotheses for use in the latter half of the proof; these predeﬁned hypotheses could\neasily be converted into standalone lemmas for reuse.Example 3: Full Proof Generation.We analyze the application of ImProver to neural theorem\nproving in the MIL example from Figure 4.This theorem relating to group theory originally has no proof, however, ImProver generates one\nfrom scratch.This generated proof is veriﬁed to be correct by Lean, utilizing all the included hy-\npotheses as well as a retrieved mathlib theorem.10\n\n\fOriginal (human-written)\n\nImProver (completeness-optimized)\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n: S ≤ T) : comap ϕ S ≤ comap ϕ T := by\n\nexample (ϕ : G →* H) (S T : Subgroup H) (hST\n\n: S ≤ T) : comap ϕ S ≤ comap ϕ T\n\n:= by\n\nsorry\n\nintro g\nsimp only [mem_comap]\nintro hS\nexact hST hS\n\nFigure 4: Solving a group theorem exercise from MIL Chapter 8 Section 1 for readability.5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.By incorporating CoS, RAG, and other features, ImProver signiﬁcantly outperforms base\nlanguage models in proof optimization over undergraduate, competition, and research-level prob-\nlems.However, ImProver is limited by its high cost and slow runtime, which is exacerbated by its reliance\non black-box LLM’s.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.As such, we believe that ImProver sets the stage for further\nwork on proof optimization to advance the study and use of AI in mathematics.ACKNOWLEDGEMENTS\n\nRiyaz Ahuja thanks the L3 Lab and Hoskinson Center for Formal Mathematics for their support.Sean Welleck thanks Convergent Research, the Lean FRO, and the OpenAI Researcher Access Pro-\ngram for their support.11\n\n\fREFERENCES\n\nand\n\nAlphaGeometry\n\nAlphaProof\nstandard\nhttps://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/\n2024.\n\nsilver-medal\nproblems.achieves\nolympiad\n\nmathematical\n\ninternational\n\nsolving\n\nTeams.AI\n\nSerge Autexier and Dominik Dietrich.A tactic language for declarative proofs.In Matt Kaufmann\nand Lawrence C. Paulson (eds.), Interactive Theorem Proving, pp.99–114, Berlin, Heidelberg,\n2010. Springer Berlin Heidelberg.Jaime Carbonell and Jade Goldstein.The use of mmr, diversity-based reranking for reordering doc-\numents and producing summaries.In Proceedings of the 21st Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR ’98, pp.335–336,\nISBN 1581130155. doi:\nNew York, NY, USA, 1998.Association for Computing Machinery. 10.1145/290941.291025.URL https://doi.org/10.1145/290941.291025.Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.Teaching large language models\n\nto self-debug, 2023.URL https://arxiv.org/abs/2304.05128. David Renshaw. compﬁles.https://github.com/dwrensha/compfiles, 2024.Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun.Baldur: Whole-proof generation and\n\nrepair with large language models, 2023.Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu.Proof artifact co-\nIn International Conference on Learning\n\ntraining for theorem proving with language models.Representations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.Jiewen Hu, Thomas Zhu, and Sean Welleck.minictx: Neural theorem proving with (long-)contexts,\n\n2024.URL https://arxiv.org/abs/2408.03350.Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu.Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs.In The Eleventh International Conference on Learning\nRepresentations, 2023.URL https://openreview.net/forum?id=SMa9EAovKMC.Kim Morrison. lean-training-data.https://github.com/kim-em/lean-training-data,\n\nGuillaume Lample, Timothee Lacroix, Marie anne Lachaux, Aurelien Rodriguez, Amaury\nHypertree proof search\nHayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.for neural\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=J4pX8Q8cxHH. theorem proving. 2024. 2024.Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie\n\nSi.A survey on deep learning for theorem proving, 2024.Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang.A survey of deep\nlearning for mathematical reasoning.In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp.14605–14631, Toronto, Canada, July 2023.Association for Computational Linguistics.10.18653/v1/2023.acl-long.817.URL\nhttps://aclanthology.org/2023.acl-long.817.doi:\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-reﬁne:\nIterative reﬁnement with self-feedback.In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.URL https://openreview.net/forum?id=S37hOerQLB.12\n\nleanprover-community. mathematics_in_lean.https://github.com/leanprover-community/mathematics_in_lea\n\n\fThe mathlib Community.The lean mathematical library.In Proceedings of the 9th ACM SIGPLAN\nInternational Conference on Certiﬁed Programs and Proofs, POPL ’20.ACM, January 2020. doi:\n10.1145/3372885.3373824.URL http://dx.doi.org/10.1145/3372885.3373824.Leonardo de Moura and Sebastian Ullrich.The lean 4 theorem prover and programming lan-\nIn Automated Deduction – CADE 28: 28th International Conference on Automated\nguage.Deduction, Virtual Event, July 12–15, 2021, Proceedings, pp.625–635, Berlin, Heidelberg,\n2021. Springer-Verlag.ISBN 978-3-030-79875-8. doi: 10.1007/978-3-030-79876-5_37.URL\nhttps://doi.org/10.1007/978-3-030-79876-5_37.OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red\nAvila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Moham-\nmad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher\nBerner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brit-\ntany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek\nChen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,\nHyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas\nDegry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,\nAdrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada\nFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel,\nTarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff\nHarris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jo-\nmoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,\nNitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik\nKim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, An-\ndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,\nStephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim\nMalfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew\nMayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil,\nDavid Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin,\nVinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David\nMély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeon-\nwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley\nPantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov,\nAndrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde\nde Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea\nPower, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,\nCameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick\nRyder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David\nSchnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah\nShoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama,\nIan Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie\nSummers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin\nTootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón\nUribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welin-\nder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,\nHannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah\nYoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,\nShengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.Gpt-4 technical\nreport, 2024. URL https://arxiv.org/abs/2303.08774.Stanislas Polu and Ilya Sutskever.Generative language modeling for automated theorem proving,\n\n2020.13\n\n\fStanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\n\nSutskever.Formal mathematics statement curriculum learning, 2022.Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, and Swarat Chaudhuri.An in-\n\ncontext learning agent for formal theorem-proving, 2024.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H.\nthought prompting elicits reasoning\nChi, Quoc V Le, and Denny Zhou.in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=_VjQlMeSB_J.Chain of\n\nFreek Wiedijk. Formal proof sketches.In Stefano Berardi, Mario Coppo, and Ferruccio Damiani\n(eds.), Types for Proofs and Programs, pp.378–393, Berlin, Heidelberg, 2004.Springer Berlin\nHeidelberg. ISBN 978-3-540-24849-1.Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil,\nRyan Prenger, and Anima Anandkumar.LeanDojo: Theorem proving with retrieval-augmented\nlanguage models.In Neural Information Processing Systems (NeurIPS), 2023.Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig.Docprompting: Gener-\nating code by retrieving the docs.In The Eleventh International Conference on Learning Repre-\nsentations, 2023.URL https://openreview.net/forum?id=ZTCxT2t2Ru.14\n\n\fA PROMPTS\n\nA.1 TEMPLATE\n\nIn this appendix, we note the prompts used by ImProver both for general LLM prompting, as well\nas the metric-speciﬁc prompts.For the main prompt sent to the LLM on each sample, we build a prompt string using a chat prompt\ntemplate that is then invoked at runtime to ﬁll in the variables.Namely, these variables include the set of metric prompts, previous results, input theorem, context,\na syntax documents, Mathlib documents, and examples.The prompt template is a conversation of the format:\n\nPlaceholder: All metric prompts with a ‘System’ role\nSystem: You will be given the proof context (i.e.the lean ﬁle contents/imports leading up\nto the theorem declaration) wrapped by <CONTEXT>...</CONTEXT>.You will be given the previous num_prev input/output pairs as well as their metric (met-\nric.name) score and correctness score, as well as any error messages, for your reference to\nimprove upon.Each of these previous results will be wrapped with <PREV I=0></PREV\nI=0>,...,<PREV I=num_prev-1></PREV I=num_prev-1>, with I=num_prev-1 being the\nmost recent result.Remember to use lean 4 syntax, which has signiﬁcant changes from the lean 3 syntax.To\nassist with the syntax relating to the current theorem and current error messages, you will\nbe given num_syntax_docs documents to refer to for ﬁxing these syntax issues.Each of\nthese documents will be wrapped with <SYNTAX_DOC>...</SYNTAX_DOC>.You will also receive num_mathlib_docs documents relevant to the current theorem to\nhelp with formulating your modiﬁed proof.Each of these will be wrapped with <CON-\nTENT_DOC>...</CONTENT_DOC>\nYou will also receive num_examples examples of input-output pairs of proofs that\nwere optimized for the metric metric.Each of these will be wrapped with <EXAM-\nPLE>...</EXAMPLE>\nYou will be given the tactic states as comments for reference.The current theorem will be\nwrapped in <CURRENT>...</CURRENT>\nSystem: Output format instructions\nPlaceholder: All retrieved syntax documentation\nPlaceholder: All retrieved mathlib documentation\nPlaceholder: All retrieved examples\nUser: <CONTEXT> context </CONTEXT>\nPlaceholder: Previous results and inputs/outputs\nPlaceholder: All metric prompts with a ‘User’ role\nUser: <CURRENT> theorem </CURRENT>\n\nThis prompt is then invoked and sent to the language model by ﬁlling in all the variables and place-\nholders.Notably, when we invoke the chain given by chain|llm|parser, we throttle the invoca-\ntion with a randomized exponential rate limit throttling to account for API rate limits, especially in\nhighly-parallelized requests like when benchmarking over a large number of theorems.A.2 METRIC PROMPTS\n\nLength Metric\n\nSystem: You are an AI assistant who shortens Lean 4 proofs while ensuring their correct-\nness.You will aim to reduce the number of lines of the tactic proof while ensuring that it\nproperly compiles in Lean 4.15\n\n\fUser: Shorten the current theorem (wrapped in <CURRENT>...</CURRENT>) to be as\nshort in length—measured in the number of lines of the proof—as possible, while also\nensuring that the output is still syntactically correct.\"Readability Metric\n\nCompletion Metric\n\nSystem: You are an AI assistant who rewrites Lean 4 proofs to be more readable while\nensuring their correctness.We measure readablity by considering the ratio of the number\nofexplicitly typed have tactics against the total number of tactics in the proof, as this is\nproportional to whether a proof is declarative in style, and thus, readable.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is\nmore readable and declarative and modular.System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.You will receive a Lean 4 proof you must\nmodify to eliminate any errors so that it compiles correctly and eliminate any “sorry”s with\nfull proofs.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a\nformal, complete, and correct Lean 4 proof by ﬁlling in its tactic proof.B ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this section, we provide more detailed information on the experimental setup and results used to\nevaluate ImProver.B.1 ABLATION DETAILS\n\nWe now proceed to show detailed results from our ablation testing.Table 8: Output and Chain-of-States Ablations\n\nOutput Format\n\nCoS\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.string\nTrue\nstring\nFalse\nstring list True\nstring list\nFalse\nstring tree\nTrue\nstring tree\nFalse\n\n7.53\n7.03\n8.04\n7.04\n7.62\n6.31\n\n16.12\n19.67\n12.38\n13.58\n15.34\n14.17\n\n46.72%\n35.77%\n64.96%\n51.82%\n49.64%\n44.53%\n\n16.79%\n15.33%\n21.17%\n18.98%\n18.25%\n16.06%\n\nBy Table 8, we see that the optimal combination in this testing group is a string list output\nformat with CoS enabled.Fix these values for all future tests.Examples\n\nTable 9: Example Retrieval Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.0\n3\n5\n7\n10\n\n5.67\n8.49\n8.38\n7.56\n9.34\n\n67.15%\n62.04%\n64.96%\n62.77%\n63.5%\n\n16.79%\n19.71%\n21.17%\n19.71%\n21.9%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the number of examples.By Table 9,\nwe see that the optimal combination in this testing group is 10 examples.Fix this value for all future\ntests.8.44\n13.68\n12.9\n12.04\n14.7\n\n16\n\n\fTable 10: Sampling Method Ablations\n\nMethod\n\nForward Keep Best\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.None\nreﬁnement\nreﬁnement\nreﬁnement\nreﬁnement\nbest-of-n\n\nN/A\n1\n5\n1\n5\nN/A\n\nN/A\nFalse\nFalse\nTrue\nTrue\nN/A\n\n9.34\n14.76\n12.5\n14.95\n13.15\n15.35\n\n14.7\n30.63\n20.88\n14.95\n13.15\n18.44\n\n63.5%\n48.18%\n59.85%\n100.0%\n100.0%\n83.21%\n\n21.9%\n30.66%\n30.66%\n30.66%\n29.93%\n36.5%\n\nNote that forward and keep-best values are parameters for reﬁnement of how many previous itera-\ntions to forward, and whether to keep the most recent or the best iteration in subsequent reﬁnement\nsteps.Now, with the previous optimal parameters ﬁxed, run the ablation on the sample method.By\nTable 10, we see that the optimal combination in this testing group is best-of-n.Fix this value\nfor all future tests.Table 11: Model and n Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.Model\n\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\ngpt-4o-mini\n\nn\n\n3\n5\n7\n10\n15\n3\n5\n7\n10\n15\n20\n\n19.66\n20.12\n22.44\n21.73\n23.51\n3.65\n5.12\n3.65\n4.99\n4.35\n4.87\n\n24.36\n24.97\n27.21\n25.28\n26.28\n4.63\n6.21\n4.34\n5.69\n5.06\n5.56\n\n80.7%\n80.56%\n82.46%\n85.96%\n89.47%\n78.95%\n82.46%\n84.21%\n87.72%\n85.96%\n87.72%\n\n38.6%\n36.11%\n42.11%\n40.35%\n45.61%\n8.77%\n10.53%\n8.77%\n12.28%\n12.28%\n14.04%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the value of n and model.By\nTable 11, we see that the optimal combination in this testing group is GPT-4o with n = 15.Fix this\nvalue for all future tests.Table 12: RAG and Combination Sampling Method Ablations\n\nRAG Improvement Nonempty Improve. Accuracy\n\nImproved Acc.Combination\n\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nbest-of-n\nbest-of-n\n\nm\n\nm′\n\n5\n3\n5\n3\n3\n5\n3\n5\n5\n3\n5\n3\n3\n5\n5\n3\nN/A 15\nN/A 15\n\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n33.78\n31.23\n31.85\n31.35\n51.32\n50.1\n57.56\n49.75\n32.71\n33.48\n\n100.0%\n100.0%\n100.0%\n100.0%\n63.64%\n65.62%\n60.61%\n59.38%\n90.62%\n84.38%\n\n50.0%\n46.88%\n50.0%\n50.0%\n48.48%\n53.12%\n54.55%\n43.75%\n56.25%\n53.12%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the combination methods and if\nRAG is enabled.By Table 12, we see that the optimal combination in this testing group is a 5-step\nreﬁnement with each iteration being a best-of-3 call, with RAG enabled.B.2 ADDITIONAL QUALITATIVE EXAMPLES\n\nIn this section, we provide qualitative examples demonstrating the improvements ImProver achieves\nin proof optimization.33.78\n31.23\n31.85\n31.35\n32.66\n32.88\n34.88\n29.54\n29.64\n28.25\n\n17\n\n\fCompﬁles: Length Optimization See (§4.3)\n\nCompﬁles: Readability Optimization Consider Figure 5, in which a lemma from the 2019 IMO\nproblem 1 (from the Compﬁles dataset) is optimized for readability.This introduces multiple new\nhypotheses, which generalize a linear_property of the functions, and then reuses and instanti-\nates that (and others, too) hypothesis throughout the proof, creating a signiﬁcantly more declaritive,\nmodular, and therefore readable proof.Original (human-written)\n\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\nrefine hf 1, fun a => ?_i\nchange g a = g 1 * a\nrw [mul_comm, ← smul_eq_mul, ←\n\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nImProver (readability-optimized)\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\n\nhave linear_property : ∀ a, f a = g a := by\n\nintro a\nrfl\n\nhave g_smul : ∀ a, g a = g 1 * a := by\n\nintro a\nrw [mul_comm, ← smul_eq_mul, ←\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nrefine hf 1, fun a => ?_i\nhave f_eq_g : f a = g a := linear_property a\nhave g_a_eq : g a = g 1 * a := g_smul a\nrw [f_eq_g, linear_property 1, g_a_eq]\n\nFigure 5: Optimizing a lemma from IMO 2019 P1 for readability\n\nMIL: Length Optimization Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, eliminating simp calls and introducing proof terms\ninto the structure of the proof to shorten it from 9 tactic invocations to 7.Original (human-written)\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\n\n:= by\n\next x\nsimp only [mem_map]\nconstructor\n· rintro hy, y_in, hyi\n\nexact hϕ y, hy, y_in, rfli, hyi\n\n· rintro hy, hz, z_in, hzi, hyi\n\nuse z, z_in\ncalc ψ.comp ϕ z = ψ (ϕ z) := rfl\n_\n= ψ y := by congr\n\nImProver (length-optimized)\n\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\nby\next x\nsimp only [mem_map]\nconstructor\nrintro hy, y_in, hyi; exact hϕ y, hy, y_in,\n\n:=\n\nrfli, hyi\n\nrintro hy, hz, z_in, hzi, hyi; exact hz,\nz_in, (congr_arg ψ hz).trans hyi\n\nFigure 6: Optimizing a lemma from the solutions of MIL CH08 S01 for length\n\nMIL: Length Optimization 2 Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, converting a full tactic proof into a single proof term\nto shorten it from 28 tactic invocations to 1.Note that the model does not have access to the Lean\ncommands that symbolically generate proof terms, and therefore generates and estimates the proof\nterm entirely by itself.MIL: Readability Optimization See (§4.3)\n\nMathlib: Length Optimization Consider Figure 8, which optimizes a theorem in algebraic topol-\nogy from mathlib for length, eliminating simp calls and combining tactics to shorten it from 3 tactic\ninvocations to 1.Mathlib: Readability Optimization Consider Figure 9, a theorem from Mathlib that we optimize\nfor readability.This original proof carried a score of 0, as it does not contain any declarative statements.It is concise\nand efﬁcient, however, it is difﬁcult to understand and read.18\n\n\fOriginal (human-written)\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\next x; constructor\n· rintro (hxs, xnti | hxt, xnsi)\n\nImProver (length-optimized)\n\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\nexact Set.ext fun x => hfun h => h.elim\n\n(fun hxs, xnti => hOr.inl xs, fun h_, xti =\n> xnt xti) (fun hxt, xnsi => hOr.inr xt,\nfun hxs, _i => xns xsi),\n\nfun hh, nxsti => h.elim (fun xs => Or.inl h\nxs, fun xt => nxst hxs, xtii) (fun xt =>\nOr.inr hxt, fun xs => nxst hxs, xtii)i\n\n· constructor\n\nleft\nexact xs\nrintro h_, xti\ncontradiction\n\n.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.This intermediate tactic greatly helps in the understandability and clarity of the proof.Original (human-written)\n\nImProver (readability-optimized)\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n:=\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]; rfl\n\n/-- Another version of\n\n`simply_connected_iff_paths_homotopic` -/\ntheorem simply_connected_iff_paths_homotopic'\n{Y : Type*} [TopologicalSpace Y] :\nSimplyConnectedSpace Y ↔\n\nPathConnectedSpace Y ∧ ∀ {x y : Y} (p1\n\np2 : Path x y), Path.Homotopic p1 p2\nby\nconvert\n\n:=\n\nsimply_connected_iff_paths_homotopic (Y\n:= Y)\n\nhave h1 : ∀ x y : Y, Subsingleton\n\n(Path.Homotopic.Quotient x y) ↔ ∀ {p1 p2\n: Path x y}, Path.Homotopic p1 p2 := by\nintro x y\nsimp [Path.Homotopic.Quotient,\nSetoid.eq_top_iff]\nrfl\n\nsimp only [h1]\n\nFigure 9: Optimizing a theorem from Mathlib/FundamentalGroupoid/SimplyConnected\nfor readability\n\n19",
          "time": 732.9566588401794,
          "input_token_size": 16886,
          "output_token_size": 117,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_e6228b89",
        "created_at": 1749769273.2649605,
        "processing_time": 744.9016652107239,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev1",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev1",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.845336"
      }
    }
  },
  {
    "e68f9dfc": {
      "question": "What does the document say about large language models in proof optimization?",
      "answer": "Large language models (LLMs) have been used to generate formal proofs of mathematical theorems in proof assistants such as Lean. However, naively applying LLMs to proof optimization falls short, often resulting in incorrect or poorly optimized proofs. Improvements such as the Chain-of-States technique, error-correction, and retrieval have been incorporated into ImProver, a large language model agent that rewrites proofs to optimize arbitrary user-defined metrics in Lean. ImProver is capable of rewriting proofs to be substantially shorter, more modular, and more readable.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            17,
            18,
            19,
            20,
            27,
            28,
            29,
            62,
            71,
            73,
            74,
            75,
            76,
            77,
            78,
            82,
            83,
            85,
            87,
            88,
            89,
            90,
            92,
            93,
            94,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            116,
            118,
            119,
            120,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            131,
            132,
            133,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            197,
            199,
            200,
            203,
            204,
            205,
            213,
            245,
            310,
            311,
            320,
            322,
            323,
            326,
            331,
            332,
            333,
            337,
            338,
            339,
            340,
            343,
            344,
            345,
            346,
            347,
            349,
            350,
            351,
            356,
            357,
            362,
            363,
            364,
            366,
            367,
            369,
            372,
            373,
            374,
            375
          ],
          "provenance": "4\n2\n0\n2\n \nt\nc\nO\n \n7\n \n \n]\nI\n\nA\n. s\nc\n[\n \n \n1\nv\n3\n5\n7\n4\n0\n.0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.To this end, we study a new problem of automated proof\noptimization: rewriting a proof so that it is correct and optimizes for an arbitrary\ncriterion, such as length or readability.As a ﬁrst method for automated proof opti-\nmization, we present ImProver, a large-language-model agent that rewrites proofs\nto optimize arbitrary user-deﬁned metrics in Lean.We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is\ncorrect and optimizes a criterion such as length or readability.We ﬁnd that naively applying LLMs\nto proof optimization falls short, often resulting in incorrect or poorly optimized proofs.We develop\nvarious improvements that can be applied on top of a black-box language model, including Chain-\nof-States prompting–an analogy to chain-of-thought prompting (Wei et al., 2022) that shows inter-\nmediate proof states–along with error-correction and retrieval.We incorporate these into ImProver:\na large language model agent that rewrites proofs to optimize arbitrary user-deﬁned metrics in Lean.A typical approach (Polu & Sutskever, 2020) is to train on a large corpus of mathematical proofs\nsuch as Lean’s Mathlib (mathlib Community, 2020; Han et al., 2022; Polu et al., 2022; Lample et al.,\n2022; Yang et al., 2023; Hu et al., 2024).A model learns from the distribution of proofs in the cor-\npus, such as Mathlib-style proofs.Recently, the AlphaProof (AlphaProof & Teams, 2024) system\nwas shown to produce proofs with an arcane, non-human structure and syntax.3\n\n\fWithout Chain-of-States\n\nWith Chain-of-States\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u) := by\n\nexample : s ∩ t ∪ s ∩ u ⊆ s ∩ (t ∪ u)\n\n:= by\n\nrintro x (hxs, xti | hxs, xui)\n· use xs; left; exact xt\n.It\nutilizes thread-based parallelism to speed up the relatively large number of calls to the language\nmodel, as well as process-based parallelism for the n evaluation calls to the Lean language server.The reﬁnement process relies on user-deﬁned pa-\nrameters n and prev_num to specify the number of iterations and the number of previous iteration\ninfo to forward, respectively.Each iteration carries information on the last prev_num iterations,\nincluding input, output, metric score, correctness, and error messages.The reﬁnement technique iteratively improves the prompt output by feeding back the results into\nthe prompt function, additionally forwarding errors and metric scores.Similar to the best-of-n tech-\nnique, it relies on an argument n for the number of reﬁnement steps, and is curried such that each\nreﬁnement step can be handled by any other prompting function.However, unlike best-of-n, there is\nno opportunity for parallelism as each iteration is dependent on information from the previous call.Combination Sampling and Compound Prompt Functions Compound prompt functions utilize\nthe curried nature of the implementations of best-of-n and reﬁnement to nest these techniques within\none another.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.More speciﬁcally, example retrieval selects the most relevant user-generated examples of proof op-\ntimization on a speciﬁc metric.The number of examples that are retrieved is user-speciﬁed.The\ndatabases store semantically chunked data from the Theorem Proving in Lean (TPiL) handbook –\ncontaining syntax guides and tactic explanations – and the Mathlib mathematics libary – containing\nthousands of theorems and lemmas.The chunking is handled by a recursive character splitter, which\nsplits the TPiL markdown ﬁles at on its headers and Mathlib ﬁles at the start of theorems, examples,\nlemmas, and deﬁnitions – with chunk sizes of 1000 characters with a 200 character overlap.The Mathlib retriever ﬁnds the top k documents that score the highest MMR score against the current\ntheorem, the TPiL retriever ﬁnds the top k documents that score the highest MMR score against the\ncurrent theorem in context and all current error messages.This retrieval process helps in generating\nmore contextually accurate prompts that allow the language model to better correct its own errors as\nwell as ﬁnd useful lemmas to reference.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.4.1 SETUP\n\nOur experimentation is split into three distinct stages.We ﬁrst perform ablation testing on the\nImProver model parameters (§3.1) to ensure that ImProver’s parameter speciﬁcation is the optimal\none with respect to correctness and metric optimization score.Lastly, we note the performance of ImProver in NTP applications in comparison to the base GPT-4o\nand GPT-4o-mini models.Datasets. We evaluate ImProver on subsets of the following datasets.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.We use exercise solutions from set theory, elementary number theory, group\ntheory, topology, differential calculus, and integration & measure theory.This dataset contains the-\norems at an undergraduate-level of complexity.For our main results, we evaluated on 72 theorems\nfrom exercise solutions from MIL chapters 4, 5, 8, 9, and 10.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.This is a\ndataset of internationally-renowned competitive math problems, many of which are readable, yet\nquite verbose.This dataset contains theorems of a competitive format, and although they contain\n\n5\n\n\fconcepts only at a high-school level, the logical complexity of internationally-renowned competi-\ntion results is far above that.For our main results, we used all 26 theorems and lemmas from the\nCompﬁles database of complete solutions to the International Mathematics Olympiad (IMO) and\nthe American Mathematics Olympiad (USAMO) from 2016-2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.These results and theorems often are at the cutting edge of research.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.This is the most difﬁcult dataset. Models.Our base generator uses GPT-4o (OpenAI et al., 2024).Since no prior methods currently\nexist for automated proof optimization, we consider a prompted GPT-4o without the improvements\ndescribed in (§3.1) as our baseline.Additionally, for a given metric, we write a prompt that brieﬂy\ndescribes the metric and the proof optimization task.We also provide instructions, context, and in-\nformation depending on the features selected, and add the theorem and proof to the prompt.Since proof optimization is a new task, we deﬁne four performance metrics\nfor measuring aspects of correctness and improvement.For\nreadability, we use the difference, µread(y)−µread(yo).If no correct output is generated by the model\nfor a speciﬁc theorem, improvement is deﬁned to be zero.We deﬁne nonempty improvement as\nthe improvement restricted to theorems for which some output has nonzero improvement.The nonempty improvement score is the expected improvement\nin metric score, given that there are no errors in the generation.Similar improvement scores can be\ndeﬁned for other metrics using a binary function of the metric assigned to the original and optimized\nproofs.Additionally, the accuracy is the percentage of theorems in the dataset which the model was able to\ngenerate a correct output for.The improved accuracy is the percentage of theorems in the dataset\nwhich the model was able to generate a correct output for, as well as improve the metric to be\nnonzero.4.1.1 ABLATIONS\n\nWhen performing our ablation studies, we used a ﬁxed dataset (MIL) and metric (length) and varied\nthe parameters of all the features to ﬁnd the optimal combination.However, as there are over 8640\npossible combinations, it is inefﬁcient to test all combinations at once.As such, we evaluate using a\nfactorial testing method. Testing Groups.We deﬁne the following testing groups with the speciﬁed parameter combina-\ntions:\n\nGPT-4o-mini/GPT-4o: This varies the GPT-4o model, outputting a string with no other features.Example Retrieval: We evaluate the effects of increasing the number of examples provided (multi-\nshot prompting) in the range of 0, 3, 5, 7, and 10, with the model ﬁxed as GPT-4o, CoS and output\nformatting ﬁxed as the best combination from the previous test, and no other features enabled.Sampling Method: Here, we evaluate the effects of best-of-n and reﬁnement for a ﬁxed n = 5.Additionally we test on the reﬁnement cases if forwarding the most recent iteration result, or all\nprevious iteration results is the best, and if we should keep the best out of the iterations, or the most\n\n6\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nMetric\n\nModel\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n3.7\n20.96\n\n2.21\n9.34\n\n6.25\n30.54\n\n4.18\n13.45\n\n15.15\n55.29\n\n8.02\n30.53\n\n18.58\n56.56\n\n14.48\n30.97\n\n26.36%\n100.0%\n\n18.75%\n100.0%\n\n8.31%\n35.44%\n\n6.13 %\n24.56%\n\n37.5%\n100.0%\n\n28.85%\n100.0%\n\n14.42%\n50.0%\n\n11.54%\n34.21%\n\nTable 2: MIL Proof optimization results.Improvement Nonempty Improvement Accuracy\n\nImproved Acc. recent.The model is ﬁxed as GPT-4o, CoS, output formatting, and examples are ﬁxed as the best\ncombination from the previous test, and no other features enabled.n and Model: Here, we evaluate the effects of larger n values and different models.We test n =\n3, 5, 7, 10, 15 on GPT-4o and GPT-4o-mini, as well as n = 20 for GPT-4o-mini (as it is of a far\nlower token cost).CoS, output formatting, examples, and sampling method are ﬁxed as the best\ncombination from the previous test, and no other features enabled.Combos and RAG: We evaluate combination methods refinement(best_of_m',m) and\nbest_of_m'(refinement(m)), for m 6= m′ with mm′ equal to the optimal value m from\nthe previous test.We also test the effect of enabling document retrieval.Model, CoS, output for-\nmatting, examples, n, and sampling method are ﬁxed as the best combination from the previous\ntest.Ablation data. We evaluate our ablations on a subset of MIL.However, due to the increase in\nmodel calls for larger n values, we switch a representative sample of this subset for some test groups.Namely,\n\nGPT-4o-mini, GPT-4o, Output and Cos, Example Retrieval, and Sampling Method are tested\non the 133 theorems in the solutions of C03_Logic, C04_Sets_and_Functions, and\nC05_Elementary_Number_Theory.n and Model are tested on 55 theorems from a representative sample of the aforementioned, and\nCombos and RAG are tested on a representative sample of 32 theorems from the aforementioned.4.2 RESULTS\n\nImProver is capable of optimizing proofs in all settings.From Table 2, Table 3, and Table 4,\nwe can see that ImProver is capable of optimizing proofs on all datasets for both the length and\nreadability metrics.Additionally, from Table 2, Table 3, and Table 4, we can see that ImProver outperforms\nGPT-4o on each dataset as well.We proceed to analyze this data and its implications. Length optimization.First focusing on the length metric, we see that ImProver outperforms GPT-\n4o with respect to the improvement score by 566% (aggregated over all datasets).Additionally, we\nare guaranteed that ImProver produces a correct output, although that output may just be the same\nas the input.However, 35.44% of the time, it generates a correct output that is not the same length\nas the input, and in that case, we expect an average of a 55.29% reduction in length.Comparing\nthis with GPT-4o, we conclude that not only can ImProver optimize at a higher level on arbitrary\ntheorems, but its ability to generate nontrivial correct outputs is far greater in comparison to GPT-4o.7\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 3: Compﬁles Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n2.75\n18.86\n\n0.39\n5.74\n\n0.0\n6.19\n\n0.0\n4.63\n\n30.7\n54.48\n\n3.38\n24.89\n\n0.0\n53.65\n\n0.0\n33.19\n\n11.54%\n100.0%\n\n14.1%\n100.0%\n\n5.13%\n34.62%\n\n1.28%\n19.23%\n\n16.67%\n100.0%\n\n4.65%\n100.0%\n\n0.0%\n11.54%\n\n0.0%\n11.63%\n\nMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 4: Mathlib Proof optimization results. Readability optimization.Readability optimization is similar, with ImProver outperforming\nGPT-4o by 423%.Moreover, the accuracy, improved accuracy, and nonempty improvement dis-\nparities for readability parallel those of the length tests.However, it should be noted that for both\nGPT-4o and ImProver, the accuracy and improved accuracy scores were markedly smaller for read-\nability than length optimization.This suggests that for both models, it was generally more “difﬁcult”\nto generate a correct output, and moreover, generate a correct output with a better metric score\nthan the input, for readability optimization than length optimization.Optimization varies based on dataset difﬁculty.Additionally noting Table 2, Table 3, and\nTable 4, we observe that the improvement score for both metrics for both GPT-4o and ImProver\nis highest for the MIL dataset, lower for Compﬁles, and the lowest on the Mathlib theorems.This\nsuggests that the expected improvement in metric score decreases with higher difﬁcultly – with\nundergraduate-level theorems having a signiﬁcantly higher expected improvement than research-\nlevel theorems.However, it should be noted that for both metrics, the nonempty improvement of\nImProver stayed consistent, whereas for GPT-4o, it followed the aforementioned trend of decreas-\ning with difﬁculty.Similarly, the accuracy and improved accuracy scores for both metrics and\nmodels decreased with higher difﬁculty datasets (disregarding ImProver’s accuracy scores, as they\nare ensured to be 100%).This suggests that although the base GPT-4o generator is less likely to\ngenerate a correct output for higher difﬁculty datasets, the improvements that ImProver makes to\nthe base generator allows it to maintain its improvement in the metric score whenever a correct out-\nput is generated.As such, we can speculate that the bottleneck in the improvement score is not the\nmodel’s ability to optimize the proof for a metric, but rather its ability to generate a new correct\nproof at all.As such, we conjecture that with more capable generator models, the accuracy – and\nthus, the improvement score – in optimization tasks will continue to increase, until the improvement\nscores match the nonempty improvement.Overall, we conclude that although the performance of both ImProver and GPT-4o decreases on\nlength and readability optimization on more difﬁcult datasets, ImProver signiﬁcantly outperforms\nGPT-4o on all datasets for length and readability optimization.4.2.1 ABLATION TESTING\n\nWe perform ablation studies using a subset of the MIL dataset as discussed in §4.1.1.The results\nof this factorial study are aggregated in Table 5.We measure the baseline results from the GPT-\n4o and GPT-4o-mini models, noting that GPT-4o is the better-scoring model (with respect to the\nimprovement score).8\n\n\fTable 5: Ablation results.Each cell in the ablation tests shows best / worst, which are the best\nand worst parameter combinations in the test group.The ImProver speciﬁcation outputs the input\ntheorem when no correct proof is generated, which results in an accuracy of 100% on MIL.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o-mini\nGPT-4o\n+ Output and CoS\n+ Example Retrieval\n+ Sampling Method\n+ n and Model\n+ Combos and RAG\nImProver\n\n0\n7.03\n8.04 / 6.31\n9.34 / 5.67\n15.35 / 9.34\n23.51 / 3.65\n34.88 / 28.25\n\n0\n19.67\n12.38 / 14.17\n14.7 / 8.44\n18.44 / 14.7\n26.28 / 4.63\n57.56 / 33.48\n\n3.62%\n35.77%\n\n0%\n15.33%\n\n64.96% / 44.53% 21.17% / 16.06%\n63.5% / 67.15%\n21.9% / 16.79%\n36.5% / 21.9%\n83.21% / 63.5%\n89.47% / 78.95% 45.61% / 8.77%\n60.61% / 84.38% 54.55% / 53.12%\n\n34.88\n\n57.56\n\n100%\n\n54.55%\n\nTable 6: CoS Readability Ablation results.Improvement Nonempty Improve. Accuracy\n\nImproved Acc.GPT-4o\nImProver, CoS Disabled\nImProver\n\n4.97\n9.23\n16.69\n\n15.89\n24.61\n31.42\n\n37.5%\n100.0%\n100.0%\n\n12.5%\n28.12%\n46.88%\n\nFixing these parameters, we now vary the number of examples retrieved, noting that prompting\nwith 10 examples maximizes the improvement score.Fixing this parameter, we vary the sampling\nmethods (excluding compound methods and ﬁxing n = 5) and observe that best-of-n is the best\nparameter combination.Now, as GPT-4o-mini is signiﬁcantly less computationally expensive than\nits GPT-4o counterpart, we test both models with the sample method ﬁxed to best-of-n, and vary\nn = 1, 3, 5, 7, 10, 15, and for GPT-4o-mini, also n = 20.We conﬁrm this result by considering Table 6 and observe that simply enabling CoS nearly doubles\nthe improvement score, and signiﬁcantly improves the nonempty improvement score, suggesting that\nCoS has a high degree of impact on optimizing for the readability metric, as conjectured.4.2.2 NEURAL THEOREM PROVING EVALUATION\n\nWe evaluate ImProver’s neural theorem proving (NTP) performance using the completion metric.We evaluate on 23 exercises in group theory (12) and set theory (11) from MIL, with an empty input\nproof.9\n\n\fTable 7: Proof generation results.Each cell shows percent accuracy.MIL\n\nSet Theory Group Theory\n\nOverall\n\nGPT-4o\nImProver\n\n18.18%\n45.45%\n\n25%\n33.33%\n\n21.73%\n39.13%\n\n4.3 QUALITATIVE RESULTS\n\nNext, we discuss qualitative examples showing the improvements from ImProver in proof optimiza-\ntion.Original (human-written)\n\nImProver (readability-optimized)\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ndef iso1 [Fintype G] (h : Disjoint H K) (h' :\n\ncard G = card H * card K)\n\n: K ≃* G / H := by\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\n· rw [← ker_eq_bot_iff, (QuotientGroup.mk'\n\nH).ker_restrict K]\nsimp [h]\n\n· symm\n\nexact aux_card_eq h'\n\ncard G = card H * card K)\n:= by\n\n: K ≃* G / H\n\nhave injectivity : Function.Injective\n\n((QuotientGroup.mk' H).restrict K) := by\nrw [← ker_eq_bot_iff, (QuotientGroup.mk'\nH).ker_restrict K]\nsimp [h]\n\nhave card_eq : card (G / H) = card K := by\n\nexact aux_card_eq h'\n\napply MulEquiv.ofBijective\n\n((QuotientGroup.mk' H).restrict K)\nrw [bijective_iff_injective_and_card]\nconstructor\nexact injectivity\nsymm\nexact card_eq\n\nFigure 3: Optimizing a group-theoretic result from MIL Chapter 8 Section 1 for readability.Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.LeanDojo: Theorem proving with retrieval-augmented\nlanguage models.In Neural Information Processing Systems (NeurIPS), 2023.the lean ﬁle contents/imports leading up\nto the theorem declaration) wrapped by <CONTEXT>...</CONTEXT>.Each of these previous results will be wrapped with <PREV I=0></PREV\nI=0>,...,<PREV I=num_prev-1></PREV I=num_prev-1>, with I=num_prev-1 being the\nmost recent result.Remember to use lean 4 syntax, which has signiﬁcant changes from the lean 3 syntax.You will also receive num_mathlib_docs documents relevant to the current theorem to\nhelp with formulating your modiﬁed proof.A.2 METRIC PROMPTS\n\nLength Metric\n\nSystem: You are an AI assistant who shortens Lean 4 proofs while ensuring their correct-\nness.You will aim to reduce the number of lines of the tactic proof while ensuring that it\nproperly compiles in Lean 4.15\n\n\fUser: Shorten the current theorem (wrapped in <CURRENT>...</CURRENT>) to be as\nshort in length—measured in the number of lines of the proof—as possible, while also\nensuring that the output is still syntactically correct.\"System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.You will receive a Lean 4 proof you must\nmodify to eliminate any errors so that it compiles correctly and eliminate any “sorry”s with\nfull proofs.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a\nformal, complete, and correct Lean 4 proof by ﬁlling in its tactic proof.B ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this section, we provide more detailed information on the experimental setup and results used to\nevaluate ImProver.string\nTrue\nstring\nFalse\nstring list True\nstring list\nFalse\nstring tree\nTrue\nstring tree\nFalse\n\n7.53\n7.03\n8.04\n7.04\n7.62\n6.31\n\n16.12\n19.67\n12.38\n13.58\n15.34\n14.17\n\n46.72%\n35.77%\n64.96%\n51.82%\n49.64%\n44.53%\n\n16.79%\n15.33%\n21.17%\n18.98%\n18.25%\n16.06%\n\nBy Table 8, we see that the optimal combination in this testing group is a string list output\nformat with CoS enabled.Fix these values for all future tests.Examples\n\nTable 9: Example Retrieval Ablations\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.0\n3\n5\n7\n10\n\n5.67\n8.49\n8.38\n7.56\n9.34\n\n67.15%\n62.04%\n64.96%\n62.77%\n63.5%\n\n16.79%\n19.71%\n21.17%\n19.71%\n21.9%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the number of examples.By Table 9,\nwe see that the optimal combination in this testing group is 10 examples.8.44\n13.68\n12.9\n12.04\n14.7\n\n16\n\n\fTable 10: Sampling Method Ablations\n\nMethod\n\nForward Keep Best\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.None\nreﬁnement\nreﬁnement\nreﬁnement\nreﬁnement\nbest-of-n\n\nN/A\n1\n5\n1\n5\nN/A\n\nN/A\nFalse\nFalse\nTrue\nTrue\nN/A\n\n9.34\n14.76\n12.5\n14.95\n13.15\n15.35\n\n14.7\n30.63\n20.88\n14.95\n13.15\n18.44\n\n63.5%\n48.18%\n59.85%\n100.0%\n100.0%\n83.21%\n\n21.9%\n30.66%\n30.66%\n30.66%\n29.93%\n36.5%\n\nNote that forward and keep-best values are parameters for reﬁnement of how many previous itera-\ntions to forward, and whether to keep the most recent or the best iteration in subsequent reﬁnement\nsteps.Now, with the previous optimal parameters ﬁxed, run the ablation on the sample method.By\nTable 11, we see that the optimal combination in this testing group is GPT-4o with n = 15.Fix this\nvalue for all future tests.33.78\n31.23\n31.85\n31.35\n32.66\n32.88\n34.88\n29.54\n29.64\n28.25\n\n17\n\n\fCompﬁles: Length Optimization See (§4.3)\n\nCompﬁles: Readability Optimization Consider Figure 5, in which a lemma from the 2019 IMO\nproblem 1 (from the Compﬁles dataset) is optimized for readability.This introduces multiple new\nhypotheses, which generalize a linear_property of the functions, and then reuses and instanti-\nates that (and others, too) hypothesis throughout the proof, creating a signiﬁcantly more declaritive,\nmodular, and therefore readable proof.Original (human-written)\n\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\nrefine hf 1, fun a => ?_i\nchange g a = g 1 * a\nrw [mul_comm, ← smul_eq_mul, ←\n\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nImProver (readability-optimized)\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\n\nhave linear_property : ∀ a, f a = g a := by\n\nintro a\nrfl\n\nhave g_smul : ∀ a, g a = g 1 * a := by\n\nintro a\nrw [mul_comm, ← smul_eq_mul, ←\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nrefine hf 1, fun a => ?_i\nhave f_eq_g : f a = g a := linear_property a\nhave g_a_eq : g a = g 1 * a := g_smul a\nrw [f_eq_g, linear_property 1, g_a_eq]\n\nFigure 5: Optimizing a lemma from IMO 2019 P1 for readability\n\nMIL: Length Optimization Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, eliminating simp calls and introducing proof terms\ninto the structure of the proof to shorten it from 9 tactic invocations to 7.Note that the model does not have access to the Lean\ncommands that symbolically generate proof terms, and therefore generates and estimates the proof\nterm entirely by itself.MIL: Readability Optimization See (§4.3)\n\nMathlib: Length Optimization Consider Figure 8, which optimizes a theorem in algebraic topol-\nogy from mathlib for length, eliminating simp calls and combining tactics to shorten it from 3 tactic\ninvocations to 1.This original proof carried a score of 0, as it does not contain any declarative statements.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.This intermediate tactic greatly helps in the understandability and clarity of the proof.",
          "time": 747.9048473834991,
          "input_token_size": 16876,
          "output_token_size": 123,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_e68f9dfc",
        "created_at": 1750198605.2554216,
        "processing_time": 753.0821633338928,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.845336"
      }
    }
  },
  {
    "ea9d3b18": {
      "question": "Which datasets were used to test ImProver?",
      "answer": "1. Mathematics in Lean (MIL)\n2. Compfiles\n3. Mathlib",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116
          ],
          "provenance_ids": [
            98,
            102,
            104,
            105,
            106,
            110,
            114
          ],
          "provenance": "Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.This dataset contains theorems of a competitive format, and although they contain\n\n5\n\n\fconcepts only at a high-school level, the logical complexity of internationally-renowned competi-\ntion results is far above that.For our main results, we used all 26 theorems and lemmas from the\nCompﬁles database of complete solutions to the International Mathematics Olympiad (IMO) and\nthe American Mathematics Olympiad (USAMO) from 2016-2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.This is the most difﬁcult dataset. Models.We also provide instructions, context, and in-\nformation depending on the features selected, and add the theorem and proof to the prompt.",
          "time": 23.894208192825317,
          "input_token_size": 21493,
          "output_token_size": 184,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_ea9d3b18",
        "created_at": 1750198641.429912,
        "processing_time": 27.51177453994751,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 23,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 23,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.846344"
      }
    }
  },
  {
    "f5b443ad": {
      "question": "What does ImProver outperform in terms of datasets and metrics?",
      "answer": "ImProver outperforms in the following datasets and metrics:\n\n1. Datasets:\n   - Mathematics in Lean (MIL)\n   - Compfiles\n   - Mathlib\n\n2. Metrics:\n   - Length\n   - Readability",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187
          ],
          "provenance_ids": [
            8,
            9,
            10,
            91,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            111,
            113,
            114,
            115,
            118,
            119,
            120,
            122,
            124,
            126,
            129,
            131,
            132,
            133,
            135,
            143,
            151,
            155,
            157,
            158,
            159,
            169,
            173,
            174,
            175,
            176
          ],
          "provenance": "We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.In\npractice, however, informal, natural-language proofs are prone to imprecision, ambiguity, and error.4 EXPERIMENTS\n\nWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics and compare its results to those of the base GPT-4o and GPT-4o-mini\nmodels.Datasets. We evaluate ImProver on subsets of the following datasets.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.We use exercise solutions from set theory, elementary number theory, group\ntheory, topology, differential calculus, and integration & measure theory.This dataset contains the-\norems at an undergraduate-level of complexity.For our main results, we evaluated on 72 theorems\nfrom exercise solutions from MIL chapters 4, 5, 8, 9, and 10.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.This is a\ndataset of internationally-renowned competitive math problems, many of which are readable, yet\nquite verbose.This dataset contains theorems of a competitive format, and although they contain\n\n5\n\n\fconcepts only at a high-school level, the logical complexity of internationally-renowned competi-\ntion results is far above that.For our main results, we used all 26 theorems and lemmas from the\nCompﬁles database of complete solutions to the International Mathematics Olympiad (IMO) and\nthe American Mathematics Olympiad (USAMO) from 2016-2024.Mathlib (mathlib Community, 2020): Mathlib contains many advanced results at the forefront of\nmathematics, and has been at the center of research-level formalizations.These proofs are extremely\nefﬁcient, concise, and generalized - which often comes at the cost of readability and understandabil-\nity.These results and theorems often are at the cutting edge of research.For our main results, we evaluated our methods on 43 advanced research-level proofs from\nMathlib/AlgebraicTopology/FundamentalGroupoid.Our base generator uses GPT-4o (OpenAI et al., 2024).Additionally, for a given metric, we write a prompt that brieﬂy\ndescribes the metric and the proof optimization task.We also provide instructions, context, and in-\nformation depending on the features selected, and add the theorem and proof to the prompt.Speciﬁc\nprompt information is detailed in (§A)\n\nPerformance metrics.For\nreadability, we use the difference, µread(y)−µread(yo).If no correct output is generated by the model\nfor a speciﬁc theorem, improvement is deﬁned to be zero.We deﬁne nonempty improvement as\nthe improvement restricted to theorems for which some output has nonzero improvement.The nonempty improvement score is the expected improvement\nin metric score, given that there are no errors in the generation.Additionally, the accuracy is the percentage of theorems in the dataset which the model was able to\ngenerate a correct output for.4.1.1 ABLATIONS\n\nWhen performing our ablation studies, we used a ﬁxed dataset (MIL) and metric (length) and varied\nthe parameters of all the features to ﬁnd the optimal combination.We deﬁne the following testing groups with the speciﬁed parameter combina-\ntions:\n\nGPT-4o-mini/GPT-4o: This varies the GPT-4o model, outputting a string with no other features.Example Retrieval: We evaluate the effects of increasing the number of examples provided (multi-\nshot prompting) in the range of 0, 3, 5, 7, and 10, with the model ﬁxed as GPT-4o, CoS and output\nformatting ﬁxed as the best combination from the previous test, and no other features enabled.Sampling Method: Here, we evaluate the effects of best-of-n and reﬁnement for a ﬁxed n = 5.Additionally we test on the reﬁnement cases if forwarding the most recent iteration result, or all\nprevious iteration results is the best, and if we should keep the best out of the iterations, or the most\n\n6\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nMetric\n\nModel\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n3.7\n20.96\n\n2.21\n9.34\n\n6.25\n30.54\n\n4.18\n13.45\n\n15.15\n55.29\n\n8.02\n30.53\n\n18.58\n56.56\n\n14.48\n30.97\n\n26.36%\n100.0%\n\n18.75%\n100.0%\n\n8.31%\n35.44%\n\n6.13 %\n24.56%\n\n37.5%\n100.0%\n\n28.85%\n100.0%\n\n14.42%\n50.0%\n\n11.54%\n34.21%\n\nTable 2: MIL Proof optimization results.Model, CoS, output for-\nmatting, examples, n, and sampling method are ﬁxed as the best combination from the previous\ntest.Additionally, from Table 2, Table 3, and Table 4, we can see that ImProver outperforms\nGPT-4o on each dataset as well.However, 35.44% of the time, it generates a correct output that is not the same length\nas the input, and in that case, we expect an average of a 55.29% reduction in length.7\n\n\fMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Table 3: Compﬁles Proof optimization results.Length\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\nLength\n\nReadability\n\nGPT-4o\nImProver\n\nGPT-4o\nImProver\n\n2.75\n18.86\n\n0.39\n5.74\n\n0.0\n6.19\n\n0.0\n4.63\n\n30.7\n54.48\n\n3.38\n24.89\n\n0.0\n53.65\n\n0.0\n33.19\n\n11.54%\n100.0%\n\n14.1%\n100.0%\n\n5.13%\n34.62%\n\n1.28%\n19.23%\n\n16.67%\n100.0%\n\n4.65%\n100.0%\n\n0.0%\n11.54%\n\n0.0%\n11.63%\n\nMetric\n\nModel\n\nImprovement Nonempty Improvement Accuracy\n\nImproved Acc.Additionally noting Table 2, Table 3, and\nTable 4, we observe that the improvement score for both metrics for both GPT-4o and ImProver\nis highest for the MIL dataset, lower for Compﬁles, and the lowest on the Mathlib theorems.This suggests that although the base GPT-4o generator is less likely to\ngenerate a correct output for higher difﬁculty datasets, the improvements that ImProver makes to\nthe base generator allows it to maintain its improvement in the metric score whenever a correct out-\nput is generated.As such, we can speculate that the bottleneck in the improvement score is not the\nmodel’s ability to optimize the proof for a metric, but rather its ability to generate a new correct\nproof at all.As such, we conjecture that with more capable generator models, the accuracy – and\nthus, the improvement score – in optimization tasks will continue to increase, until the improvement\nscores match the nonempty improvement.Overall, we conclude that although the performance of both ImProver and GPT-4o decreases on\nlength and readability optimization on more difﬁcult datasets, ImProver signiﬁcantly outperforms\nGPT-4o on all datasets for length and readability optimization.",
          "time": 171.3220956325531,
          "input_token_size": 15450,
          "output_token_size": 133,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_f5b443ad",
        "created_at": 1750198721.5922022,
        "processing_time": 176.07017850875854,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 188,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 188,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.846344"
      }
    }
  },
  {
    "f63b1c97": {
      "question": "What problem is associated with human-written proofs according to the document?",
      "answer": "Human-written proofs may not be optimal for learning tasks.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4
          ],
          "provenance_ids": [
            4
          ],
          "provenance": "Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.",
          "time": 5.284909009933472,
          "input_token_size": 16124,
          "output_token_size": 74,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_f63b1c97",
        "created_at": 1750198677.643821,
        "processing_time": 12.693246603012085,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 5,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 5,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.846344"
      }
    }
  },
  {
    "f73d45a9": {
      "question": "What does Chain-of-States prompting help in constructing?",
      "answer": "Chain-of-States prompting helps in constructing optimized proofs by making intermediate proof states explicit.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            1,
            2,
            4,
            21,
            23,
            30,
            33,
            36,
            37,
            41,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            53,
            54,
            56,
            57,
            63,
            68,
            79,
            82,
            83,
            85,
            86,
            87,
            92,
            97,
            98,
            99,
            101,
            102,
            197,
            198,
            216,
            217,
            218,
            219,
            221,
            222,
            224,
            225,
            227,
            228,
            229,
            230,
            231,
            233,
            234,
            235,
            236,
            237,
            239,
            240,
            242,
            243,
            244,
            245,
            246,
            247,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            329,
            335,
            336,
            337,
            338,
            339,
            341,
            342,
            365,
            373,
            374
          ],
          "provenance": "0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.1\n\n\fWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more readable, and more declarative in style.1\n\nOriginal (human-written)\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\nImProver (length-optimized)\n\nlemma lemma0 {α : Type} {p : α → α → Prop}\n\n(h1 : ∀ x, ∃!y, p x y)\n(h2 : ∀ x y, p x y ↔ p y x) :\n∀ x, Classical.choose\n\n(h1 (Classical.choose (h1\n\nx).exists)).exists=x := by\n\n-- PROOF START\nintro x\nobtain hy, h1e, h1ui := h1 x\nhave h2' : Classical.choose (h1 x).exists =\n\ny :=\nh1u _ (Classical.choose_spec (h1\nx).exists)\n\nrw [h2']\nobtain hw, h1e', h1u'i := h1 y\nhave h4 := Classical.choose_spec (h1\n\nhave hxw : x = w := by\n\ny).exists\n\napply h1u'\nrw [h2]\nexact h1e\n\nrw [hxw]\nexact h1u' _ h4\n\nFigure 1: ImProver automatically rewrites formal proofs to optimize a criterion such as length\nor readability while remaining correct.We consider the\nnew problem of rewriting a proof to optimize a metric, such as rewriting a proof into a more read-\nable or more concise one.Our model,\nImProver, builds on neural theorem proving techniques including full proof generation (Jiang et al.,\n2023; First et al., 2023), conditioning on example proofs (Jiang et al., 2023), retrieval (Yang et al.,\n2023; Thakur et al., 2024), and preceding ﬁle context (First et al., 2023; Hu et al., 2024), as well as\nerror correction (Madaan et al., 2023; Chen et al., 2023) and documentation retrieval (Zhou et al.,\n2023) from code generation.Given a theorem statement x, additional context c, and an initial\nproof y0, proof optimization consists of generating a new proof y that is correct and minimizes (or\nmaximizes) a metric µ(x, c, y0, y) → R.\n\n1Code is available at https://github.com/riyazahuja/ImProver.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.Concretely, we evaluate this using the ratio of number of explicitly typed\nhave tactics to total number of tactic invocations.3.1\n\nIMPROVER\n\nWe develop several improvements that can be applied to a black-box LLM generator yout ∼\nG(·|xin), such as GPT-4 (OpenAI et al., 2024), and specify ImProver with respect to these pa-\nrameters.The explicit prompts and templates that are sent to the LLM can be found in (§A).3.1.1 CHAIN-OF-STATES PROMPTING\n\nTypical formal proofs are a sequence of tactics (akin to steps) and states that show the hypotheses and\ngoals at each step.The intermediate states often contain valuable information (e.g., an expression\nafter it has been simpliﬁed) that is not present in the tactics.To allow the model to reason about\nthese intermediate goals and hypotheses, we use tools from Lean metaprogramming to automatically\nannotate each proof state as a comment prior to each tactic.We refer to this method as Chain-of-\nStates (CoS) prompting since it makes intermediate states explicit, akin to how chain-of-thought\nprompting (Wei et al., 2022) makes intermediate steps of a solution explicit.These states are extracted directly and symbolically from the underlying Lean compilation steps\nusing Lean’s rich metaprogramming suite.Speciﬁcally, in the compiler’s elaboration and eval-\nuation stages – where the parsed theorem code is ﬁrst converted into concrete syntax trees (in\npractice, Syntax objects) and abstract syntax trees (Expr objects) – we convert the CST and\nAST output objects into the relevant proof data and proof states in the form of proof trees\n(Lean.Elab.InfoTree).These proof trees contain detailed context and information on a tactic-\nby-tactic level relating to the modiﬁcation of the proof state, metavariable context, and proof cor-\nrectness.Figure 2 shows an example. This explicit reasoning aims to help the generator model construct more optimized proofs via addi-\ntional symbolic data.3.1.2 OUTPUT FORMATTING. LLM outputs often contain ancillary and syntactically invalid content, especially before and after\nthe actual proof.use xs; right; exact xu\n\nrintro x (hxs, xti | hxs, xui)\n/-\ncase inl.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxt : x ∈ t\n⊢ x ∈ s ∩ (t ∪ u)\ncase inr.intro\nα : Type u_1\ns t u : Set α\nx : α\nxs : x ∈ s\nxu : x ∈ u\n⊢ x ∈ s ∩ (t ∪ u)\n-/\n· use xs; left; exact xt\n/-\nGoals Solved!max(y, y′, key: x 7→ µ(x)), E(y) = E(y′) = 0\ny,\ny′,\nmin(y, y′, key: x 7→ E(x)), E(y) = E(y′) > 0\n\nE(y) = 0, E(y′) > 0\nE(y) > 0, E(y′) = 0\n\nS(y, y′) =\n\n\n\n\n\n\nWhere µ(x) is the metric score of x, and E(x) is the number of errors in x.For example:\n\n4\n\n\fbest_of_n((refinement,m),n) is a compound sampling method that run a best-of-n,\nwhere each call is a m-step reﬁnement.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.More speciﬁcally, example retrieval selects the most relevant user-generated examples of proof op-\ntimization on a speciﬁc metric.The number of examples that are retrieved is user-speciﬁed.Document retrieval extracts information using MMR from a pair of ﬁxed (vector) databases.The\ndatabases store semantically chunked data from the Theorem Proving in Lean (TPiL) handbook –\ncontaining syntax guides and tactic explanations – and the Mathlib mathematics libary – containing\nthousands of theorems and lemmas.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.Datasets. We evaluate ImProver on subsets of the following datasets.Mathematics in Lean (MIL) (leanprover-community, 2024):\nthis dataset contains pedagogical so-\nlutions of common undergraduate-level exercises, and as such contains many readable, yet verbose\nand inefﬁcient proofs.We use exercise solutions from set theory, elementary number theory, group\ntheory, topology, differential calculus, and integration & measure theory.For our main results, we evaluated on 72 theorems\nfrom exercise solutions from MIL chapters 4, 5, 8, 9, and 10.Compﬁles (David Renshaw, 2024): Solutions of International Mathematics Olympiad (IMO) and\nAmerican Mathematics Olympiad (USAMO) competition problems from 2016 to 2024.We conﬁrm this result by considering Table 6 and observe that simply enabling CoS nearly doubles\nthe improvement score, and signiﬁcantly improves the nonempty improvement score, suggesting that\nCoS has a high degree of impact on optimizing for the readability metric, as conjectured.However,\nwe also note a signiﬁcant increase in improved accuracy, which suggests that embedding the chain\nof states also improves the ability of the model to generate nontrivial correct outputs, implying that\nthe symbolic information contained in the states are critical to effectively modifying the structure\nand content of a proof.This original proof carried a score of 0, as it does not contain any declarative statements.In compar-\nison, after applying ImProver, we transformed the proof to be more declarative, with many more\nintermediate steps with explicit have tactics for improved clarity.Additionally observe how the\nmodel deﬁnes hypotheses for use in the latter half of the proof; these predeﬁned hypotheses could\neasily be converted into standalone lemmas for reuse.Example 3: Full Proof Generation.This theorem relating to group theory originally has no proof, however, ImProver generates one\nfrom scratch.This generated proof is veriﬁed to be correct by Lean, utilizing all the included hy-\npotheses as well as a retrieved mathlib theorem.5 CONCLUSION\n\nIn this paper, we introduced ImProver, a novel agent-based tool for automated proof optimization\nin Lean.By incorporating CoS, RAG, and other features, ImProver signiﬁcantly outperforms base\nlanguage models in proof optimization over undergraduate, competition, and research-level prob-\nlems.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.As such, we believe that ImProver sets the stage for further\nwork on proof optimization to advance the study and use of AI in mathematics.ACKNOWLEDGEMENTS\n\nRiyaz Ahuja thanks the L3 Lab and Hoskinson Center for Formal Mathematics for their support.Sean Welleck thanks Convergent Research, the Lean FRO, and the OpenAI Researcher Access Pro-\ngram for their support.achieves\nolympiad\n\nmathematical\n\ninternational\n\nsolving\n\nTeams.AI\n\nSerge Autexier and Dominik Dietrich.A tactic language for declarative proofs.In Matt Kaufmann\nand Lawrence C. Paulson (eds.), Interactive Theorem Proving, pp.Jaime Carbonell and Jade Goldstein.The use of mmr, diversity-based reranking for reordering doc-\numents and producing summaries.335–336,\nISBN 1581130155. doi:\nNew York, NY, USA, 1998.Association for Computing Machinery. 10.1145/290941.291025.URL https://doi.org/10.1145/290941.291025.Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.Teaching large language models\n\nto self-debug, 2023.URL https://arxiv.org/abs/2304.05128. David Renshaw. compﬁles.Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun.Baldur: Whole-proof generation and\n\nrepair with large language models, 2023.Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu.Proof artifact co-\nIn International Conference on Learning\n\ntraining for theorem proving with language models.Representations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.Jiewen Hu, Thomas Zhu, and Sean Welleck.minictx: Neural theorem proving with (long-)contexts,\n\n2024.URL https://arxiv.org/abs/2408.03350.Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu.Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs.In The Eleventh International Conference on Learning\nRepresentations, 2023.URL https://openreview.net/forum?id=SMa9EAovKMC.Kim Morrison. lean-training-data.https://github.com/kim-em/lean-training-data,\n\nGuillaume Lample, Timothee Lacroix, Marie anne Lachaux, Aurelien Rodriguez, Amaury\nHypertree proof search\nHayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.for neural\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL\nhttps://openreview.net/forum?id=J4pX8Q8cxHH. theorem proving. 2024. 2024.Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie\n\nSi.A survey on deep learning for theorem proving, 2024.Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang.In Anna Rogers, Jordan Boyd-Graber, and Naoaki\nOkazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp.14605–14631, Toronto, Canada, July 2023.Association for Computational Linguistics.10.18653/v1/2023.acl-long.817.URL\nhttps://aclanthology.org/2023.acl-long.817.doi:\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-reﬁne:\nIterative reﬁnement with self-feedback.In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.URL https://openreview.net/forum?id=S37hOerQLB.12\n\nleanprover-community. mathematics_in_lean.https://github.com/leanprover-community/mathematics_in_lea\n\n\fThe mathlib Community.The lean mathematical library.ACM, January 2020. doi:\n10.1145/3372885.3373824.URL http://dx.doi.org/10.1145/3372885.3373824.Leonardo de Moura and Sebastian Ullrich.The lean 4 theorem prover and programming lan-\nIn Automated Deduction – CADE 28: 28th International Conference on Automated\nguage.Deduction, Virtual Event, July 12–15, 2021, Proceedings, pp.625–635, Berlin, Heidelberg,\n2021. Springer-Verlag.ISBN 978-3-030-79875-8. doi: 10.1007/978-3-030-79876-5_37.URL\nhttps://doi.org/10.1007/978-3-030-79876-5_37.The current theorem will be\nwrapped in <CURRENT>...</CURRENT>\nSystem: Output format instructions\nPlaceholder: All retrieved syntax documentation\nPlaceholder: All retrieved mathlib documentation\nPlaceholder: All retrieved examples\nUser: <CONTEXT> context </CONTEXT>\nPlaceholder: Previous results and inputs/outputs\nPlaceholder: All metric prompts with a ‘User’ role\nUser: <CURRENT> theorem </CURRENT>\n\nThis prompt is then invoked and sent to the language model by ﬁlling in all the variables and place-\nholders.We measure readablity by considering the ratio of the number\nofexplicitly typed have tactics against the total number of tactics in the proof, as this is\nproportional to whether a proof is declarative in style, and thus, readable.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is\nmore readable and declarative and modular.System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.You will receive a Lean 4 proof you must\nmodify to eliminate any errors so that it compiles correctly and eliminate any “sorry”s with\nfull proofs.User: Rewrite the current theorem (wrapped in <CURRENT>...</CURRENT>) so it is a\nformal, complete, and correct Lean 4 proof by ﬁlling in its tactic proof.B.1 ABLATION DETAILS\n\nWe now proceed to show detailed results from our ablation testing.Table 8: Output and Chain-of-States Ablations\n\nOutput Format\n\nCoS\n\nImprovement Nonempty Improve. Accuracy\n\nImproved Acc.Original (human-written)\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\n\n:= by\n\next x\nsimp only [mem_map]\nconstructor\n· rintro hy, y_in, hyi\n\nexact hϕ y, hy, y_in, rfli, hyi\n\n· rintro hy, hz, z_in, hzi, hyi\n\nuse z, z_in\ncalc ψ.comp ϕ z = ψ (ϕ z) := rfl\n_\n= ψ y := by congr\n\nImProver (length-optimized)\n\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\nby\next x\nsimp only [mem_map]\nconstructor\nrintro hy, y_in, hyi; exact hϕ y, hy, y_in,\n\n:=\n\nrfli, hyi\n\nrintro hy, hz, z_in, hzi, hyi; exact hz,\nz_in, (congr_arg ψ hz).trans hyi\n\nFigure 6: Optimizing a lemma from the solutions of MIL CH08 S01 for length\n\nMIL: Length Optimization 2 Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, converting a full tactic proof into a single proof term\nto shorten it from 28 tactic invocations to 1.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.",
          "time": 253.49941182136536,
          "input_token_size": 16872,
          "output_token_size": 54,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_f73d45a9",
        "created_at": 1750198656.0058484,
        "processing_time": 256.874849319458,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.847343"
      }
    }
  },
  {
    "f774fbc4": {
      "question": "What is the main goal of the ImProver tool?",
      "answer": "The main goal of the ImProver tool is to automate proof optimization by rewriting formal proofs to optimize for arbitrary user-defined metrics, such as length or readability, while ensuring correctness.",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            0,
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            17,
            18,
            19,
            20,
            21,
            22,
            23,
            24,
            25,
            26,
            27,
            28,
            29,
            30,
            31,
            32,
            33,
            34,
            35,
            36,
            37,
            38,
            39,
            40,
            41,
            42,
            43,
            44,
            45,
            46,
            47,
            48,
            49,
            50,
            51,
            52,
            53,
            54,
            55,
            56,
            57,
            58,
            59,
            60,
            61,
            62,
            63,
            64,
            65,
            66,
            67,
            68,
            69,
            70,
            71,
            72,
            73,
            74,
            75,
            76,
            77,
            78,
            79,
            80,
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91,
            92,
            93,
            94,
            95,
            96,
            97,
            98,
            99,
            100,
            101,
            102,
            103,
            104,
            105,
            106,
            107,
            108,
            109,
            110,
            111,
            112,
            113,
            114,
            115,
            116,
            117,
            118,
            119,
            120,
            121,
            122,
            123,
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131,
            132,
            133,
            134,
            135,
            136,
            137,
            138,
            139,
            140,
            141,
            142,
            143,
            144,
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155,
            156,
            157,
            158,
            159,
            160,
            161,
            162,
            163,
            164,
            165,
            166,
            167,
            168,
            169,
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177,
            178,
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186,
            187,
            188,
            189,
            190,
            191,
            192,
            193,
            194,
            195,
            196,
            197,
            198,
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207,
            208,
            209,
            210,
            211,
            212,
            213,
            214,
            215,
            216,
            217,
            218,
            219,
            220,
            221,
            222,
            223,
            224,
            225,
            226,
            227,
            228,
            229,
            230,
            231,
            232,
            233,
            234,
            235,
            236,
            237,
            238,
            239,
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248,
            249,
            250,
            251,
            252,
            253,
            254,
            255,
            256,
            257,
            258,
            259,
            260,
            261,
            262,
            263,
            264,
            265,
            266,
            267,
            268,
            269,
            270,
            271,
            272,
            273,
            274,
            275,
            276,
            277,
            278,
            279,
            280,
            281,
            282,
            283,
            284,
            285,
            286,
            287,
            288,
            289,
            290,
            291,
            292,
            293,
            294,
            295,
            296,
            297,
            298,
            299,
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307,
            308,
            309,
            310,
            311,
            312,
            313,
            314,
            315,
            316,
            317,
            318,
            319,
            320,
            321,
            322,
            323,
            324,
            325,
            326,
            327,
            328,
            329,
            330,
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340,
            341,
            342,
            343,
            344,
            345,
            346,
            347,
            348,
            349,
            350,
            351,
            352,
            353,
            354,
            355,
            356,
            357,
            358,
            359,
            360,
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "provenance_ids": [
            1,
            2,
            3,
            4,
            5,
            6,
            7,
            8,
            9,
            10,
            11,
            12,
            13,
            14,
            15,
            16,
            31,
            35,
            36,
            37,
            44,
            76,
            81,
            82,
            83,
            84,
            85,
            87,
            92,
            93,
            94,
            225,
            227,
            228,
            233,
            235,
            238,
            243,
            253,
            254,
            257,
            258,
            259,
            260,
            261,
            269,
            277,
            278,
            279,
            280,
            282,
            283,
            285,
            286,
            294,
            295,
            296,
            305,
            306,
            307,
            308,
            337,
            359,
            360,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369,
            370,
            371,
            372,
            373,
            374,
            375
          ],
          "provenance": "0\n1\n4\n2\n:\nv\ni\nX\nr\na\n\nIMPROVER: AGENT-BASED AUTOMATED\nPROOF OPTIMIZATION\n\nRiyaz Ahuja\nCarnegie Mellon University\n\nJeremy Avigad Prasad Tetali Sean Welleck\n\nABSTRACT\n\nLarge language models (LLMs) have been used to generate formal proofs of math-\nematical theorems in proofs assistants such as Lean.However, we often want to\noptimize a formal proof with respect to various criteria, depending on its down-\nstream use.For example, we may want a proof to adhere to a certain style, or to be\nreadable, concise, or modularly structured.Having suitably optimized proofs is\nalso important for learning tasks, especially since human-written proofs may not\noptimal for that purpose.To this end, we study a new problem of automated proof\noptimization: rewriting a proof so that it is correct and optimizes for an arbitrary\ncriterion, such as length or readability.As a ﬁrst method for automated proof opti-\nmization, we present ImProver, a large-language-model agent that rewrites proofs\nto optimize arbitrary user-deﬁned metrics in Lean.We ﬁnd that naively applying\nLLMs to proof optimization falls short, and we incorporate various improvements\ninto ImProver, such as the use of symbolic Lean context in a novel Chain-of-\nStates technique, as well as error-correction and retrieval.We test ImProver on\nrewriting real-world undergraduate, competition, and research-level mathematics\ntheorems, ﬁnding that ImProver is capable of rewriting proofs so that they are\nsubstantially shorter, more modular, and more readable.1\n\nINTRODUCTION\n\nThe fundamental virtue of a mathematical proof is that it provides certainty: a deductive argument\nshows that the assumptions of a mathematical statement logically guarantee the conclusion.In\npractice, however, informal, natural-language proofs are prone to imprecision, ambiguity, and error.Using a formal language such as Lean (Moura & Ullrich, 2021) removes ambiguity and precision\nand enables a proof assistant to verify correctness down to the primitives of a formal axiomatic\nsystem.Formal proofs, however, can be hard to read and often suffer from low reusability or excessive detail.For example, formal proofs in Lean’s extensive mathematical library, Mathlib (mathlib Community,\n2020), are generally designed to be concise and very general, often at the expense of readability.Formal proofs in an expository text, in contrast, may include detailed calculations steps, making\nthem readable but verbose.Machine learning systems trained on such proofs learn to mimic these\nvaried conventions (Hu et al., 2024), which may not be the optimal use of the limited supply of\nhuman-written proofs.As a result, we would like to be able to automatically refactor proofs to meet\na secondary objective such as length or readability.Proof optimization is more general than theorem proving, since we can\nalso rewrite an empty proof to optimize correctness.3 AUTOMATED PROOF OPTIMIZATION WITH ImProver\n\nAutomated Proof Optimization.Given a theorem statement x, additional context c, and an initial\nproof y0, proof optimization consists of generating a new proof y that is correct and minimizes (or\nmaximizes) a metric µ(x, c, y0, y) → R.\n\n1Code is available at https://github.com/riyazahuja/ImProver.2\n\n\fBy varying the metric, we can perform tasks such as shortening proofs, making them more readable,\nor even automated proving.The completion metric is used for concretely\nviewing proof optimization as a generalization of neural theorem proving.Similar to the best-of-n tech-\nnique, it relies on an argument n for the number of reﬁnement steps, and is curried such that each\nreﬁnement step can be handled by any other prompting function.Note that with each of these compound prompt functions, there are always a total of mn iterations.3.1.4 RETRIEVAL\n\nImProver uses MMR (Maximum Marginal Relevance)-based (Carbonell & Goldstein, 1998)\nretrieval-augmented generation to select relevant examples and documents.More speciﬁcally, example retrieval selects the most relevant user-generated examples of proof op-\ntimization on a speciﬁc metric.Namely, each metric is loaded with a cached (vector) database\npopulated with human-made examples of preoptimized and postoptimized pairs of Lean theorems.The number of examples that are retrieved is user-speciﬁed.The\ndatabases store semantically chunked data from the Theorem Proving in Lean (TPiL) handbook –\ncontaining syntax guides and tactic explanations – and the Mathlib mathematics libary – containing\nthousands of theorems and lemmas.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.4.1 SETUP\n\nOur experimentation is split into three distinct stages.We ﬁrst perform ablation testing on the\nImProver model parameters (§3.1) to ensure that ImProver’s parameter speciﬁcation is the optimal\none with respect to correctness and metric optimization score.By incorporating CoS, RAG, and other features, ImProver signiﬁcantly outperforms base\nlanguage models in proof optimization over undergraduate, competition, and research-level prob-\nlems.We intend to address this inefﬁciency in future work by applying ﬁne-tuning\nand RL on a smaller model to match performance at a lower cost.ImProver demonstrates its ability to generate substantially shorter, more readable, and modular\nproofs while maintaining correctness.achieves\nolympiad\n\nmathematical\n\ninternational\n\nsolving\n\nTeams.A tactic language for declarative proofs.99–114, Berlin, Heidelberg,\n2010. Springer Berlin Heidelberg.Association for Computing Machinery. 10.1145/290941.291025.Representations, 2022. URL https://openreview.net/forum?id=rpxJc9j04U.Jiewen Hu, Thomas Zhu, and Sean Welleck.Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li,\nMateja Jamnik, Guillaume Lample, and Yuhuai Wu.Draft, sketch, and prove: Guiding formal\ntheorem provers with informal proofs.In The Eleventh International Conference on Learning\nRepresentations, 2023.URL https://openreview.net/forum?id=SMa9EAovKMC.Kim Morrison. lean-training-data.A survey of deep\nlearning for mathematical reasoning.Self-reﬁne:\nIterative reﬁnement with self-feedback.In Thirty-seventh Conference on Neural Information Pro-\ncessing Systems, 2023.URL https://openreview.net/forum?id=S37hOerQLB.12\n\nleanprover-community. mathematics_in_lean.The lean mathematical library.In Proceedings of the 9th ACM SIGPLAN\nInternational Conference on Certiﬁed Programs and Proofs, POPL ’20.URL http://dx.doi.org/10.1145/3372885.3373824.Leonardo de Moura and Sebastian Ullrich.Stanislas Polu and Ilya Sutskever.Generative language modeling for automated theorem proving,\n\n2020.13\n\n\fStanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya\n\nSutskever.In Stefano Berardi, Mario Coppo, and Ferruccio Damiani\n(eds.), Types for Proofs and Programs, pp.378–393, Berlin, Heidelberg, 2004.Springer Berlin\nHeidelberg. ISBN 978-3-540-24849-1.System: You are an AI assistant who automatically solves Lean 4 proofs (as in, generates\nthe tactic proof) and ensures its correctness.Combination\n\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nbest-of-n(reﬁnement)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nreﬁnement(best-of-n)\nbest-of-n\nbest-of-n\n\nm\n\nm′\n\n5\n3\n5\n3\n3\n5\n3\n5\n5\n3\n5\n3\n3\n5\n5\n3\nN/A 15\nN/A 15\n\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\n\n33.78\n31.23\n31.85\n31.35\n51.32\n50.1\n57.56\n49.75\n32.71\n33.48\n\n100.0%\n100.0%\n100.0%\n100.0%\n63.64%\n65.62%\n60.61%\n59.38%\n90.62%\n84.38%\n\n50.0%\n46.88%\n50.0%\n50.0%\n48.48%\n53.12%\n54.55%\n43.75%\n56.25%\n53.12%\n\nWith the previous optimal parameters ﬁxed, run the ablation on the combination methods and if\nRAG is enabled.By Table 12, we see that the optimal combination in this testing group is a 5-step\nreﬁnement with each iteration being a best-of-3 call, with RAG enabled.33.78\n31.23\n31.85\n31.35\n32.66\n32.88\n34.88\n29.54\n29.64\n28.25\n\n17\n\n\fCompﬁles: Length Optimization See (§4.3)\n\nCompﬁles: Readability Optimization Consider Figure 5, in which a lemma from the 2019 IMO\nproblem 1 (from the Compﬁles dataset) is optimized for readability.This introduces multiple new\nhypotheses, which generalize a linear_property of the functions, and then reuses and instanti-\nates that (and others, too) hypothesis throughout the proof, creating a signiﬁcantly more declaritive,\nmodular, and therefore readable proof.Original (human-written)\n\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\nrefine hf 1, fun a => ?_i\nchange g a = g 1 * a\nrw [mul_comm, ← smul_eq_mul, ←\n\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nImProver (readability-optimized)\nlemma additive_to_int_linear (f : Z → Z) (h:\n∀ (x y : Z), f (x + y) = f x + f y):\n\n∃ c, ∀ a, f a = c * a\n\n:= by\n\nlet g := AddMonoidHom.toIntLinearMap <|\n\nAddMonoidHom.mk' f h\n\nhave linear_property : ∀ a, f a = g a := by\n\nintro a\nrfl\n\nhave g_smul : ∀ a, g a = g 1 * a := by\n\nintro a\nrw [mul_comm, ← smul_eq_mul, ←\nLinearMap.map_smul, smul_eq_mul, mul_one]\n\nrefine hf 1, fun a => ?_i\nhave f_eq_g : f a = g a := linear_property a\nhave g_a_eq : g a = g 1 * a := g_smul a\nrw [f_eq_g, linear_property 1, g_a_eq]\n\nFigure 5: Optimizing a lemma from IMO 2019 P1 for readability\n\nMIL: Length Optimization Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, eliminating simp calls and introducing proof terms\ninto the structure of the proof to shorten it from 9 tactic invocations to 7.Original (human-written)\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\n\n:= by\n\next x\nsimp only [mem_map]\nconstructor\n· rintro hy, y_in, hyi\n\nexact hϕ y, hy, y_in, rfli, hyi\n\n· rintro hy, hz, z_in, hzi, hyi\n\nuse z, z_in\ncalc ψ.comp ϕ z = ψ (ϕ z) := rfl\n_\n= ψ y := by congr\n\nImProver (length-optimized)\n\nexample (ϕ : G →* H) (ψ : H →* K) (S :\n\nSubgroup G) :\nmap (ψ.comp ϕ) S = map ψ (S.map ϕ)\nby\next x\nsimp only [mem_map]\nconstructor\nrintro hy, y_in, hyi; exact hϕ y, hy, y_in,\n\n:=\n\nrfli, hyi\n\nrintro hy, hz, z_in, hzi, hyi; exact hz,\nz_in, (congr_arg ψ hz).trans hyi\n\nFigure 6: Optimizing a lemma from the solutions of MIL CH08 S01 for length\n\nMIL: Length Optimization 2 Consider Figure 6, which optimizes an exercise solution from MIL\nChapter 8, Section 1 (Group theory) for length, converting a full tactic proof into a single proof term\nto shorten it from 28 tactic invocations to 1.Note that the model does not have access to the Lean\ncommands that symbolically generate proof terms, and therefore generates and estimates the proof\nterm entirely by itself.MIL: Readability Optimization See (§4.3)\n\nMathlib: Length Optimization Consider Figure 8, which optimizes a theorem in algebraic topol-\nogy from mathlib for length, eliminating simp calls and combining tactics to shorten it from 3 tactic\ninvocations to 1.Mathlib: Readability Optimization Consider Figure 9, a theorem from Mathlib that we optimize\nfor readability.This original proof carried a score of 0, as it does not contain any declarative statements.It is concise\nand efﬁcient, however, it is difﬁcult to understand and read.18\n\n\fOriginal (human-written)\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\next x; constructor\n· rintro (hxs, xnti | hxt, xnsi)\n\nImProver (length-optimized)\n\nexample : s \\ t ∪ t \\ s = (s ∪ t) \\ (s ∩ t)\n\n:= by\n\nexact Set.ext fun x => hfun h => h.elim\n\n(fun hxs, xnti => hOr.inl xs, fun h_, xti =\n> xnt xti) (fun hxt, xnsi => hOr.inr xt,\nfun hxs, _i => xns xsi),\n\nfun hh, nxsti => h.elim (fun xs => Or.inl h\nxs, fun xt => nxst hxs, xtii) (fun xt =>\nOr.inr hxt, fun xs => nxst hxs, xtii)i\n\n· constructor\n\nleft\nexact xs\nrintro h_, xti\ncontradiction\n\n.constructor\n\nright\nexact xt\nrintro hxs, _i\ncontradiction\nrintro hxs | xt, nxsti\n· left\n\nuse xs\nintro xt\napply nxst\nconstructor <;> assumption\n\n.right; use xt; intro xs\n\napply nxst\nconstructor <;> assumption\n\nFigure 7: Optimizing a lemma from MIL CH04 S01 solution for length\n\nOriginal (human-written)\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n:= by\n\nf).map JpK) ((πm g).map JqK)\n\nsimp only [map_eq, ←\n\nPath.Homotopic.map_lift]; apply\nPath.Homotopic.hpath_hext; exact hfg\n\nImProver (length-optimized)\n\n/-- If `f(p(t) = g(q(t))` for two paths `p`\n\nand `q`, then the induced path homotopy\nclasses\n\n`f(p)` and `g(p)` are the same as well,\n\ndespite having a priori different types\n-/\n\ntheorem heq_path_of_eq_image : HEq ((πm\n\nf).map JpK) ((πm g).map JqK)\nexact Path.Homotopic.hpath_hext hfg\n\n:= by\n\nFigure 8: Optimizing a theorem from Mathlib/FundamentalGroupoid/InducedMaps for\nlength\n\nAfter optimizing for readability, we see that the model did not change the structure of the proof.Rather, it added an intermediate declaration so that users can better understand the state after the\nconvert.This intermediate tactic greatly helps in the understandability and clarity of the proof.",
          "time": 289.6434292793274,
          "input_token_size": 16874,
          "output_token_size": 64,
          "original_provenance_id": 0
        }
      ],
      "metadata": {
        "question_id": "paper_f774fbc4",
        "created_at": 1749665196.9129083,
        "processing_time": 295.15119433403015,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs_prev",
        "reason": "shorter_provenance",
        "sentence_count": 377,
        "provenance_count": 1,
        "alternatives": [
          {
            "source": "test_outputs_prev",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          },
          {
            "source": "test_outputs_prev1",
            "sentence_count": 377,
            "provenance_count": 1,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.847343"
      }
    }
  },
  {
    "fe198c97": {
      "question": "Which models were compared to ImProver in the experiments?",
      "answer": "GPT-4o, GPT-4o-mini",
      "provenance": [
        {
          "provenance_id": 0,
          "input_sentence_ids": [
            176,
            177,
            178,
            179,
            180,
            181
          ],
          "provenance_ids": [
            176,
            179
          ],
          "provenance": "Overall, we conclude that although the performance of both ImProver and GPT-4o decreases on\nlength and readability optimization on more difﬁcult datasets, ImProver signiﬁcantly outperforms\nGPT-4o on all datasets for length and readability optimization.We measure the baseline results from the GPT-\n4o and GPT-4o-mini models, noting that GPT-4o is the better-scoring model (with respect to the\nimprovement score).",
          "time": 10.599233388900757,
          "input_token_size": 25349,
          "output_token_size": 101,
          "original_provenance_id": 0
        },
        {
          "provenance_id": 1,
          "input_sentence_ids": [
            135,
            136,
            137,
            138,
            139,
            140
          ],
          "provenance_ids": [
            137,
            138,
            139
          ],
          "provenance": "The model is ﬁxed as GPT-4o, CoS, output formatting, and examples are ﬁxed as the best\ncombination from the previous test, and no other features enabled.n and Model: Here, we evaluate the effects of larger n values and different models.We test n =\n3, 5, 7, 10, 15 on GPT-4o and GPT-4o-mini, as well as n = 20 for GPT-4o-mini (as it is of a far\nlower token cost).",
          "time": 18.427612781524658,
          "input_token_size": 30791,
          "output_token_size": 154,
          "original_provenance_id": 1
        },
        {
          "provenance_id": 2,
          "input_sentence_ids": [
            94,
            95,
            96,
            97,
            98
          ],
          "provenance_ids": [
            94,
            95,
            96
          ],
          "provenance": "We ﬁrst perform ablation testing on the\nImProver model parameters (§3.1) to ensure that ImProver’s parameter speciﬁcation is the optimal\none with respect to correctness and metric optimization score.We then evaluate this optimal param-\neter combination on datasets of varying complexity and analyze the performance and results thereof.Lastly, we note the performance of ImProver in NTP applications in comparison to the base GPT-4o\nand GPT-4o-mini models.",
          "time": 24.56247305870056,
          "input_token_size": 33318,
          "output_token_size": 188,
          "original_provenance_id": 2
        },
        {
          "provenance_id": 3,
          "input_sentence_ids": [
            91,
            92,
            93
          ],
          "provenance_ids": [
            91,
            92
          ],
          "provenance": "4 EXPERIMENTS\n\nWe test ImProver on rewriting real-world undergraduate theorems, competition problems, and\nresearch-level mathematics and compare its results to those of the base GPT-4o and GPT-4o-mini\nmodels.We examine the optimization capabilities of ImProver for the length and readability met-\nrics - studying the effectiveness in maintaining the correctness of the tactic proof while making it\nmore concise, as well as making it more declarative in style and readable in practice.",
          "time": 29.250159978866577,
          "input_token_size": 41316,
          "output_token_size": 248,
          "original_provenance_id": 3
        }
      ],
      "metadata": {
        "question_id": "paper_fe198c97",
        "created_at": 1750198597.7852976,
        "processing_time": 40.37224054336548,
        "processing_complete": true,
        "max_provenances": 5
      },
      "source_info": {
        "selected_from": "test_outputs",
        "reason": "shorter_provenance",
        "sentence_count": 20,
        "provenance_count": 4,
        "alternatives": [
          {
            "source": "test_outputs",
            "sentence_count": 20,
            "provenance_count": 4,
            "has_answer": true
          }
        ],
        "selection_timestamp": "2025-06-17T16:48:52.848711"
      }
    }
  }
]